[
  {
    "objectID": "nmRoots1.html",
    "href": "nmRoots1.html",
    "title": "3¬† Roots 1",
    "section": "",
    "text": "3.1 Introduction to Numerical Root Finding\nIn this chapter and the next we want to solve equations using a computer. Because it takes time to get used to doing numerical calculations we have split this material over two weeks.1 In this chapter we will cover the bisection method and fixed point iteration. In the next chapter we will cover the Newton-Raphson and secant methods.\nThe goal of equation solving is to find the value of the independent variable which makes the equation true. These are the sorts of equations that you learned to solve at school. For a very simple example, solve for \\(x\\) if \\(x+5 = 2x- 3\\). Or, for another example, the equation \\(x^2+x=2x - 7\\) is an equation that could be solved with the quadratic formula. The equation \\(\\sin(x) = \\frac{\\sqrt{2}}{2}\\) is an equation which can be solved using some knowledge of trigonometry. The topic of Numerical Root Finding really boils down to approximating the solutions to equations without using all of the by-hand techniques that you learned in high school. The down side to everything that we are about to do is that our answers are only ever going to be approximations.\nThe fact that we will only ever get approximate answers begs the question: why would we want to do numerical algebra if by-hand techniques exist? The answers are relatively simple:\nLet us first take a look at equations in a more abstract way. Consider the equation \\(\\ell(x) = r(x)\\) where \\(\\ell(x)\\) and \\(r(x)\\) stand for left-hand and right-hand expressions respectively. To begin solving this equation we can first rewrite it by subtracting the right-hand side from the left to get \\[\\begin{equation}\n\\ell(x) - r(x) = 0.\n\\end{equation}\\] Hence, we can define a function \\(f(x)\\) as \\(f(x)=\\ell(x)-r(x)\\) and observe that every equation can be written as: \\[\\begin{equation}\n\\text{ Find } x \\text{ such that } f(x) = 0.\n\\end{equation}\\] This gives us a common language for which to frame all of our numerical algorithms. An \\(x\\) where \\(f(x)=0\\) is called a root of \\(f\\) and thus we have seen that solving an equation is always a root finding problem.\nFor example, if we want to solve the equation \\(3\\sin(x) + 9 = x^2 - \\cos(x)\\) then this is the same as solving \\((3\\sin(x) + 9 ) - (x^2 - \\cos(x)) = 0\\). We illustrate this idea in Figure¬†3.1. You should pause and notice that there is no way that you are going to apply by-hand techniques from algebra to solve this equation ‚Ä¶ an approximate answer is pretty much our only hope.\nCode\nimport numpy as np \nimport matplotlib.pyplot as plt\nx = np.linspace(-4,4, 100) \nl = 3 * np.sin(x) + 9\nr = x**2 - np.cos(x)\n\nfig, axes = plt.subplots(nrows = 1, ncols = 2)\n\naxes[0].plot(x, l, 'b-.', label=r\"$3\\sin(x)+9$\")\naxes[0].plot(x, r, 'r-', label=r\"$x^2-\\cos(x)$\")\naxes[0].grid()\naxes[0].legend()\naxes[0].set_title(r\"$3\\sin(x)+9 = x^2-\\cos(x)$\")\n\naxes[1].plot(x, l-r, 'g:', label=r\"(3\\sin(x)+9) - (x^2-\\cos(x))\")\naxes[1].plot(x, np.zeros(100), 'k-')\naxes[1].grid()\naxes[1].legend()\naxes[1].set_title(r\"$(3\\sin(x)+9) - (x^2-\\cos(x))=0$\")\n\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†3.1: Two ways to visualise the same root finding problem\nOn the left-hand side of Figure¬†3.1 we see the solutions as the intersections of the graph of \\(3\\sin(x) + 9\\) with the graph of \\(x^2 - \\cos(x)\\), and on the right-hand side we see the solutions as the intersections of the graph of \\(\\left( 3\\sin(x)+9 \\right) - \\left( x^2 - \\cos(x) \\right)\\) with the \\(x\\) axis. From either plot we can read off the approximate solutions: \\(x_1 \\approx -2.55\\) and \\(x_2 \\approx 2.88\\). Figure¬†3.1 should demonstrate what we mean when we say that solving equations of the form \\(\\ell(x) = r(x)\\) will give the same answer as finding the roots of \\(f(x) = \\ell(x)-r(x)\\).\nWe now have one way to view every equation-solving problem. As we will see in this chapter, if \\(f(x)\\) has certain properties then different numerical techniques for solving the equation will apply ‚Äì and some will be much faster and more accurate than others. In the following sections you will develop several different techniques for solving equations of the form \\(f(x) = 0\\). You will start with the simplest techniques to implement and then move to the more powerful techniques that use some ideas from Calculus to understand and analyse. Throughout this chapter you will also work to quantify the amount of error that one makes when using these techniques.\nThis chapter is split over two weeks. In the first week we will cover the bisection method and fixed point iteration. In the second week we will cover the Newton-Raphson and secant methods.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Roots 1</span>"
    ]
  },
  {
    "objectID": "nmRoots1.html#introduction-to-numerical-root-finding",
    "href": "nmRoots1.html#introduction-to-numerical-root-finding",
    "title": "3¬† Roots 1",
    "section": "",
    "text": "Most equations do not lend themselves to by-hand solutions. The reason you may not have noticed that is that we tend to show you only nice equations that arise in often very simplified situations. When equations arise naturally they are often not nice.\nBy-hand algebra is often very challenging, quite time consuming, and error prone. You will find that the numerical techniques are quite elegant, work very quickly, and require very little overhead to actually implement and verify.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Roots 1</span>"
    ]
  },
  {
    "objectID": "nmRoots1.html#sec-bisection",
    "href": "nmRoots1.html#sec-bisection",
    "title": "3¬† Roots 1",
    "section": "3.2 The Bisection Method",
    "text": "3.2 The Bisection Method\n\n3.2.1 Intuition\nThis section contains several discussion exercises to get you thinking about the bisection method. You may not have anyone around to discuss with, in which case you should just discuss the questions with yourself, i.e., think through the questions. When you next meet with your group you can then discuss them with your peers.\n\nExercise 3.1 üí¨ A friend tells you that she is thinking of a number between 1 and 100. She will allow you multiple guesses with some feedback for where the mystery number falls. How do you systematically go about guessing the mystery number? Is there an optimal strategy?\nFor example, the conversation might go like this.\n\nSally: I am thinking of a number between 1 and 100.\nJoe: Is it 35?\nSally: No, 35 is too low.\nJoe: Is it 99?\nSally: No, 99 is too high.\n‚Ä¶\n\n\n\n\nExercise 3.2 üí¨ Imagine that Sally likes to formulate her answer not in the form ‚Äú\\(x\\) is too small‚Äù or ‚Äú\\(x\\) is too large‚Äù but in the form ‚Äú\\(f(x)\\) is positive‚Äù or ‚Äú\\(f(x)\\) is negative‚Äù. If she uses \\(f(x) = x-x_0\\), where \\(x_0\\) is Sally‚Äôs chosen number then her new answers contain exactly the same information as her previous answers? Can you now explain how Sally‚Äôs game is a root finding game?\n\n\n\nExercise 3.3 üí¨ Now go and play the game with other functions \\(f(x)\\). Choose someone from your group to be Sally and someone else to be Joe. Sally can choose a continuous function and Joe needs to guess its root. Does your strategy still allow Joe to find the root of \\(f(x)\\) at least approximately? When should the game stop? Does Sally need to give Joe some extra information to give him a fighting chance?\n\n\n\nExercise 3.4 üí¨ Was it necessary to say that Sally‚Äôs function was continuous? Does your strategy work if the function is not continuous.\n\n\nNow let us get to the maths. We will start the mathematical discussion with a theorem from Calculus.\n\nTheorem 3.1 (The Intermediate Value Theorem (IVT)) If \\(f(x)\\) is a continuous function on the closed interval \\([a,b]\\) and \\(y_*\\) lies between \\(f(a)\\) and \\(f(b)\\), then there exists some point \\(x_* \\in [a,b]\\) such that \\(f(x_*) = y_*\\).\n\n\n\nExercise 3.5 üñã Draw a picture of what the intermediate value theorem says graphically.\n\n\n\nExercise 3.6 üñã If \\(y_*=0\\) the Intermediate Value Theorem gives us important information about solving equations. What does it tell us? Fill in the blanks in the following corollary.\n\n\n\nCorollary 3.1 If \\(f(x)\\) is a continuous function on the closed interval \\([a,b]\\) and if \\(f(a)\\) and \\(f(b)\\) have opposite signs then from the Intermediate Value Theorem we know that there exists some point \\(x_* \\in [a,b]\\) such that ____.\n\n\nThe Intermediate Value Theorem (IVT) and its corollary are existence theorems in the sense that they tell us that some point exists. The annoying thing about mathematical existence theorems is that they typically do not tell us how to find the point that is guaranteed to exist. The method that you developed in Exercise¬†3.1 to Exercise¬†3.3 gives one possible way to find the root.\nIn those exercises you likely came up with an algorithm such as this:\n\nSay we know that a continuous function has opposite signs at \\(x=a\\) and \\(x=b\\).\nGuess that the root is at the midpoint \\(m = \\frac{a+b}{2}\\).\nBy using the signs of the function, narrow the interval that contains the root to either \\([a,m]\\) or \\([m,b]\\).\nRepeat until the interval is small enough.\n\nNow we will turn this strategy into computer code that will simply play the game for us. But first we need to pay careful attention to some of the mathematical details.\n\n\nExercise 3.7 üí¨ Where is the Intermediate Value Theorem used in the root-guessing strategy?\n\n\n\nExercise 3.8 üí¨ Why was it important that the function \\(f(x)\\) is continuous when playing this root-guessing game? Provide a few sketches to demonstrate your answer.\n\n\n\n3.2.2 Implementation\n\nExercise 3.9 (The Bisection Method) üñã üí¨ Goal: We want to solve the equation \\(f(x) = 0\\) for \\(x\\) assuming that the solution \\(x^*\\) is in the interval \\([a,b]\\). To understand the Bisection Method, have someone in the group sketch a graph of the function \\(f(x)\\) on the interval \\([a,b]\\). Then together, draw in the steps of the following algorithm:\nThe Algorithm: We assume that your group member has followed the instructions above and has sketched the graph of a function \\(f(x)\\) that is continuous on the interval \\([a,b]\\). To make approximations of the solutions to the equation \\(f(x) = 0\\), do the following:\n\nCheck to see if \\(f(a)\\) and \\(f(b)\\) have opposite signs. You can do this taking the product of \\(f(a)\\) and \\(f(b)\\).\n\nIf \\(f(a)\\) and \\(f(b)\\) have different signs then what does the IVT tell you?\nIf \\(f(a)\\) and \\(f(b)\\) have the same sign then what does the IVT not tell you? What should you do in this case?\nWhy does the product of \\(f(a)\\) and \\(f(b)\\) tell us something about the signs of the two numbers?\n\nCompute the midpoint of the closed interval, \\(m=\\frac{a+b}{2}\\). Mark the result in your plot so that you can see the value of \\(f(m)\\).\n\nWill \\(m\\) always be a better guess of the root than \\(a\\) or \\(b\\)? Why?\nWhat should you do here if \\(f(m)\\) is really close to zero?\n\nCompare the signs of \\(f(a)\\) versus \\(f(m)\\) and \\(f(b)\\) versus \\(f(m)\\).\n\nWhat do you do if \\(f(a)\\) and \\(f(m)\\) have opposite signs?\nWhat do you do if \\(f(m)\\) and \\(f(b)\\) have opposite signs?\n\nChoose the new interval \\([a,m]\\) or \\([m,b]\\) and repeat steps 2 and 3 for this interval. Stop when the interval containing the root is small enough.\n\n\n\n\nExercise 3.10 üíª We want to write a Python function for the Bisection Method. Instead of jumping straight into writing the code we should first come up with the structure of the code. It is often helpful to outline the structure as comments in your file. Use the template below and complete the comments. Note how the function starts with a so-called docstring that describes what the function does and explains the function parameters and its return value. This is standard practice and is how the help text is generated that you see when you hover over a function name in your code.\nDon‚Äôt write the code yet, just complete the comments. I recommend switching off the AI for this exercise because otherwise the AI will keep already suggesting the code while you write the comments.\ndef bisection(f, a, b, tol=1e-5):\n    \"\"\"\n    Find a root of f(x) in the interval [a, b] using the bisection method.\n\n    Parameters:\n        f   : function, the function for which we seek a root\n        a   : float, left endpoint of the interval\n        b   : float, right endpoint of the interval\n        tol : float, stopping tolerance\n\n    Returns:\n        float: approximate root of f(x)\n    \"\"\"\n\n    # check that a and b have opposite signs\n    # if not, return an error and stop\n\n    # calculate the midpoint m = (a+b)/2\n\n    # start a while loop that runs while the interval is \n    # larger than 2 * tol\n    \n        # if ...\n        # elif ...\n        # elif ...\n        \n        # Calculate midpoint of new interval\n\n    # end the while loop\n    # return the approximate root\n\n\n\nExercise 3.11 üíª Now use the comments from Exercise¬†3.10 as structure to complete a Python function for the Bisection Method. Test your Bisection Method code on the following equations. For each equation make a plot of the function on the interval to verify your result.\n\n\\(x^2 - 2 = 0\\) on \\(x \\in [0,2]\\)\n\\(\\sin(x) + x^2 = 2\\log(x) + 5\\) on \\(x \\in [1,5]\\)\nüéì \\(3\\sin(x) + 9 = x^2+\\cos(x)\\) on \\(x \\in [1,5]\\)\n\n\n\nFigure¬†3.2 shows the bisection method in action on the equation \\(x^2 - 2 = 0\\).\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nFigure¬†3.2: An animation of the bisection method in action on the equation \\(x^2 - 2 = 0\\).\n\n\n\n\n\n\n\n3.2.3 Analysis\nAfter we build any root finding algorithm we need to stop and think about how it will perform on new problems. The questions that we typically have for a root-finding algorithm are:\n\nWill the algorithm always converge to a solution?\nHow fast will the algorithm converge to a solution?\nAre there any pitfalls that we should be aware of when using the algorithm?\n\n\n\nExercise 3.12 üí¨ Discussion:\n\nWhat must be true in order to use the bisection method?\nDoes the bisection method work if the Intermediate Value Theorem does not apply? (Hint: what does it mean for the IVT to ‚Äúnot apply?‚Äù)\nIf there is a root of a continuous function \\(f(x)\\) between \\(x=a\\) and \\(x=b\\), will the bisection method always be able to find it? Why / why not?\n\n\n\nNext we will focus on a deeper mathematical analysis that will allow us to determine exactly how fast the bisection method actually converges to within a pre-set tolerance. Work through the next problem to develop a formula that tells you exactly how many steps the bisection method needs to take before it gets close enough to the true solution.\n\n\nExercise 3.13 üñã Let \\(f(x)\\) be a continuous function on the interval \\([a,b]\\) and assume that \\(f(a) \\cdot f(b) &lt;0.\\) A recurring theme in Numerical Analysis is to approximate some mathematical thing to within some tolerance. For example, if we want to approximate the solution to the equation \\(f(x)=0\\) to within \\(\\varepsilon\\) with the bisection method, we should be able to figure out how many steps it will take to achieve that goal.\n\nLet us say that \\(a = 3\\) and \\(b = 8\\) and \\(f(a) \\cdot f(b) &lt; 0\\) for some continuous function \\(f(x)\\). The width of this interval is \\(5\\), so if we guess that the root is \\(m=(3+8)/2 = 5.5\\) then our error is less than \\(5/2\\). In the more general setting, if there is a root of a continuous function in the interval \\([a,b]\\) then how far off could the midpoint approximation of the root be? In other words, what is the error in using \\(m=(a+b)/2\\) as the approximation of the root?\nThe bisection method cuts the width of the interval down to a smaller size at every step. As such, the approximation error gets smaller at every step. Fill in the blanks in the following table to see the pattern in how the approximation error changes with each iteration.\n\n\n\n\nIteration\nWidth of Interval\nMaximal Error\n\n\n\n\n1\n\\(|b-a|\\)\n\\(\\frac{|b-a|}{2}\\)\n\n\n2\n\\(\\frac{|b-a|}{2}\\)\n\n\n\n3\n\\(\\frac{|b-a|}{2^2}\\)\n\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(n\\)\n\\(\\frac{|b-a|}{2^{n-1}}\\)\n\n\n\n\n\nNow to the key question:\nIf we want to approximate the solution to the equation \\(f(x)=0\\) to within some tolerance \\(\\varepsilon\\) then how many iterations of the bisection method do we need to take?\nHint: Set the \\(n^{th}\\) approximation error from the table equal to \\(\\varepsilon\\). What should you solve for from there?\n\n\n\nIn Exercise¬†3.13 you actually proved the following theorem.\n\nTheorem 3.2 (Convergence Rate of the Bisection Method) If \\(f(x)\\) is a continuous function with a root in the interval \\([a,b]\\) and if the bisection method is performed to find the root then:\n\nThe error between the actual root and the approximate root will decrease by a factor of 2 at every iteration.\nIf we want the approximate root found by the bisection method to be within a tolerance of \\(\\varepsilon\\) then \\[\\begin{equation}\n\\frac{|b-a|}{2^{n}} = \\varepsilon\n\\end{equation}\\] where \\(n\\) is the number of iterations that it takes to achieve that tolerance.\n\n\nSolving for the number \\(n\\) of iterations we get \\[\\begin{equation}\nn = \\log_2\\left( \\frac{|b-a|}{\\varepsilon} \\right).\n\\end{equation}\\]\nRounding the value of \\(n\\) up to the next integer gives the number of iterations necessary to approximate the root to a precision less than \\(\\varepsilon\\).\n\n\nExercise 3.14 üñã üéì Apply what you have learned in Theorem¬†3.2 to determine how many iterations the bisection method will take to approximate the solution of the equation \\[3\\sin(x) + 9 = x^2+\\cos(x)\\] on \\(x \\in [1,5]\\) to within a tolerance of \\(10^{-6}\\).\nüíª Now create a second version of your Python Bisection Method function that uses a for loop that takes the exact number of steps required to guarantee that the approximation to the root lies within a requested tolerance. This should be in contrast to your first version which likely used a while loop to decide when to stop. Is there an advantage to using one of these version of the Bisection Method over the other?\n\n\nThe final type of analysis that we should do on the bisection method is to make plots of the error between the approximate solution that the bisection method gives you and the exact solution to the equation. This is a bit of a funny thing! Stop and think about this for a second: if you know the exact solution to the equation then why are you solving it numerically in the first place!?!? However, whenever you build an algorithm you need to test it on problems where you actually do know the answer so that you can be somewhat sure that it is not giving you nonsense. Furthermore, analysis like this tells us how fast the algorithm is expected to perform.\nFrom Theorem¬†3.2 you know that the bisection method cuts the interval in half at every iteration. You proved in Exercise¬†3.13 that the error given by the bisection method is therefore cut in half at every iteration as well. The following example demonstrate this theorem graphically.\n\n\nExample 3.1 Let us solve the very simple equation \\(x^2 - 2 = 0\\) for \\(x\\) to get the solution \\(x = \\sqrt{2}\\) with the bisection method. Since we know the exact answer we can compare the exact answer to the value of the midpoint given at each iteration and calculate an absolute error: \\[\\begin{equation}\n\\text{Absolute Error} = | \\text{Approximate Solution} - \\text{Exact Solution}|.\n\\end{equation}\\]\nLet us write a Python function that implements the bisection method and collects the absolute errors at each iteration into a list.\n\ndef bisection_with_error_tracking(f, x_exact, a, b, tol):\n    \"\"\"\n    Implements the bisection method and tracks absolute error at each iteration.\n    \n    Parameters:\n        f (callable): Function for which to find the root\n        x_exact (float): The exact root of the function\n        a (float): Left endpoint of initial interval\n        b (float): Right endpoint of initial interval \n        tol (float): Tolerance for stopping criterion\n        \n    Returns:\n        list: List of absolute errors between approximate and exact solution at each iteration\n    \"\"\"\n    errors = []\n    while (b - a) / 2.0 &gt; tol:\n        midpoint = (a + b) / 2.0\n        if f(midpoint) == 0:\n            break\n        elif f(a) * f(midpoint) &lt; 0:\n            b = midpoint\n        else:\n            a = midpoint\n        error = abs(midpoint - x_exact)\n        errors.append(error)\n    return errors\n\nWe can now use this function to see the absolute error at each iteration when solving the equation \\(x^2-2=0\\) with the bisection method.\n\nimport numpy as np\n\ndef f(x):\n    return x**2 - 2\n\nx_exact = np.sqrt(2)\n\n# Using the interval [1, 2] and a tolerance of 1e-7\ntolerance = 1e-7\nerrors = bisection_with_error_tracking(f, x_exact, 1, 2, tolerance)\nerrors\n\n[np.float64(0.08578643762690485),\n np.float64(0.16421356237309515),\n np.float64(0.039213562373095145),\n np.float64(0.023286437626904855),\n np.float64(0.007963562373095145),\n np.float64(0.0076614376269048545),\n np.float64(0.00015106237309514547),\n np.float64(0.0037551876269048545),\n np.float64(0.0018020626269048545),\n np.float64(0.0008255001269048545),\n np.float64(0.0003372188769048545),\n np.float64(9.307825190485453e-05),\n np.float64(2.8992060595145475e-05),\n np.float64(3.2043095654854525e-05),\n np.float64(1.5255175298545254e-06),\n np.float64(1.3733271532645475e-05),\n np.float64(6.103877001395475e-06),\n np.float64(2.2891797357704746e-06),\n np.float64(3.818311029579746e-07),\n np.float64(5.718432134482754e-07),\n np.float64(9.500605524515038e-08),\n np.float64(1.4341252385641212e-07),\n np.float64(2.4203234305630872e-08)]\n\n\nNext we write a function to plot the absolute error on the vertical axis and the iteration number on the horizontal axis. We get Figure¬†3.3. As expected, the absolute error follows an exponentially decreasing trend. Notice that it is not a completely smooth curve since we will have some jumps in the accuracy just due to the fact that sometimes the root will be near the midpoint of the interval and sometimes it will not be.\n\nimport matplotlib.pyplot as plt\ndef plot_errors(errors):\n    \"\"\"\n    Plot the absolute errors.\n    \n    Parameters\n        errors (list): List of absolute errors\n    \"\"\"\n    # Creating the x values for the plot (iterations) \n    iterations = np.arange(len(errors))\n\n    # Plotting the errors\n    plt.scatter(iterations, errors, label='Error per Iteration')\n    \n    plt.xlabel('Iteration')\n    plt.ylabel('Absolute Error') \n    plt.title('Absolute Error in Each Iteration')\n    plt.legend()\n    plt.show()\n\nplot_errors(errors)\n\n\n\n\n\n\n\nFigure¬†3.3: The evolution of the absolute error when solving the equation \\(x^2-2=0\\) with the bisection method.\n\n\n\n\n\nWithout Theorem¬†3.2 it would be rather hard to tell what the exact behaviour is in the exponential plot above. We know from Theorem¬†3.2 that the error will divide by 2 at every step, so if we instead plot the base-2 logarithm of the absolute error against the iteration number we should see a linear trend as shown in Figure¬†3.4.\n\ndef plot_log_errors(errors):\n    \"\"\"\n    Plot the base-2 logarithm of absolute errors and a best fit line.\n    \n    Parameters:\n        errors (list): List of absolute errors\n    \"\"\"\n    # Convert errors to base 2 logarithm\n    log_errors = np.log2(errors)\n    # Creating the x values for the plot (iterations)\n    iterations = np.arange(len(log_errors))\n\n    # Plotting the errors\n    plt.scatter(iterations, log_errors, label='Log Error per Iteration')\n\n    # Determine slope and intercept of the best-fit straight line\n    slope, intercept = np.polyfit(iterations, log_errors, deg=1)\n    best_fit_line = slope * iterations + intercept\n    # Plot the best-fit line\n    plt.plot(iterations, best_fit_line, label='Best Fit Line', color='red')\n\n    plt.xlabel('Iteration')\n    plt.ylabel('Base 2 Log of Absolute Error')\n    plt.title('Absolute Error in Each Iteration')\n    plt.legend()\n    plt.show()\n\nplot_log_errors(errors)\n\n\n\n\n\n\n\nFigure¬†3.4: Iteration number vs the base-2 logarithm of the absolute error. Notice the slope of \\(-1\\) indicating that the error is divided by a factor of 2 at each step of the algorithm.\n\n\n\n\n\nThere will be times later in this course where we will not have a nice theorem like Theorem¬†3.2 and instead we will need to deduce the relationship from plots like these.\n\nThe trend is linear since logarithms and exponential functions are inverses. Hence, applying a logarithm to an exponential will give a linear function.\nThe slope of the resulting linear function should be \\(-1\\) in this case since we are dividing by a factor of 2 each iteration. You can visually verify that the slope in the plot above follows this trend (the red dashed line in the plot is shown to help you see the slope).\n\n\n\n\nExercise 3.15 üíª Carefully read and understand all of the details of the previous example and plots. Then create plots similar to that example to solve the equation \\[\ne^{x-3}+\\sqrt{x+6}=4\n\\] on the interval \\(x \\in [0,5]\\) to which the exact solution is \\(x=3\\). You should see the same basic behaviour based on the theorem that you proved in Exercise¬†3.13. If you do not see the same basic behaviour then something has gone wrong. üí¨ Also, the plot will look much more regular than in the previous example. Can you explain why?\n\n\n\nExample 3.2 Another plot that numerical analysts use quite frequently for determining how an algorithm is behaving as it progresses is shown in Figure¬†3.5. and is defined by the following axes:\n\nThe horizontal axis is the absolute error at iteration \\(n\\).\nThe vertical axis is the absolute error at iteration \\(n+1\\).\n\n\ndef plot_error_progression(errors):\n    # Calculating the log2 of the absolute error at step n and n+1\n    log_errors = np.log2(errors)\n    log_errors_n = log_errors[:-1]  # log errors at step n (excluding the last one)\n    log_errors_n_plus_1 = log_errors[1:]  # log errors at step n+1 (excluding the first one)\n\n    # Plotting log_errors_n+1 vs log_errors_n\n    plt.scatter(log_errors_n, log_errors_n_plus_1, label='Log Error at n+1 vs Log Error at n')\n\n    # Fitting a straight line to the data points\n    slope, intercept = np.polyfit(log_errors_n, log_errors_n_plus_1, deg=1)\n    best_fit_line = slope * log_errors_n + intercept\n    plt.plot(log_errors_n, best_fit_line, color='red', label='Best Fit Line')\n\n    # Setting up the plot\n    plt.xlabel('Log2 of Absolute Error at Step n')\n    plt.ylabel('Log2 of Absolute Error at Step n+1')\n    plt.title('Log2 of Absolute Error at Step n+1 vs Step n')\n    plt.legend()\n    plt.show()\n\nplot_error_progression(errors)\n\n\n\n\n\n\n\nFigure¬†3.5: The base-2 logarithm of the absolute error at iteration \\(n\\) vs the base-2 logarithm of the absolute error at iteration \\(n+1\\).\n\n\n\n\n\nThis type of plot takes a bit of explaining the first time you see it. Each point in the plot corresponds to an iteration of the algorithm. The x-coordinate of each point is the base-2 logarithm of the absolute error at step \\(n\\) and the y-coordinate is the base-2 logarithm of the absolute error at step \\(n+1\\). The initial interations are on the right-hand side of the plot where the error is the largest (this will be where the algorithm starts). As the iterations progress and the error decreases the points move to the left-hand side of the plot. Examining the slope of the trend line in this plot shows how we expect the error to progress from step to step. The slope appears to be about \\(1\\) in Figure¬†3.5 and the intercept appears to be about \\(-1\\). In this case we used a base-2 logarithm for each axis so we have just empirically shown that \\[\\begin{equation}\n\\log_2(\\text{absolute error at step $n+1$}) \\approx 1\\cdot \\log_2(\\text{absolute error at step $n$}) -1.\n\\end{equation}\\] Exponentiating both sides we see that this linear relationship turns into (You should stop now and do this algebra.) Rearranging a bit more we get \\[\\begin{equation}\n(\\text{absolute error at step $n+1$}) = \\frac{1}{2}(\\text{absolute error at step $n$}),\n\\end{equation}\\] exactly as expected!! Pause and ponder this result for a second ‚Äì we just empirically verified the convergence rate for the bisection method just by examining Figure¬†3.5. That‚Äôs what makes these types of plots so useful!\n\n\n\nExercise 3.16 üíª Reproduce plots like that in the previous example but for the equation that you used in Exercise¬†3.15. Again check that the plots have the expected shape.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Roots 1</span>"
    ]
  },
  {
    "objectID": "nmRoots1.html#sec-fixedpoint1d",
    "href": "nmRoots1.html#sec-fixedpoint1d",
    "title": "3¬† Roots 1",
    "section": "3.3 Fixed Point Iteration",
    "text": "3.3 Fixed Point Iteration\nWe will now investigate a different problem that is closely related to root finding: the fixed point problem. Given a function \\(g\\) (of one real argument with real values), we look for a number \\(p\\) such that \\[\ng(p)=p.\n\\] This \\(p\\) is called a fixed point of \\(g\\).\nAny root finding problem \\(f(x)=0\\) can be reformulated as a fixed point problem, and this can be done in many (in fact, infinitely many) ways. For example, given \\(f\\), we can define \\(g(x):=f(x) + x\\); then \\[\nf(x) = 0 \\quad \\Leftrightarrow\\quad g(x)=x.\n\\] Just as well, we could set \\(g(x):=\\lambda f(x) + x\\) with any \\(\\lambda\\in{\\mathbb R}\\backslash\\{0\\}\\), and there are many other possibilities.\nThe heuristic idea for approximating a fixed point of a function \\(g\\) is quite simple. We take an initial approximation \\(x_{0}\\) and calculate subsequent approximations using the formula \\[\nx_{n}:=g(x_{n-1}).\n\\] A graphical representation of this sequence when \\(g = \\cos\\) and \\(x_0=2\\) is shown in Figure¬†3.6.\nTo view the animation, click on the play button below the plot.\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nFigure¬†3.6: Fixed point iteration \\(x_{n} = \\cos(x_{n-1})\\).\n\n\n\n\n\n\nExercise 3.17 üí¨ The plot that emerges in Figure¬†3.6 is known as a cobweb diagram, for obvious reason. Explain to others in your group what is happening in the animation in Figure¬†3.6 and how that animation is related to the fixed point iteration \\(x_{n} = \\cos(x_{n-1})\\).\n\n\n\nExercise 3.18 üéì The animation in Figure¬†3.6 is a graphical representation of the fixed point iteration \\(x_{n} = \\cos(x_{n-1})\\). Use Python to calculate the first 10 iterations of this sequence with \\(x_0=0.2\\). Use that to get an estimate of the solution to the equation \\(\\cos(x)-x=0\\).\n\n\nWhy is the sequence \\((x_n)\\) expected to approximate a fixed point? Suppose for a moment that the sequence \\((x_n)\\) converges to some number \\(p\\), and that \\(g\\) is continuous. Then \\[\np=\\lim_{n\\to\\infty}x_{n}=\\lim_{n\\to\\infty}g(x_{n-1})=\ng\\left(\\lim_{n\\to\\infty}x_{n-1}\\right)=g(p).\n\\tag{3.1}\\] Thus, if the sequence converges, then it converges to a fixed point. However, this resolves the problem only partially. One would like to know:\n\nUnder what conditions does the sequence \\((x_n)\\) converge?\nHow fast is the convergence, i.e., can one obtain an estimate for the approximation error?\n\nSo there is much for you to investigate!\n\n\nExercise 3.19 üñã Copy the two plots in Figure¬†3.7 to a piece of paper and draw the first few iterations of the fixed point iteration \\(x_{n} = g(x_{n-1})\\) on each of them. In the first plot start with \\(x_0=0.2\\) and in the second plot start with \\(x_0=1.5\\) and in the second plot start with \\(x=0.9\\) What do you observe about the convergence of the sequence in each case?\n\n\n\n\n\n\n\n\nFigure¬†3.7: Two plots for practicing your cobweb skills.\n\n\n\n\n\nCan you make some conjectures about when the sequence \\((x_n)\\) will converge to a fixed point and when it will not?\n\n\n\nExercise 3.20 üñã Make similar plots as in the previous exercise but with different slopes of the blue line. Can you make some conjectures about how the speed of convergence is related to the slope of the blue line?\n\n\nNow see if your observations are in agreement with the following theorem:\n\nTheorem 3.3 (Fixed Point Theorem) Suppose that \\(g:[a,b]\\to [a,b]\\) is differentiable, and that there exists \\(0&lt;k&lt;1\\) such that \\[\n\\lvert g^{\\prime}(x)\\rvert\\leq k\\quad \\text{for all }x \\in (a,b).\n\\tag{3.2}\\] Then, \\(g\\) has a unique fixed point \\(p\\in [a,b]\\); and for any choice of \\(x_0 \\in [a,b]\\), the sequence defined by \\[\nx_{n}:=g(x_{n-1}) \\quad \\text{for all }n\\ge1\n\\tag{3.3}\\] converges to \\(p\\). The following estimate holds: \\[\n\\lvert p- x_{n}\\rvert \\leq k^n \\lvert p-x_{0}\\rvert \\quad \\text{for all }n\\geq1.\n\\tag{3.4}\\]\n\n\nProof. The proof of this theorem is not difficult, but you can skip it and go directly to Exercise¬†3.21 if you feel that the theorem makes intuitive sense and you are not interested in proofs.\nWe first show that \\(g\\) has a fixed point \\(p\\) in \\([a,b]\\). If \\(g(a)=a\\) or \\(g(b)=b\\) then \\(g\\) has a fixed point at an endpoint. If not, then it must be true that \\(g(a)&gt;a\\) and \\(g(b)&lt;b\\). This means that the function \\(h(x):=g(x)-x\\) satisfies \\[\n\\begin{aligned}\nh(a) &= g(a)-a&gt;0, & h(b)&=g(b)-b&lt;0\n\\end{aligned}\n\\] and since \\(h\\) is continuous on \\([a,b]\\) the Intermediate Value Theorem guarantees the existence of \\(p\\in(a,b)\\) for which \\(h(p)=0\\), equivalently \\(g(p)=p\\), so that \\(p\\) is a fixed point of \\(g\\).\nTo show that the fixed point is unique, suppose that \\(q\\neq p\\) is a fixed point of \\(g\\) in \\([a,b]\\). The Mean Value Theorem implies the existence of a number \\(\\xi\\in(\\min\\{p,q\\},\\max\\{p,q\\})\\subseteq(a,b)\\) such that \\[\n\\frac{g(p)-g(q)}{p-q}=g'(\\xi).\n\\] Then \\[\n\\lvert p - q\\rvert = \\lvert g(p)-g(q) \\rvert = \\lvert (p-q)g'(\\xi) \\rvert = \\lvert p-q\\rvert \\lvert g'(\\xi) \\rvert \\le  k\\lvert p-q\\rvert &lt; \\lvert p-q\\rvert,\n\\] where the inequalities follow from Eq.¬†3.2. This is a contradiction, which must have come from the assumption \\(p\\neq q\\). Thus \\(p=q\\) and the fixed point is unique.\nSince \\(g\\) maps \\([a,b]\\) onto itself, the sequence \\(\\{x_n\\}\\) is well defined. For each \\(n\\ge0\\) the Mean Value Theorem gives the existence of a \\(\\xi\\in(\\min\\{x_n,p\\},\\max\\{x_n,p\\})\\subseteq(a,b)\\) such that \\[\n\\frac{g(x_n)-g(p)}{x_n-p}=g'(\\xi).\n\\] Thus for each \\(n\\ge1\\) by Eq.¬†3.2, Eq.¬†3.3 \\[\n\\lvert x_n-p\\rvert = \\lvert g(x_{n-1})-g(p) \\rvert = \\lvert (x_{n-1}-p)g'(\\xi) \\rvert = \\lvert x_{n-1}-p\\rvert \\lvert g'(\\xi) \\rvert \\le  k\\lvert x_{n-1}-p\\rvert.\n\\] Applying this inequality inductively, we obtain the error estimate Eq.¬†3.4. Moreover since \\(k &lt;1\\) we have \\[\\lim_{n\\rightarrow\\infty}\\lvert x_{n}-p\\rvert \\le \\lim_{n\\rightarrow\\infty} k^n \\lvert x_{0}-p\\rvert = 0,\n\\] which implies that \\((x_n)\\) converges to \\(p\\).¬† ‚óª\n\n\n\nExercise 3.21 üñã üéì This exercise shows why the conditions of the Theorem¬†3.3 are important.\nThe equation \\[\nf(x)=x^{2}-2=0\n\\] has a unique root \\(\\sqrt{2}\\) in \\([1, 2]\\). There are many ways of writing this equation in the form \\(x=g(x)\\); we consider two of them: \\[\n\\begin{aligned}\nx&=g(x)=x-(x^{2}-2), &\n  x&=h(x)=x-\\frac{x^{2}-2}{3}.\n\\end{aligned}\n\\] Calculate the first terms in the sequences generated by the fixed point iteration procedures \\(x_{n}=g(x_{n-1})\\) and \\(x_{n}=h(x_{n-1})\\) with start value \\(x_0=1\\). Which of these fixed point problems generate a rapidly converging sequence? Calculate the derivatives of \\(g\\) and \\(h\\) and check if the conditions of the fixed point theorem are satisfied.\n\nThe previous exercise illustrates that one needs to be careful in rewriting root finding problems as fixed point problems‚Äîthere are many ways to do so, but not all lead to a good approximation. In the next section about Newton‚Äôs method we will discover a very good choice.\nNote at this point that Theorem¬†3.3 gives only sufficient conditions for convergence; in practice, convergence might occur even if the conditions are violated.\n\n\nExercise 3.22 üíª üéì In this exercise you will write a Python function to implement the fixed point iteration algorithm.\nFor implementing the fixed point method as a computer algorithm, there‚Äôs one more complication to be taken into account: how many steps of the iteration should be taken, i.e., how large should \\(n\\) be chosen, in order to reach the desired precision? The error estimate in Eq.¬†3.4 is often difficult to use for this purpose because it involves estimates on the derivative of \\(g\\) which may not be known.\nInstead, one uses a different stopping condition for the algorithm. Since the sequence is expected to converge rapidly, one uses the difference \\(|x_n-x_{n-1}|\\) to measure the precision reached. If this difference is below a specified limit, say \\(\\tau\\), the iteration is stopped. Since it is possible that the iteration does not converge‚Äîsee the example above‚Äîone would also stop the iteration (with an error message) if a certain number of steps is exceeded, in order to avoid infinite loops. In pseudocode the fixed point iteration algorithm is then implemented as follows:\n\nFixed point iteration\n\n\\[\n\\begin{array}{ll}\n\\ 1: \\ \\textbf{function} \\ FixedPoint(g,x_0,tol, N) &\\sharp \\ function \\ g, \\ start \\ point \\ x_0,\\\\\n\\ 2: \\ \\quad x \\gets x_0; \\ n \\gets 0               &\\sharp \\ tolerance \\ tol,\\\\\n\\ 3: \\ \\quad \\textbf{for} \\ i \\gets 1 \\ \\textbf{to}\\ N                         &\\sharp \\ max. \\ num. \\ of \\ iterations \\ N \\\\\n\\ 4: \\ \\quad\\quad y \\gets x; \\ x \\gets g(x)  & \\\\\n\\ 5: \\ \\quad\\quad \\textbf{if} \\ |y-x| &lt; tol \\ \\textbf{then}  &\\sharp \\ Desired \\ tolerance \\ reached  \\\\\n\\ 6: \\ \\quad\\quad\\quad \\textbf{return} \\ x  & \\\\\n\\ 7: \\ \\quad\\quad \\textbf{end \\ if}  &  \\\\\n\\ 8: \\ \\quad \\textbf{end for}  & \\\\\n\\ 9: \\ \\quad \\textbf{exception}(Iteration \\ has \\ not \\ converged) & \\\\\n10: \\ \\textbf{end function} &\n\\end{array}\n\\] Implement this algorithm in Python. Use it to approximate the fixed point of the function \\(g(x)=\\cos(x)\\) with start value \\(x_0=2\\) and tolerance \\(tol=10^{-8}\\).\n\n\nFurther reading: Section 2.2 of (Burden and Faires 2010).\n\n\n\n\n\nBurden, Richard L., and J. Douglas Faires. 2010. Numerical Analysis. 9th ed. Brooks Cole.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Roots 1</span>"
    ]
  },
  {
    "objectID": "nmNumbers.html",
    "href": "nmNumbers.html",
    "title": "1¬† Numbers",
    "section": "",
    "text": "1.1 Binary Numbers\nHave you ever wondered how computers, which operate in a realm of zeros and ones, manage to perform mathematical calculations with real numbers? The secret lies in approximation.\nIn this chapter and the next we will investigate the foundations that allow a computer to do mathematical calculations at all. How can it store real numbers? How can it calculate the values of mathematical functions? We will understand that the computer can do these things only approximately and will thus always make errors. Numerical Analysis is all about keeping these errors as small as possible while still being able to do efficient calculations.\nWe will meet the two kinds of errors that a computer makes: rounding errors and truncation errors. Rounding errors arise from the way the computer needs to approximate real numbers by binary floating point numbers, which are the numbers it know how to add, subtract, multiply and divide. We‚Äôll discuss this in this chapter. Truncation errors arise from the way the computer needs to reduce all calculations to a finite number of these four basic arithmetic operations. We will see that for the first time in Chapter 2 when we discuss how computers approximate functions by power series and then have to truncate these at some finite order.\nLet‚Äôs start with a striking example of how bad computers actually are at doing even simple calculations:\n(Even if you don‚Äôt know Python, you should be able to do this exercise after having read up to Section A.2.1 in the chapter on Essential Python.)\nIn the previous exercise it seems like the computer has failed you! What do you think happened on the computer and why did it give you a different answer? What, do you suppose, is the cautionary tale hiding behind the scenes with this problem?\nA computer circuit knows two states: on and off. As such, anything saved in computer memory is stored using base-2 numbers. This is called a binary number system. To fully understand a binary number system it is worthwhile to pause and reflect on our base-10 number system for a few moments.\nWhat do the digits in the number ‚Äú735‚Äù really mean? The position of each digit tells us something particular about the magnitude of the overall number. The number 735 can be represented as a sum of powers of 10 as\n\\[\\begin{equation}\n735 = 700 + 30 + 5 = 7 \\times 10^2 + 3 \\times 10^1 + 5 \\times 10^0\n\\end{equation}\\]\nand we can read this number as 7 hundreds, 3 tens, and 5 ones.\nNow let us switch to the number system used by computers: the binary number system. In a binary number system the base is 2 so the only allowable digits are 0 and 1 (just like in base-10 the allowable digits were 0 through 9). In binary (base-2), the number ‚Äú101,101‚Äù can be interpreted as\n\\[\\begin{equation}\n101,101_2 = 1 \\times 2^5 + 0 \\times 2^4 + 1 \\times 2^3 + 1 \\times 2^2 + 0 \\times 2^1 + 1 \\times 2^0\n\\end{equation}\\]\n(where the subscript ‚Äú2‚Äù indicates the base). If we put this back into base 10, so that we can read it more comfortably, we get\n\\[101,101_2 = 32 + 0 + 8 + 4 + 0 + 1 = 45_{10}.\\]\n(The commas in the numbers are only to allow for greater readability ‚Äì we can easily see groups of three digits and mentally keep track of what we are reading.)\nNext we will work with fractions and decimals.\nWe can do a similar thing with binary decimals.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Numbers</span>"
    ]
  },
  {
    "objectID": "nmNumbers.html#sec-binary_numbers",
    "href": "nmNumbers.html#sec-binary_numbers",
    "title": "1¬† Numbers",
    "section": "",
    "text": "Exercise 1.5 üñã Express the following binary numbers in base-10.\n\n\\(111_2\\)\n\\(10,101_2\\)\n\\(1,111,111,111_2\\)\n\nFor the last one you can save yourself some work by noticing that \\(1,111,111,111_2 = 10,000,000,000_2 - 1\\). This is a generally useful trick that will help you again in some later exercises.\n\n\n\nExercise 1.6 üí¨ Explain the joke: There are 10 types of people. Those who understand binary and those who do not.\n\n\n\nExercise 1.7 üí¨ üñã Discussion: With your group, discuss how you would convert a base-10 number into its binary representation, without using a calculator or computer. Once you have a proposed method put it into action on the number \\(237_{10}\\) to show that the base-2 expression is \\(11,101,101_2\\).\n\n\n\nExercise 1.8 üñã By hand, using the methods you developed above, convert the following numbers from base 10 to base 2 or visa versa.\n\nWrite \\(12_{10}\\) in binary\nWrite \\(11_{10}\\) in binary\nWrite \\(23_{10}\\) in binary üéì\nWrite \\(11_2\\) in base \\(10\\)\nWhat is \\(100101_2\\) in base \\(10\\)? üéì\n\nThe üéì icons indicate that you should enter your answers into the feedback quiz. This gives you feedback on whether or not your answer is correct, but just as importantly, it lets your lecturer know how you are progressing with the material. The feedback quizzes are not used for assessment purposes, solely for feedback.\n\n\n\nExercise 1.9 üñã üí¨ Write down an explanation of the technique that your group has come up with to do the conversion from base 10 to base 2.\n\n\n\n\nExample 1.1 Let us take the base \\(10\\) number \\(5.341_{10}\\) and expand it out to get\n\\[5.341_{10} = 5 + \\frac{3}{10} + \\frac{4}{100} + \\frac{1}{1000} = 5 \\times 10^0 + 3 \\times 10^{-1} + 4 \\times 10^{-2} + 1 \\times 10^{-3}.\\]\nThe position to the right of the decimal point is the negative power of 10 for the given position.\n\n\n\n\nExercise 1.10 üñã The base-2 number \\(1,101.01_2\\) can be expanded in powers of \\(2\\). Fill in the question marks below and observe the pattern in the powers.\n\\[1,101.01_2 = ? \\times 2^3 + 1 \\times 2^2 + 0 \\times 2^1 + ? \\times 2^0 + 0 \\times 2^{?} + 1 \\times 2^{-2}.\\]\n\n\n\nExample 1.2 Convert \\(11.01011_2\\) to base \\(10\\).\nSolution:\n\\[\\begin{aligned} 11.01011_2 &= 2 + 1 + \\frac{0}{2} + \\frac{1}{4} + \\frac{0}{8} + \\frac{1}{16} + \\frac{1}{32} \\\\ &= 1 \\times 2^1 + 1 \\times 2^0 + 0 \\times 2^{-1} + 1 \\times 2^{-2} + 0 \\times 2^{-3} + 1 \\times 2^{-4} + 1 \\times 2^{-5}\\\\ &= 3.34375_{10}. \\end{aligned}\\]\n\n\n\nExercise 1.11 üí¨ üñã üéì Convert the base \\(10\\) decimal \\(0.15625_{10}\\) to binary using the following steps.\n\nMultiply \\(0.15625\\) by \\(2\\). The whole number part of the result is the first binary digit to the right of the decimal point.\nTake the result of the previous multiplication and ignore the digit to the left of the decimal point. Multiply the remaining decimal by \\(2\\). The whole number part is the second binary decimal digit.\nRepeat the previous step until you have nothing left.\n\nExplain to each other in the group why each step gives the binary digit that it does.\n\n\n\nExercise 1.12 üñã Convert the base \\(10\\) fraction \\(0.1\\) into binary. Use this to explain why the computer made errors when it calculated with this number in Exercise¬†1.3.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Numbers</span>"
    ]
  },
  {
    "objectID": "nmNumbers.html#sec-floating_point_numbers",
    "href": "nmNumbers.html#sec-floating_point_numbers",
    "title": "1¬† Numbers",
    "section": "1.2 Floating Point Numbers",
    "text": "1.2 Floating Point Numbers\nIn this section we will discuss how a computer actually stores a number. More specifically, since computers only have finite memory, we would really like to know the full range of numbers that are possible to store in a computer. Clearly, given the uncountable nature of the real numbers, there will be gaps between the numbers that can be stored. We would like to know what gaps in our number system to expect when using a computer to store and do computations on numbers. For this it is important to know that computers store numbers in a way that is similar to how we write numbers in scientific notation.\n\n\nExample 1.3 Let us start the discussion with a very concrete example. Consider the number \\(x = -123.15625\\) (in base 10). As we have seen this number can be converted into binary. Indeed\n\\[x = -123.15625_{10} = -1111011.00101_2\\]\n(you should check this).\nIf a computer needs to store this number then first they put in the binary version of scientific notation. In this case this will be\n\\[\nx = -1.11101100101_2 \\times 2^{6}.\n\\] This is the floating point representation of the number.\n\n\n\nDefinition 1.1 For any non-zero base-2 number \\(x\\) the floating point representation is given by\n\\[x = (-1)^{s} \\times (1+ m) \\times 2^E\\]\nwhere \\(s \\in \\{0,1\\}\\), \\(m\\) is a binary number such that \\(0 \\le m &lt; 1\\), and \\(E\\) is an integer.\nThe number \\(1+m\\) is called the significand, \\(s\\) is known as the sign bit, and \\(E\\) is known as the exponent. We will refer to \\(m\\), the fractional part of the significand that actually contains the information, as the mantissa, but this use is not universal.\n\nTo allow for both very large and very small numbers, the exponent \\(E\\) can be positive or negative. However, inside the computer it is efficiently stored as an unsigned integer \\(e\\) called the biased exponent. The true exponent \\(E\\) is obtained by subtracting a fixed bias \\(B\\) from \\(e\\): \\[ E = e - B. \\] The bias is chosen to be roughly half of the maximum possible value of the stored exponent \\(e\\).\n\n\nExample 1.4 What are the mantissa, sign bit, and unbiased exponent for the numbers \\(7_{10}, -7_{10}\\), and \\((0.1)_{10}\\)?\nSolution:\n\nFor the number \\(7_{10}=111_2 = 1.11 \\times 2^2\\) we have \\(s=0, m=0.11\\) and \\(E=2\\).\nFor the number \\(-7_{10}=111_2 = -1.11 \\times 2^2\\) we have \\(s=1, m=0.11\\) and \\(E=2\\).\nFor the number \\(\\frac{1}{10} = 0.000110011001100\\cdots = 1.100110011\\cdots \\times 2^{-4}\\) we have \\(s=0, m=0.100110011\\cdots\\), and \\(E = -4\\).\n\n\n\nIn the last part of the previous example we saw that the number \\((0.1)_{10}\\) is actually a repeating decimal in base-2. This means that in order to completely represent the number \\((0.1)_{10}\\) in base-2 we need infinitely many decimal places. Obviously that cannot happen since we are dealing with computers with finite memory. Each number can only be allocated a finite number of bits. Thus the number needs to be rounded to the nearest number that can be represented with that number of bits. That leads to an error called the rounding error (sometimes also called roundoff error). We‚Äôll look into these in more detail in Section 1.3 below.\n\n\nDefinition 1.2 Machine precision is the gap between the number 1 and the next larger floating point number. Often it is represented by the symbol \\(\\epsilon\\). To clarify: the number 1 can always be stored in a computer system exactly and if \\(\\epsilon\\) is machine precision for that computer then \\(1+\\epsilon\\) is the next largest number that can be stored with that machine.\n\n\nFor all practical purposes the computer cannot tell the difference between two numbers if the relative difference is smaller than machine precision. It is important to remember this when you want to check the equality of two numbers in a computer.\n\nExercise 1.13 üñã üéì To make all of these ideas concrete let us play with a small computer system where each number is stored in the following format, using 6 bits:\n\\[s \\, e_1\\, e_2 \\, b_1 \\, b_2 \\, b_3\\]\nThe first bit is for the sign (\\(0=+\\) and \\(1=-\\)). The next two bits, \\(e_1\\) and \\(e_2\\) are for the biased exponent, and we will assume in this example that the bias is \\(B=1\\). The three bits on the right represent the significand of the number. Hence, every number in this number system takes the form\n\\[\n(-1)^s \\times (1+ 0.b_1b_2b_3) \\times 2^{e_1e_2-1}\n\\]\n\nWhat is the smallest positive number that can be represented in this form?\nWhat is the largest positive number that can be represented in this form?\nWhat is the machine precision in this number system?\n\n\n\nOver the course of the past several decades there have been many systems developed to properly store numbers. The IEEE standard that we now use is the accumulated effort of many computer scientists, much trial and error, and deep scientific research. We now have two standard precisions for storing numbers on a computer: single and double precision. The double precision standard is what most of our modern computers use.\n\nDefinition 1.3 According to the IEEE 754 standard:\n\nA single-precision number consists of 32 bits, with 1 bit for the sign, 8 for the exponent, and 23 for the mantissa. The bias is \\(B=127\\).\nA double-precision number consists of 64 bits with 1 bit for the sign, 11 for the exponent, and 52 for the mantissa. The bias is \\(B=1023\\).\n\n\n\n\nExercise 1.14 üñã What are the largest numbers that can be stored in single and double precision?\n\n\n\nExercise 1.15 üñã What is machine precision for the single and double precision standard?\n\n\n\nExercise 1.16 üñã üéì What is the gap between \\(2^n\\) and the next largest number that can be stored in double precision?\n\n\n\nExercise 1.17 üñã Computers contain hardware that can perform the basic operations of addition, subtraction, multiplication, and division on floating point numbers. To get a feel for what goes on under the hood, figure out how to add the numbers \\(x = 1.010_2 \\times 2^3\\) and \\(y = 1.110_2 \\times 2^1\\). How do you deal with the different exponents? What do you do so that the result is in the correct floating point format?\n\n\nMuch more can be said about floating point numbers such as how we store infinity, how we store NaN, and how we store 0. The Wikipedia page for floating point arithmetic might be of interest for the curious reader. It is beyond the scope of this module to go into all of those details here.\nThe biggest takeaway points from this section and the previous are:\n\nReal numbers are stored with finite precision in a computer.\nNice rational numbers like 0.1 are sometimes not machine representable in binary.\nMachine precision is the gap between 1 and the next largest number that can be stored.\nThe gap between one number and the next grows in proportion to the number.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Numbers</span>"
    ]
  },
  {
    "objectID": "nmNumbers.html#sec-rounding_errors",
    "href": "nmNumbers.html#sec-rounding_errors",
    "title": "1¬† Numbers",
    "section": "1.3 Rounding Errors",
    "text": "1.3 Rounding Errors\nWhen the binary representation of a real number has too many binary digits to be represented faithfully by a floating point number, we need to round it to the nearest floating point number that can be represented. That introduces rounding errors. We have seen above that the gap between two consecutive floating point numbers grows in proportion to the number. This means that the relative error of rounding is bounded by \\(\\epsilon/2\\) where \\(\\epsilon\\) is the machine precision.\nThe rounding rule that is used is ‚Äúround to nearest, ties to even‚Äù, which means that if the number is exactly halfway between two numbers that can be represented then we round the mantissa to an even binary number, i.e., to a mantissa that ends in 0.\n\nExample 1.5 If we want to store the number \\(1.625 = 1.101_2\\) in a floating point number system where the mantissa has only 2 bits then we round to \\(1.10_2 = 1.5_{10}\\) because \\(1.101_2\\) is exactly halfway between \\(1.100_2\\) and \\(1.110_2\\) and the rounding rule is ‚Äúround to nearest, ties to even‚Äù.\n\n\nTo dive a little deeper into what happened in Exercise¬†1.3, simplify the detailed analysis by working with only a 4 bit mantissa:\n\nExercise 1.18 üñã Write down how the number \\(1/10\\) is represented in a floating point number system where the mantissa has only 4 bits. Then calculate the first 10 terms of the sequence \\[\\begin{equation}\nx_{n+1} = \\left\\{ \\begin{array}{ll} 2x_n, & x_n \\in [0,\\frac{1}{2}] \\\\ 2x_n - 1, & x_n \\in\n        (\\frac{1}{2},1] \\end{array} \\right. \\quad \\text{with} \\quad x_0 = \\frac{1}{10}\n\\end{equation}\\] using this number system.\n\n\n\nExercise 1.19 (This problem is modified from (Greenbaum and Chartier 2012))\nüíª Sometimes floating point arithmetic does not work like we would expect (and hope) as compared to exact mathematics. In each of the following problems we have a mathematical problem that the computer gets wrong. Explain why the computer is getting these wrong.\n\nMathematically we know that \\(\\sqrt{5}^2\\) should just give us \\(5\\) back. In Python type np.sqrt(5)**2 == 5. What do you get and why do you get it?\nMathematically we know that \\(49\\cdot \\left( \\frac{1}{49} \\right)\\) should just be 1. In Python type 49*(1/49) == 1. What do you get and why do you get it?\nMathematically we know that \\(e^{\\log(3)}\\) should just give us 3 back. In Python type np.exp(np.log(3)) == 3. What do you get and why do you get it?\nCreate your own example of where Python gets something incorrect because of floating point arithmetic.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Numbers</span>"
    ]
  },
  {
    "objectID": "nmNumbers.html#sec-loss_of_significant_digits",
    "href": "nmNumbers.html#sec-loss_of_significant_digits",
    "title": "1¬† Numbers",
    "section": "1.4 Loss of Significant Digits",
    "text": "1.4 Loss of Significant Digits\nAs we have discussed, when representing real numbers by floating point numbers in the computer, rounding errors will usually occur. When doing a calculation with double-precision floating point numbers then the rounding error is only a tiny fraction of the actual number, so one might think that they really don‚Äôt matter. However, calculations usually involve a number of steps, and we saw in Exercise¬†1.3 that the rounding errors can accumulate and become quite noticeable after a large number of steps.\nBut the problem is even worse. If we are not careful, then the rounding errors can get magnified already after very few steps if we perform the steps in an unfortunate way. The following examples and exercises will illustrate this.\n\n\nExample 1.6 Consider the expression \\[\n(10^{10} + 0.123456789) - 10^{10}.\n\\] Mathematically, this is strictly equal to \\[\n(10^{10}  - 10^{10}) + 0.123456789=0.123456789.\n\\] However, let us evaluate this in Python:\n\n10**10 + 0.123456789 - 10**10\n\n0.12345695495605469\n\n\nOnly the first six digits after the decimal point were preserved, the other digits were replaced by something seemingly random. The reason should be clear. The computer makes a rounding error when it tries to store the \\(10000000000.123456789\\). This is known as the loss of significant digits. It occurs whenever you subtract two almost equal numbers from each other.\n\n\n\nExercise 1.20 üñã Consider these two mathematically equivalent ways to compute the same thing:\n\n\\((a + b) - c\\)\n\\(a + (b - c)\\)\n\n\nWhy might these give different results in floating-point arithmetic?\nIf \\(a\\) is very small compared to \\(b\\) and \\(c\\), which form would you expect to be more accurate? Why?\n\n\n\n\nExercise 1.21 üíª Consider the trigonometric identity\n\\[\n2\\sin^2(x/2) = 1 - \\cos(x).\n\\] It gives us two different methods to calculate the same quantity. Ask Python to evaluate both sides of the identity when \\(x=0.0001\\). Hint: as described in Section A.2.8, use import math so that you can then use math.cos() and math.sin(). Also remember that exponentiation in Python is represented by **.\nüí¨ What do you observe? If you want to calculate \\(1 - \\cos(x)\\) with the highest precision, which expression would you use? Discuss.\n\n\n\nExercise 1.22 üíª üñã üí¨ You know how to find the solutions to the quadratic equation \\[\na x^2+bx+c=0.\n\\] You know the quadratic formula. For the larger of the two solutions the formula is \\[\nx = \\frac{-b+\\sqrt{b^2-4ac}}{2a}.\n\\] Let‚Äôs assume that the parameters are given as \\[ a = 1,~~~b = 1000000, ~~~ c = 1.\\] Use the quadratic formula to find the larger of the two solutions, by coding the formula up in Python. You should get a solution slightly smaller than \\(-10^{-6}\\). Hint: use math.sqrt() to code up the square root.\nThen check whether your value for \\(x\\) really does solve the quadratic equation by evaluating \\(ax^2+bx+c\\) with your value of \\(x\\). You will notice that it does not work. Discuss the cause of the error.\nNow, on a piece of paper, rearrange the quadratic formula for the larger solution by multiplying both the numerator and denominator by \\(-b-\\sqrt{b^2-4ac}\\) and then simplify by multiplying out the resulting numerator. This should give you the alternative formula \\[\nx = \\frac{2c}{-b-\\sqrt{b^2-4ac}}.\n\\] Can you see why this expression will work better for the given parameter values? Again evaluate \\(x\\) with Python and then check it by substituting into the quadratic expression. What do you find?\n\n\n\nExercise 1.23 üíª üí¨ Google the term ‚Äúcatastrophic cancellation‚Äù to find more examples of this phenomenon of loss of significant digits.\n\n\nThese exercises will give much material for in-class discussion. The aim is to make you sensitive to the issue of loss of significant figures and the fact that expressions that are mathematically equal are not always computationally equal.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Numbers</span>"
    ]
  },
  {
    "objectID": "nmNumbers.html#exam-style-question",
    "href": "nmNumbers.html#exam-style-question",
    "title": "1¬† Numbers",
    "section": "1.5 Exam-style question",
    "text": "1.5 Exam-style question\nConsider a hypothetical ‚Äú8-bit Mini-Float‚Äù system based on the IEEE 754 standard. The 8 bits are allocated as follows:\n\nSign bit (\\(s\\)): 1 bit (Bit 7)\nExponent (\\(e\\)): 3 bits (Bits 6-4), using a bias of 3.\nMantissa (\\(m\\)): 4 bits (Bits 3-0), normalized with an implied leading 1.\n\nThe value of a number in this system is given by: \\(x = (-1)^s \\times (1.m)_2 \\times 2^{e-3}\\).\n\nHow is the machine precision \\(\\epsilon\\) defined. Give its value for this floating-point system. How is machine precision related to rounding errors? [3 marks]\nConvert the decimal number 13.5 into this 8-bit floating-point representation. Write your final answer as an 8-bit binary pattern (e.g., 0 101 1010). [4 marks]\nUsing this specific floating-point system, perform the addition of the number 13.5 (from part b) and 0.25.\n\nWrite 0.25 in this floating point system.\nPerform the addition simulating the hardware: align exponents, add significands.\nApply the rounding rule (‚ÄúRound to Nearest, Ties to Even‚Äù) to determine the final stored bits.\nWhat is the final decimal value stored by the system, and what is the absolute error compared to the exact mathematical sum? [4 marks]\n\nA student attempts to calculate the function \\(f(x) = \\sqrt{x^2 + 1} - x\\) for a very large value \\(x = 10^8\\).\n\nExplain why the result computed by a standard computer might be inaccurate (specifically naming the type of error).\nPropose an algebraically equivalent formula for \\(f(x)\\) that avoids this error. [3 marks]",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Numbers</span>"
    ]
  },
  {
    "objectID": "nmNumbers.html#sec-approximation_problems",
    "href": "nmNumbers.html#sec-approximation_problems",
    "title": "1¬† Numbers",
    "section": "1.6 Problems",
    "text": "1.6 Problems\nThese problem exercises will let you consolidate what you have learned so far and combine it with your Python coding skills, see Appendix A.\n\n\nExercise 1.24 (This problem is modified from (Greenbaum and Chartier 2012))\nüñã üíª In the 1999 film Office Space, a character creates a program that takes fractions of cents that are truncated in a bank‚Äôs transactions and deposits them to his own account. This idea has been attempted in the past and now banks look for this sort of thing. In this problem you will build a simulation of the program to see how long it takes to become a millionaire.\nAssumptions:\n\nAssume that you have access to 50,000 bank accounts.\nAssume that the account balances are uniformly distributed between $100 and $100,000.\nAssume that the annual interest rate on the accounts is 5% and the interest is compounded daily and added to the accounts, except that fractions of cents are truncated.\nAssume that your illegal account initially has a $0 balance.\n\nYour Tasks:\n\nExplain what the code below does.\n\nimport numpy as np\naccounts = 100 + (100000-100) * np.random.rand(50000,1);\naccounts = np.floor(100*accounts)/100;\n\nBy hand (no computer) write the mathematical steps necessary to increase the accounts by (5/365)% per day, truncate the accounts to the nearest penny, and add the truncated amount into an account titled ‚Äúillegal.‚Äù\nWrite code to complete your plan from part 2.\nUsing a while loop, iterate over your code until the illegal account has accumulated $1,000,000. How long does it take?\n\n\n\n\nExercise 1.25 (This problem is modified from (Greenbaum and Chartier 2012))\nüñã In the 1991 Gulf War, the Patriot missile defence system failed due to rounding error. The troubles stemmed from a computer that performed the tracking calculations with an internal clock whose integer values in tenths of a second were converted to seconds by multiplying by a 24-bit binary approximation to \\(\\frac{1}{10}\\): \\[\\begin{equation}\n0.1_{10} \\approx 0.00011001100110011001100_2.\n\\end{equation}\\]\n\nConvert the binary number above to a fraction by hand.\nThe approximation of \\(\\frac{1}{10}\\) given above is clearly not equal to \\(\\frac{1}{10}\\). What is the absolute error in this value?\nWhat is the time error, in seconds, after 100 hours of operation?\nDuring the 1991 war, a Scud missile travelled at approximately Mach 5 (3750 mph). Find the distance that the Scud missile would travel during the time error computed in part 3.\n\n\n\n\nExercise 1.26 (The Python Caret Operator) üñã üíª Now that you‚Äôre used to using Python to do some basic computations you are probably comfortable with the fact that the caret, ^, does NOT do exponentiation like it does in many other programming languages. But what does the caret operator do? That‚Äôs what we explore here.\n\nConsider the numbers \\(9\\) and \\(5\\). Write these numbers in binary representation. We are going to use four bits to represent each number (it is OK if the first bit happens to be zero). \\[\\begin{equation}\n\\begin{aligned} 9 &=& \\underline{\\hspace{0.2in}} \\, \\underline{\\hspace{0.2in}} \\, \\underline{\\hspace{0.2in}} \\, \\underline{\\hspace{0.2in}} \\\\ 5 &=& \\underline{\\hspace{0.2in}} \\, \\underline{\\hspace{0.2in}} \\, \\underline{\\hspace{0.2in}} \\, \\underline{\\hspace{0.2in}} \\end{aligned}\n\\end{equation}\\]\nNow go to Python and evaluate the expression 9^5. Convert Python‚Äôs answer to a binary representation (again using four bits).\nMake a conjecture: How do we go from the binary representations of \\(a\\) and \\(b\\) to the binary representation for Python‚Äôs a^b for numbers \\(a\\) and \\(b\\)? Test and verify your conjecture on several different examples and then write a few sentences explaining what the caret operator does in Python.\n\n\n\n\n\n\nGreenbaum, Anne, and Tim P. Chartier. 2012. Numerical Methods: Design, Analysis, and Computer Implementation of Algorithms. Princeton University Press.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Numbers</span>"
    ]
  },
  {
    "objectID": "nmFunctions.html",
    "href": "nmFunctions.html",
    "title": "2¬† Functions",
    "section": "",
    "text": "2.1 Polynomial Approximations\nHow does a computer understand a function like \\(f(x) = e^x\\) or \\(f(x) = \\sin(x)\\) or \\(f(x) = \\log(x)\\)? What happens under the hood, so to speak, when you ask a computer to do a computation with one of these functions? A computer is good at arithmetic operations, but working with transcendental functions like these, or really any other sufficiently complicated functions for that matter, is not something that comes naturally to a computer. What is actually happening under the hood is that the computer only approximates the functions.\nA class of functions that computers are very good at working with are polynomial functions. This is because to evaluate a polynomial function at any point we only need to addition and multiplication operations. In this section we will explore how we can use polynomial functions to approximate other functions.\nIn the previous 3 exercises you have built up some basic intuition for what we would want out of a mathematical operation that might build an approximation of a complicated function. What we have built is actually a way to get better and better approximations for functions out to pretty much any arbitrary accuracy that we like so long as we are near some anchor point (which we called \\(x_0\\) in the previous exercises).\nIn the next several problems you will unpack the polynomial approximations of \\(f(x) = e^x\\) and we will wrap the whole discussion with a little bit of formal mathematical language. Then we will examine other functions like \\(\\sin(x)\\) and \\(\\log(x)\\). One of the points of this whole discussion is to give you a little glimpse as to what is happening behind the scenes in scientific programming languages when you do computations with these functions. A bigger point is to start getting a feel for how we might go in reverse and approximate an unknown function out of much simpler parts. This last goal is one of the big takeaways from numerical analysis: we can mathematically model highly complicated functions out of fairly simple pieces.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "nmFunctions.html#sec-polynomial_approximations",
    "href": "nmFunctions.html#sec-polynomial_approximations",
    "title": "2¬† Functions",
    "section": "",
    "text": "Exercise 2.1 üí¨ In this exercise you are going to make a bit of a wish list for all of the things that a computer will do when approximating a function. We are going to complete the following sentence:\nIf we are going to approximate a smooth function \\(f(x)\\) near the point \\(x=x_0\\) with a simpler function \\(g(x)\\) then ‚Ä¶\n(I will get us started with the first two things that seems natural to wish for. The rest of the wish list is for you to complete.)\n\nthe functions \\(f(x)\\) and \\(g(x)\\) should agree at \\(x=x_0\\). In other words, \\(f(x_0) = g(x_0)\\)\nthe function \\(g(x)\\) should only involve addition, subtraction, multiplication, division, and integer exponents since computer are very good at those sorts of operations.\nif \\(f(x)\\) is increasing / decreasing near \\(x=x_0\\) then \\(g(x)\\) ‚Ä¶\nif \\(f(x)\\) is concave up / down near \\(x=x_0\\) then \\(g(x)\\)‚Ä¶\nif we zoom into plots of the functions \\(f(x)\\) and \\(g(x)\\) near \\(x=x_0\\) then ‚Ä¶\n‚Ä¶ is there anything else that you would add?\n\n\n\n\nExercise 2.2 üí¨ Discuss: Could a polynomial function with a high enough degree satisfy everything in the wish list from the previous problem? Explain your reasoning.\n\n\n\nExercise 2.3 üñã üéì Let us put some parts of the wish list into action. If \\(f(x)\\) is a differentiable function at \\(x=x_0\\) and if \\(g(x) = A + B (x-x_0) + C (x-x_0)^2 + D (x-x_0)^3\\) then\n\nWhat is the value of \\(A\\) such that \\(f(x_0) = g(x_0)\\)? (Hint: substitute \\(x=x_0\\) into the \\(g(x)\\) function)\nWhat is the value of \\(B\\) such that at \\(x_0\\) \\(f\\) and \\(g\\) have the same slope? In other words, what is the value of \\(B\\) such that \\(f'(x_0) = g'(x_0)\\)? (Hint: Start by taking the derivative of \\(g(x)\\))\nWhat is the value of \\(C\\) such that at \\(x_0\\) \\(f\\) and \\(g\\) have the same concavity? In other words, what is the value of \\(C\\) such that \\(f''(x_0) = g''(x_0)\\)?\nWhat is the value of \\(D\\) such that at \\(x_0\\) \\(f\\) and \\(g\\) have the same third derivative? In other words, what is the value of \\(D\\) such that \\(f'''(x_0) = g'''(x_0)\\)?\n\n\n\n\n\n\n\n2.1.1 Approximating the exponential function\n\nExercise 2.4 üñã What is Euler‚Äôs number \\(e\\)? You have been using this number often in Calculus and Differential Equations. Do you know the decimal approximation for this number? Moreover, is there a way that we could approximate something like \\(\\sqrt{e} = e^{0.5}\\) or \\(e^{-1}\\) without actually having access to the full decimal expansion?\nFor all of the questions below let us work with the function \\(f(x) = e^x\\).\n\nThe function \\(g_0(x) = 1\\) matches \\(f(x) = e^x\\) exactly at the point \\(x=0\\) since \\(f(0) = e^0 = 1\\). Furthermore if \\(x\\) is very very close to \\(0\\) then the functions \\(f(x)\\) and \\(g_0(x)\\) are really close to each other. Hence we could say that \\(g_0(x) = 1\\) is an approximation of the function \\(f(x) = e^x\\) for values of \\(x\\) very very close to \\(x=0\\). Admittedly, though, it is probably pretty clear that this is a horrible approximation for any \\(x\\) just a little bit away from \\(x=0\\).\nLet us get a better approximation. What if we insist that our approximation \\(g_1(x)\\) matches \\(f(x) = e^x\\) exactly at \\(x=0\\) and ALSO has exactly the same first derivative as \\(f(x)\\) at \\(x=0\\).\n\nWhat is the first derivative of \\(f(x)\\)?\nWhat is \\(f'(0)\\)?\nUse the point-slope form of a line to write the equation of the function \\(g_1(x)\\) that goes through the point \\((0,f(0))\\) and has slope \\(f'(0)\\). Recall from algebra that the point-slope form of a line is \\(y = f(x_0) + m(x-x_0).\\) In this case we are taking \\(x_0 = 0\\) so we are using the formula \\(g_1(x) = f(0) + f'(0) (x-0)\\) to get the equation of the line.\n\nüíª Write Python code to build a plot like Figure¬†2.1. This plot shows \\(f(x) = e^x\\), our first approximation \\(g_0(x) = 1\\) and our second approximation \\(g_1(x) = 1+x\\). You may want to look at Example¬†A.43 in the Python chapter for a refresher on how to build plots containing the graphs of several functions. If you need a hint on how to plot the function \\(g_0(x)\\) since it is a constant function, take a look at the np.ones_like() function in Example¬†A.40.\n\n\n\n\n\n\n\n\n\nFigure¬†2.1: The first two polynomial approximations of the exponential function.\n\n\n\n\n\n\n\nExercise 2.5 üñã Let us extend the idea from the previous problem to much better approximations of the function \\(f(x) = e^x\\).\n\nLet us build a function \\(g_2(x)\\) that matches \\(f(x)\\) exactly at \\(x=0\\), has exactly the same first derivative as \\(f(x)\\) at \\(x=0\\), AND has exactly the same second derivative as \\(f(x)\\) at \\(x=0\\). To do this we will use a quadratic function. For a quadratic approximation of a function we just take a slight extension to the point-slope form of a line and use the equation \\[\\begin{equation}\ng_2(x) = f(x_0) + f'(x_0) (x-x_0) + \\frac{f''(x_0)}{2} (x-x_0)^2.\n\\end{equation}\\] In this case we are using \\(x_0 = 0\\) so the quadratic approximation function looks like \\[\\begin{equation}\ng_2(x) = f(0) + f'(0) x + \\frac{f''(0)}{2} x^2.\n\\end{equation}\\]\n\nFind the quadratic approximation for \\(f(x) = e^x\\).\nüíª Add your new function to the plot you created in the previous problem.\n\nLet us keep going!! Next we will do a cubic approximation. A cubic approximation takes the form \\[\\begin{equation}\ng_3(x) = f(x_0) + f'(0) (x-x_0) + \\frac{f''(0)}{2}(x-x_0)^2 + \\frac{f'''(0)}{3!}(x-x_0)^3\n\\end{equation}\\]\n\nFind the cubic approximation for \\(f(x) = e^x\\).\nHow do we know that this function matches the first, second, and third derivatives of \\(f(x)\\) at \\(x=0\\)? What‚Äôs the deal with the \\(3!\\) on the cubic term?\nüíª Add your function to the plot.\n\n\n\n\n\nExercise 2.6 üíª Write a function that takes the arguments x and n and returns the nth order Taylor series approximation of \\(f(x) = e^x\\). To remind yourself of how functions are defined in Python, you may want to look at Section A.2.6 in the Python chapter. You will also need a loop, see Section A.2.5. Please start from the following skeleton and put your code where it says ‚ÄúTODO‚Äù.\ndef exp_approx(x, n):\n    \"\"\"\n    Computes the nth order Taylor series approximation of e^x at x=0.\n\n    Parameters:\n    x (float): The value at which to evaluate the approximation.\n    n (int): The order of the Taylor series expansion.\n\n    Returns:\n    float: The nth order Taylor approximation of e^x.\n    \"\"\"\n    if n &lt; 0:\n        raise ValueError(\"n must be at least 0\")\n    # Start with zero-order approximation\n    approximation = 1.0\n    # Add higher-order terms\n    for i in range(1, n + 1):\n        approximation += # TODO: calculate the i'th term of the Taylor series\n    return approximation\nMake sure you perfectly understand the code. Use the AI to explain everything to you in detail. You can select parts of the code and ask the AI questions like ‚ÄúWhy do we need to use range(1, n + 1)?‚Äù and ‚ÄúWhat does approximation += do?‚Äù\n\n\n\nExercise 2.7 üéì üíª Use the function exp_approx that you have built in Exercise¬†2.6 to approximate \\(\\frac{1}{e} = e^{-1}\\). Check the accuracy of your answer using np.exp(-1) in Python.\n\n\n\nExercise 2.8 üíª üí¨ Brainstorm within your group to see if you can make your function more efficient by writing it without using exponentiation and the factorial function, coding all multiplications and divisions explicitly. Try to minimise the number of arithmetic operations that you need to perform? Can you make it so that it only needs \\(n-1\\) multiplications, \\(n-1\\) divisions and \\(n\\) additions?\nUse the %timeit magic command to measure how fast your exp_approx function is. In a new code cell, run\n%timeit exp_approx(1.5, 100)\nThis will run the function multiple times and give you an estimate of the execution time.\n\n\n\n\n2.1.2 Taylor Series\nWhat we have been exploring so far in this section is the Taylor Series of a function.\n\nDefinition 2.1 (Taylor Series) If \\(f(x)\\) is an infinitely differentiable function at the point \\(x_0\\) then \\[\\begin{equation}\nf(x) = f(x_0) + f'(x_0)(x-x_0) + \\frac{f''(x_0)}{2}(x-x_0)^2 + \\cdots \\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n + \\cdots\n\\end{equation}\\] for any reasonably small interval around \\(x_0\\). The infinite polynomial expansion is called the Taylor Series of the function \\(f(x)\\). Taylor Series are named for the mathematician Brook Taylor.\n\n\nThe Taylor Series of a function is often written with summation notation as \\[\\begin{equation}\nf(x) = \\sum_{k=0}^\\infty \\frac{f^{(k)}(x_0)}{k!} (x-x_0)^k.\n\\end{equation}\\] Do not let the notation scare you. In a Taylor Series you are just saying: give me a function that\n\nmatches \\(f(x)\\) at \\(x=x_0\\) exactly,\nmatches \\(f'(x)\\) at \\(x=x_0\\) exactly,\nmatches \\(f''(x)\\) at \\(x=x_0\\) exactly,\nmatches \\(f'''(x)\\) at \\(x=x_0\\) exactly,\netc.\n\n(Take a moment and make sure that the summation notation makes sense to you.)\nMoreover, Taylor Series are built out of the easiest types of functions: polynomials. Computers are rather good at doing computations with addition, subtraction, multiplication, division, and integer exponents, so Taylor Series are a natural way to express functions in a computer. The down side is that we can only get true equality in the Taylor Series if we have infinitely many terms in the series. A computer cannot do infinitely many computations. So, in practice, we truncate Taylor Series after many terms and think of the new polynomial function as being close enough to the actual function so far as we do not stray too far from the anchor \\(x_0\\).\n\n\nExercise 2.9 üñã Do all of the calculations to show that the Taylor Series centred at \\(x_0 = 0\\) for the function \\(f(x) = \\sin(x)\\) is indeed \\[\\begin{equation}\n\\sin(x) = x - \\frac{x^3}{3!} + \\frac{x^5}{5!} - \\frac{x^7}{7!} + \\cdots .\n\\end{equation}\\]\n\n\n\nExercise 2.10 üíª Write a Pyton function sin_approx(x, n) that computes the \\(n\\)th order Taylor Series approximation of \\(\\sin(x)\\) centred at \\(x_0 = 0\\). Test your function by comparing its output to np.sin(x) for a few values of \\(x\\) and \\(n\\). Use it to make a plot of the first three approximations for \\(x\\) in the range \\([-\\pi, \\pi]\\) similar to the plots you made for approximations of \\(e^x\\) above.\n\n\n\nExercise 2.11 üñã Let us compute a Taylor Series that is not centred at \\(x_0 = 0\\). For example, let us approximate the function \\(f(x) = \\log (x)\\) near \\(x_0 = 1\\). Near the point \\(x_0 = 1\\), the Taylor Series approximation will take the form \\[\\begin{equation}\nf(x) = f\\left( 1 \\right) + f'\\left( 1 \\right)\\left( x - 1 \\right) + \\frac{f''\\left( 1 \\right)}{2!}\\left( x - 1 \\right)^2 + \\frac{f'''\\left( 1 \\right)}{3!}\\left( x - 1 \\right)^3 + \\cdots\n\\end{equation}\\]\nWrite the first several terms of the Taylor Series for \\(f(x) = \\log x\\) centred at \\(x_0 = 1\\) until you get a feel for the pattern.\n\n\n\nExercise 2.12 üíª Write a Pyton function log_approx(x, n) that computes the \\(n\\)th order Taylor Series approximation of \\(\\log(x)\\) centred at \\(x_0 = 1\\). Use it to build the plot below showing the approximations.\n\n\n\n\n\n\n\n\nFigure¬†2.2: Taylor series approximation of the logarithm.\n\n\n\n\n\n\n\n\nExample 2.1 Let us conclude this brief section by examining an interesting example. Consider the function \\[\\begin{equation}\nf(x) = \\frac{1}{1-x}.\n\\end{equation}\\] If we build a Taylor Series centred at \\(x_0 = 0\\) it is not too hard to show that we get \\[\\begin{equation}\nf(x) = 1 + x + x^2 + x^3 + x^4 + x^5 + \\cdots\n\\end{equation}\\] (you should stop now and verify this!). However, if we plot the function \\(f(x)\\) along with several successive approximations for \\(f(x)\\) we find that beyond \\(x=1\\) we do not get the correct behaviour of the function (see Figure¬†2.3). More specifically, we cannot get the Taylor Series to change behaviour across the vertical asymptote of the function at \\(x=1\\). This example is meant to point out the fact that a Taylor Series will only ever make sense near the point at which you centre the expansion. For the function \\(f(x) = \\frac{1}{1-x}\\) centred at \\(x_0 = 0\\) we can only get good approximations within the interval \\(x \\in (-1,1)\\) and no further.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# build the x and y values\nx = np.linspace(-1,2,101)\ny0 = 1/(1-x)\ny1 = 1 + 0*x\ny2 = 1 + x\ny3 = 1 + x + x**2\ny4 = 1 + x + x**2 + x**3 + x**4 + x**5 + x**6 + x**7 + x**8\n\n# plot each of the functions \nplt.plot(x, y0, 'r-', label=r\"$f(x)=1/(1-x)$\")\nplt.plot(x, y1, 'c-', label=r\"constant\")\nplt.plot(x, y2, 'g:', label=r\"linear\")\nplt.plot(x, y3, 'b-.', label=r\"quadratic\")\nplt.plot(x, y4, 'k--', label=r\"8th order\")\n\n# set limits on the y axis\nplt.ylim(-3,5)\n\n# put in a grid, legend, title, and axis labels\nplt.grid()\nplt.legend()\nplt.title(r\"Taylor approximations of $f(x)=\\frac{1}{1-x}$ around $x=0$\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†2.3: Several Taylor Series approximations of the function \\(f(x) = 1/(1-x)\\).\n\n\n\n\n\n\n\nIn the previous example we saw that we cannot always get approximations from Taylor Series that are good everywhere. For every Taylor Series there is a domain of convergence where the Taylor Series actually makes sense and gives good approximations. It is beyond the scope of this section to give all of the details for finding the domain of convergence for a Taylor Series. You have done that in your first-year Calculus module. However a good heuristic is to observe that a Taylor Series will only give reasonable approximations of a function from the centre of the series to the nearest asymptote. The domain of convergence is typically symmetric about the centre as well. For example:\n\nIf we were to build a Taylor Series approximation for the function \\(f(x) = \\log(x)\\) centred at the point \\(x_0 = 1\\) then the domain of convergence should be \\(x \\in (0,2)\\) since there is a vertical asymptote for the natural logarithm function at \\(x=0\\).\nIf we were to build a Taylor Series approximation for the function \\(f(x) = \\frac{5}{2x-3}\\) centred at the point \\(x_0 = 4\\) then the domain of convergence should be \\(x \\in (1.5, 6.5)\\) since there is a vertical asymptote at \\(x=1.5\\) and the distance from \\(x_0 = 4\\) to \\(x=1.5\\) is 2.5 units.\nIf we were to build a Taylor Series approximation for the function \\(f(x) = \\frac{1}{1+x^2}\\) centred at the point \\(x_0 = 0\\) then the domain of convergence should be \\(x \\in (-1,1)\\). This may seem quite odd (and perhaps quite surprising!) but let us think about where the nearest asymptote might be. To find the asymptote we need to solve \\(1+x^2 = 0\\) but this gives us the values \\(x = \\pm i\\). In the complex plane, the numbers \\(i\\) and \\(-i\\) are 1 unit away from \\(x_0 = 0\\), so the ‚Äúasymptote‚Äù is not visible in a real-valued plot but it is still only one unit away. Hence the domain of convergence is \\(x \\in (-1,1)\\). You may want to pause now and build some plots to show yourself that this indeed appears to be true.\n\nOf course you learned all this and more in your first-year Calculus but I hope it was fun to now rediscover these things yourself. In your Calculus module it was probably not stressed how fundamental Taylor series are to doing numerical computations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "nmFunctions.html#sec-truncation_error",
    "href": "nmFunctions.html#sec-truncation_error",
    "title": "2¬† Functions",
    "section": "2.2 Truncation Error",
    "text": "2.2 Truncation Error\nThe great thing about Taylor Series is that they allow for the representation of potentially very complicated functions as polynomials ‚Äì and polynomials are easily dealt with on a computer since they involve only addition, subtraction, multiplication, division, and integer powers. The down side is that the order of the polynomial is infinite. Hence, every time we use a Taylor series on a computer, what we are actually going to be using is a Truncated Taylor Series where we only take a finite number of terms. The idea here is simple in principle:\n\nIf a function \\(f(x)\\) has a Taylor Series representation it can be written as an infinite sum.\nComputers cannot do infinite sums.\nSo stop the sum at some point \\(n\\) and throw away the rest of the infinite sum.\nNow \\(f(x)\\) is approximated by some finite sum so long as you stay pretty close to \\(x = x_0\\),\nand everything that we just chopped off of the end is called the remainder for the finite sum.\n\nLet us be a bit more concrete about it. The Taylor Series for \\(f(x) = e^x\\) centred at \\(x_0 = 0\\) is \\[\\begin{equation}\ne^x = 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\frac{x^4}{4!} + \\cdots.\n\\end{equation}\\]\nWhen we truncate this series at order \\(n\\), we separate it into an approximation and a remainder: \\[\\begin{equation}\ne^x = \\underbrace{1 + x + \\frac{x^2}{2!} + \\cdots + \\frac{x^n}{n!}}_{\\text{$n^{th}$ order approximation}} + \\underbrace{\\frac{x^{n+1}}{(n+1)!} + \\frac{x^{n+2}}{(n+2)!} + \\cdots}_{\\text{remainder}}.\n\\end{equation}\\]\nFor small values of \\(x\\) near \\(x_0 = 0\\), the largest term in the remainder is \\(\\frac{x^{n+1}}{(n+1)!}\\), since higher powers of \\(x\\) become progressively smaller. We use Big-O notation to express this: \\[\\begin{equation}\ne^x \\approx 1 + x + \\frac{x^2}{2!} + \\cdots + \\frac{x^n}{n!} + \\mathcal{O}(x^{n+1}),\n\\end{equation}\\] where the notation \\(\\mathcal{O}(x^{n+1})\\) (read ‚ÄúBig-O of \\(x^{n+1}\\)‚Äù) signifies that the error is bounded by \\(C|x|^{n+1}\\) for some constant \\(C\\) as \\(x \\to 0\\). This indicates that the error scales like \\(x^{n+1}\\) for values of \\(x\\) near the center \\(x_0=0\\).\nFor example:\n\n\\(0^{th}\\) order: \\(e^x \\approx 1 + \\mathcal{O}(x)\\)\n\\(1^{st}\\) order: \\(e^x \\approx 1 + x + \\mathcal{O}(x^2)\\)\n\\(2^{nd}\\) order: \\(e^x \\approx 1 + x + \\frac{x^2}{2} + \\mathcal{O}(x^3)\\)\n\nKeep in mind that this sort of analysis is only good for values of \\(x\\) that are very close to the centre of the Taylor Series. If you are making approximations that are too far away then all bets are off.\n\n\nExercise 2.13 üéì üíª Now make the previous discussion a bit more concrete. You know the Taylor Series for \\(f(x) = e^x\\) around \\(x=0\\) quite well at this point so use it to approximate the values of \\(f(0.1) = e^{0.1}\\) and \\(f(0.2)=e^{0.2}\\) by truncating the Taylor series at different orders. Because \\(x=0.1\\) and \\(x=0.2\\) are pretty close to the centre of the Taylor Series \\(x_0 = 0\\), this sort of approximation is reasonable.\nThen compare your approximate values to Python‚Äôs values \\(f(0.1)=e^{0.1} \\approx\\) np.exp(0.1) \\(=1.1051709180756477\\) and \\(f(0.2)=e^{0.2} \\approx\\) np.exp(0.2) \\(=1.2214027581601699\\) to calculate the truncation errors \\(\\epsilon_n(0.1)=|f(0.1)-f_n(0.1)|\\) and \\(\\epsilon_n(0.2)=|f(0.2)-f_n(0.2)|\\).\nFill in the blanks in the table. If you like, you can copy and paste the code and extend it to fill in the missing rows. For a bit of explanation of the syntax of the print commands see Example¬†A.20 but for more detailed information ask Gemini.\n\n\nCode\nimport numpy as np\n\n# Create table header\nprint(\n    f\"{'Order n':&lt;8} | {'f_n(0.1)':&lt;15} | {'Œµ_n(0.1)':&lt;15} | \"\n    f\"{'f_n(0.2)':&lt;15} | {'Œµ_n(0.2)':&lt;15} \"\n)\nprint(\"-\" * 80)\n\n# Fill in n=0 row\nf_n = lambda x: 1\ne_n = lambda x: abs(np.exp(x) - f_n(x))\nprint(\n    f\"{0:&lt;8} | {f_n(0.1):&lt;15.10g} | {e_n(0.1):&lt;15.10g} | \"\n    f\"{f_n(0.2):&lt;15.10g} | {e_n(0.2):&lt;15.10g} \"\n)\n\n# Fill in n=1 row\nf_n = lambda x: 1 + x\ne_n = lambda x: abs(np.exp(x) - f_n(x))\nprint(\n    f\"{1:&lt;8} | {f_n(0.1):&lt;15.10g} | {e_n(0.1):&lt;15.10g} | \"\n    f\"{f_n(0.2):&lt;15.10g} | {e_n(0.2):&lt;15.10g} \"\n)\n\n# Fill in more rows.\nfor n in range(2,6):\n    # TODO. You can use your `exp_approx()` function here.\n    print(f\"{n:&lt;8} | {'':&lt;15} | {'':&lt;15} | {'':&lt;15} | {'':&lt;15} \")\n\n\nOrder n  | f_n(0.1)        | Œµ_n(0.1)        | f_n(0.2)        | Œµ_n(0.2)        \n--------------------------------------------------------------------------------\n0        | 1               | 0.1051709181    | 1               | 0.2214027582    \n1        | 1.1             | 0.005170918076  | 1.2             | 0.02140275816   \n2        |                 |                 |                 |                 \n3        |                 |                 |                 |                 \n4        |                 |                 |                 |                 \n5        |                 |                 |                 |                 \n\n\nYou will find that, as expected, the truncation errors \\(\\epsilon_n(x)\\) decrease with \\(n\\) but increase with \\(x\\).\n\n\n\nExercise 2.14 üíª To investigate the dependence of the truncation error \\(\\epsilon_n(x)\\) on \\(n\\) and \\(x\\) a bit more, add an extra column to the table from the previous exercise with the ratio \\(\\epsilon_n(0.2) / \\epsilon_n(0.1)\\).\n\n\nOrder n  | Œµ_n(0.1)        | Œµ_n(0.2)        |  Œµ_n(0.2) / Œµ_n(0.1)\n--------------------------------------------------------------------------------\n0        | 0.1051709181    | 0.2214027582    | 2.105170918     \n1        | 0.005170918076  | 0.02140275816   | 4.139063479     \n2        |                 |                 |                \n3        |                 |                 |                \n4        |                 |                 |                \n5        |                 |                 |                \n\n\nüí¨ Formulate a conjecture about how \\(\\epsilon_n\\) changes as \\(x\\) changes.\n\n\n\nExercise 2.15 üíª To test your conjecture, examine the truncation error for the sine function near \\(x_0 = 0\\). You know that the sine function has the Taylor Series centred at \\(x_0 = 0\\) as \\[\\begin{equation}\nf(x) = \\sin(x) = x - \\frac{x^3}{3!} + \\frac{x^5}{5!} - \\frac{x^7}{7!} + \\cdots.\n\\end{equation}\\] So there are only approximations of odd order. Use the truncated Taylor series to approximate \\(f(0.1)=\\sin(0.1)\\) and \\(f(0.2)=\\sin(0.2)\\) and use Python‚Äôs values np.sin(0.1) and np.sin(0.2) to calculate the truncation errors \\(\\epsilon_n(0.1)=|f(0.1)-f_n(0.1)|\\) and \\(\\epsilon_n(0.2)=|f(0.2)-f_n(0.2)|\\).\nComplete the following table:\n\n\nOrder n  | Œµ_n(0.1)        | Œµ_n(0.2)        |  Œµ_n(0.2) / Œµ_n(0.1)      \n--------------------------------------------------------------------------------\n1        | 0.0001665833532 | 0.001330669205  | 7.988008283               \n3        |                 |                 |                          \n5        |                 |                 |                          \n7        |                 |                 |                          \n9        |                 |                 |                          \n\n\nTo learn how you can loop over only odd integers in Python, see Example¬†A.7.\nüí¨ Did these results force you to revise your conjecture of how \\(\\epsilon_n\\) changes as \\(x\\) changes?\nThe entry in the last row of the table will almost certainly not agree with your conjecture. That is okay! That discrepancy has a different explanation. Can you figure out what it is? Hint: Think about the discussion of machine precision in Chapter 1.\n\n\n\nExercise 2.16 üéì üíª Perform another check of your conjecture by approximating \\(\\log(1.02)\\) and \\(\\log(1.1)\\) from truncations of the Taylor series around \\(x=1\\): \\[\n\\log(1+x) = x - \\frac{x^2}{2} + \\frac{x^3}{3} - \\frac{x^4}{4} + \\frac{x^5}{5} - \\cdots.\n\\]\n\n\n\nExercise 2.17 üí¨ üñã Write down your groups‚Äôs observations about how the truncation error changes as \\(x\\) changes. Explain this in terms of the form of the remainder of the truncated Taylor series.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "nmFunctions.html#exam-style-question",
    "href": "nmFunctions.html#exam-style-question",
    "title": "2¬† Functions",
    "section": "2.3 Exam-style question",
    "text": "2.3 Exam-style question\nYou know the Taylor Series expansion for \\(f(x) = \\sin(x)\\) centred at \\(x_0 = 0\\): \\[\\sin(x) = x - \\frac{x^3}{3!} + \\frac{x^5}{5!} - \\frac{x^7}{7!} + \\cdots.\\]\n\nExplain why there is no \\(x^2\\) term in this Taylor Series expansion. [2 marks]\nComplete the Python code to calculate the Taylor expansion for \\(f(x) = \\sin(x)\\) centred at \\(x_0 = 0\\) up to order \\(n\\). [4 marks]\n\n\ndef sin_taylor(x, n):\n    \"\"\"\n    Calculate the Taylor expansion for f(x) = sin(x) centred at x_0 = 0 up to order n.\n    \"\"\"\n    ....\n    for i in range(...):\n        ....\n    \n    return result\n\n\nIf we only keep terms up to \\(x^3\\) in this Taylor Series expansion, what is the truncation error in Big-O notation for the approximation? [2 marks]\nIf we use the same approximation to calculate \\(\\sin(0.05)\\) instead of \\(\\sin(0.1)\\), by what factor do we expect the truncation error to decrease? [2 marks]",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "nmFunctions.html#sec-nn",
    "href": "nmFunctions.html#sec-nn",
    "title": "2¬† Functions",
    "section": "2.4 Neural Networks",
    "text": "2.4 Neural Networks\nSo far we have discussed approximating functions with polynomials, and in particular with Taylor Series. However there are other families of functions that can be used to approximate an arbitrary function. One of them you have already met in your first year: Fourier series. Another one that is usually discussed in a course on Numerical Analysis is splines. In this module we will not discuss these families of functions. Our aim in this module is not to be exhaustive, but to get the fundamental ideas across, so that you will be well equipped to acquire further knowledge on the topic later on.\nThere is however a family of functions that has recently become very popular in machine learning: neural networks. This section guides you through exercises to obtain a good intuitive understanding of neural networks.\nIt may well be that by the time you reach this point in the learning guide, you will not have much time left this week. For that reason the material on neural networks is not examinable. You are under no pressure to work through this material. However, if you do have the time and interest, I would encourage you to do so.\nSimilar to how a polynomial \\(p(x)\\) is determined by giving the coefficients in front of the powers of \\(x\\), a neural network is determined by giving a set of parameters, called weights and biases. In this section you will explore how the weights and biases determine the neural network function.\n\n\n2.4.1 A Single Neuron\nLet us start by building up the components of a neural network one piece at a time. The fundamental building block is called a neuron.\n\nExercise 2.18 üñã Consider the simple function \\[\\begin{equation}\nf(x) = \\max(0, x).\n\\end{equation}\\] The function \\(g(x) = \\max(0, x)\\) is called the Rectified Linear Unit or ReLU for short.\n\nBy hand, sketch the graph of this function for \\(x \\in [-3, 3]\\).\nWhat is the derivative of \\(f(x)\\) for \\(x &gt; 0\\)? What about for \\(x &lt; 0\\)? What happens at \\(x = 0\\)?\n\n\n\nThe ReLU function is an example of what is called an activation function in neural networks. The idea is that the neuron ‚Äúactivates‚Äù (produces a non-zero output) only when the input exceeds a certain threshold.\n\nExercise 2.19 üñã Now let us make our neuron a bit more interesting by allowing it to have adjustable parameters.\nConsider the function \\[\\begin{equation}\nh(x) = \\max(0, w x + b)\n\\end{equation}\\] where \\(w\\) and \\(b\\) are parameters that we can choose. These are called the weight and bias respectively.\n\nBy hand, sketch the graph of \\(h(x)\\) for \\(x \\in [-3, 3]\\) for each of the following parameter values:\n\n\\(w = 1, b = 0\\)\n\\(w = 2, b = 0\\)\n\\(w = 1, b = 1\\)\n\\(w = 1, b = -1\\)\n\\(w = -1, b = 0\\)\n\nDescribe in words what the weight \\(w\\) controls about the function \\(h(x)\\).\nDescribe in words what the bias \\(b\\) controls about the function \\(h(x)\\).\nFor which values of \\(x\\) is the function ‚Äúactive‚Äù (i.e., non-zero) when \\(w = 1\\) and \\(b = -1\\)?\n\n\n\n\nExercise 2.20 üñã So far we have been working with a neuron that takes a single input \\(x\\). In many applications, we want to work with functions of multiple variables.\nSuppose we have two inputs \\(x_1\\) and \\(x_2\\), and we define a neuron as \\[\\begin{equation}\nh(x_1, x_2) = \\max(0, w_1 x_1 + w_2 x_2 + b)\n\\end{equation}\\] where \\(w_1, w_2\\) are weights and \\(b\\) is a bias.\n\nWhat is the output of this neuron when \\(x_1 = 1, x_2 = 2\\), using the parameters \\(w_1 = 1, w_2 = -1, b = 0\\)?\nThe expression \\(w_1 x_1 + w_2 x_2 + b = 0\\) defines a line in the \\((x_1, x_2)\\) plane. For the parameters in part 1, sketch this line in the region \\(x_1 \\in [-2, 2], x_2 \\in [-2, 2]\\).\nOn which side of this line is the neuron ‚Äúactive‚Äù (produces non-zero outputs)? Shade that area in your sketch from part 2.\n\n\n\n\n\n2.4.2 Combining Neurons into a Layer\nA single neuron can detect when the input crosses a particular threshold. But to approximate more complex functions, we need to combine multiple neurons together. Let us explore this idea.\n\nExercise 2.21 üñã Suppose we have two neurons, both taking the same input \\(x\\), but with different weights and biases: \\[\\begin{align}\nh_1(x) &= \\max(0, w_1 x + b_1) \\\\\nh_2(x) &= \\max(0, w_2 x + b_2)\n\\end{align}\\]\nConsider the specific case where \\(w_1 = 1, b_1 = -1\\) and \\(w_2 = -1, b_2 = 1\\).\n\nFor what values of \\(x\\) is \\(h_1(x)\\) active (non-zero)?\nFor what values of \\(x\\) is \\(h_2(x)\\) active (non-zero)?\nBy hand, sketch the graphs of both \\(h_1(x)\\) and \\(h_2(x)\\) on the same axes for \\(x \\in [-2, 2]\\).\nNow consider the sum \\(h_1(x) + h_2(x)\\). By hand, sketch the graph of this function. Describe its shape.\nWhat is the minimum value of \\(h_1(x) + h_2(x)\\)? At what value of \\(x\\) does this minimum occur?\n\n\n\n\nExercise 2.22 üñã Rather than just adding two ReLU neurons, consider the weighted combination: \\[\\begin{equation}\nf(x) = c_1 h_1(x) + c_2 h_2(x)\n\\end{equation}\\] where \\(c_1\\) and \\(c_2\\) are output weights that can be positive or negative.\nLet \\(h_1(x) = \\max(0, x)\\) and \\(h_2(x) = \\max(0, x - 1)\\).\n\nWhat is \\(f(x) = h_1(x) - 2h_2(x)\\) for \\(x &lt; 0\\)?\nWhat is \\(f(x) = h_1(x) - 2h_2(x)\\) for \\(0 \\leq x &lt; 1\\)?\nWhat is \\(f(x) = h_1(x) - 2h_2(x)\\) for \\(x \\geq 1\\)?\n\nBy hand, sketch the graph of this function.\n\n\n\nExercise 2.23 üñã In the previous exercise you created a function with a triangular bump but the function continued decreasing to negative values beyond \\(x=2\\). Now see if you can find a way to combine three ReLU neurons to create a triangular bump function that:\n\nis zero for \\(x &lt; 0\\) and \\(x &gt; 2\\)\nequals \\(x\\) for \\(0 \\leq x \\leq 1\\)\nequals \\(2-x\\) for \\(1 &lt; x \\leq 2\\)\n\n\nWrite down the weights \\(w_1, b_1, w_2, b_2, w_3, b_3\\) and output weights \\(c_1, c_2\\) and \\(c_3\\) that produce the desired function. Hint: You need \\(h_3\\) to ‚Äúcancel out‚Äù the negative values after \\(x=2\\).\nSketch the graph of your function to verify that it works.\nHow could you modify the output weights to make the bump twice as tall?\nHow could you modify the weights and biases to shift the bump so that it is centred at \\(x = 5\\) instead of \\(x = 1\\)?\n\n\n\n\n\n2.4.3 A Two-Layer Neural Network\nIn the previous exercises, you discovered that by combining multiple neurons (each with a ReLU activation), we can build flexible function approximations. Let us now formalise this into what is called a two-layer neural network or single hidden layer network.\nThe structure is as follows:\n\nInput: We start with an input value \\(x\\).\nHidden Layer: We apply \\(n\\) neurons to the input, creating \\(n\\) hidden values: \\[\\begin{equation}\nh_i(x) = \\max(0, w_i x + b_i) \\quad \\text{for } i = 1, 2, \\ldots, n\n\\end{equation}\\] The parameters \\(w_i\\) and \\(b_i\\) are called the weights and biases of the hidden layer.\nOutput Layer: We combine the hidden values using output weights: \\[\\begin{equation}\nf(x) = c_1 h_1(x) + c_2 h_2(x) + \\cdots + c_n h_n(x) + c_0\n\\end{equation}\\] where \\(c_0, c_1, \\ldots, c_n\\) are the output layer parameters.\n\nWe can write this more compactly using summation notation: \\[\\begin{equation}\nf(x) = \\sum_{i=1}^{n} c_i \\max(0, w_i x + b_i) + c_0.\n\\end{equation}\\]\n\n\n\n\n\n\n\n\n\nThis function \\(f(x)\\) is determined by the parameters:\n\nHidden layer weights: \\(w_1, w_2, \\ldots, w_n\\)\nHidden layer biases: \\(b_1, b_2, \\ldots, b_n\\)\n\nOutput weights: \\(c_1, c_2, \\ldots, c_n\\)\nOutput bias: \\(c_0\\)\n\nThat gives us a total of \\(3n + 1\\) parameters to work with.\nNote that, due to the simple linear nature of the ReLU activation function that we have chosen to use here, we can absorb the weights of the hidden layer neurons into a rescaling of the of the output weights and the biases. This would not be true for more general activation functions.\nSo we choose to set all hidden weights to \\(1\\) and thus work with \\[\\begin{equation}\nf_{nn}(x) = \\sum_{i=1}^{n} c_i \\max(0, x + b_i) + c_0\n\\end{equation}\\]\nThe following Python code implements such a neural network. Make sure you understand the code.\n\nimport numpy as np\n\ndef two_layer_network(x, biases, output_weights, output_bias=0):\n    \"\"\"\n    Evaluates a two-layer neural network approximation.\n    \n    Parameters:\n    x (float): The value at which to evaluate the network.\n    biases (list or array): Biases for hidden layer\n        These determine the knot points: x_i = -b_i\n    output_weights (list or array) : Weights for output layer\n    output_bias (float): Bias for output layer (default: 0)\n    \n    Returns:\n    --------\n    float: The output of the neural network.\n    \"\"\"\n    f_nn = output_bias * np.ones_like(x)  # bias term\n    for i in range(len(biases)):\n        f_nn += output_weights[i] * np.maximum(0, x + biases[i])\n    return f_nn\n\n\n\nExercise 2.24 üí¨ Explain in your own words why a two-layer neural network is flexible enough to approximate many different functions. How do the output weights control the shape of the approximation?\nHow is this similar to polynomial approximation? How is it different?\n\n\n\nExercise 2.25 üíª In this exercise we will approximate a smooth function with a neural network using a simple but powerful idea: we can create a piecewise linear approximation by using a sum of ReLU neurons, each of which activates at a new point. Each ReLU neuron that activates at a new point changes the slope of our piecewise linear approximation.\nConsider approximating \\(f(x) = \\sin(x)\\) on the interval \\([0, \\pi]\\) using a two-layer neural network. We‚Äôll place ReLU neurons at several ‚Äúknot points‚Äù along the curve, and each neuron will adjust the slope.\n\nUnderstanding slope changes: Suppose we use knot points at \\(x_1 = -b_1 = 0, x_2 = -b_2 = \\pi/2\\).\n\nFor \\(x \\in [0, \\pi/2)\\): What is the slope of \\(f_{nn}(x)\\)? (Hint: which neurons are active?)\nFor \\(x \\in [\\pi/2, \\pi)\\): What is the slope?\n\nExplain how the output weight \\(c_2\\) controls the change in slope at \\(x = \\pi/2\\).\n\nSuggest a choice for the output weights \\(c_0, c_1, c_2\\) that you think will give some kind of crude approximation of \\(\\sin(x)\\) on the interval \\([0,\\pi]\\). Use the Python function plot_neural_network provided below to make a plot of your approximation.\n\nUse the Python function plot_neural_network with 4 knot points at \\(x = 0, \\pi/4, \\pi/2, 3\\pi/4\\). Try to find output weights that create a good approximation.\nCalculate the maximum approximation error and discuss how it could be improved.\nExperiment with more knot points (try 9 or 17 points evenly spaced). How does the approximation improve?\n\nPython helper function:\n\nimport numpy as np\nimport matplotlib.pyplot as plt \n\ndef plot_neural_network(biases, output_weights, output_bias=0,\n                        x_range=(0, np.pi), target_func=np.sin):\n    \"\"\"\n    Plot a two-layer neural network approximation.\n    \n    Parameters:\n    biases (list or array): Biases for hidden layer\n        These determine the knot points: x_i = -b_i\n    output_weights (list or array) : Weights for output layer\n    output_bias (float): Bias for output layer (default: 0)\n    x_range : tuple\n        (x_min, x_max) for plotting (default: (0, np.pi))\n    target_func : function\n        The target function to approximate (default: np.sin)\n    \n    Returns:\n    float: Maximum absolute error\n    \"\"\"\n    x = np.linspace(x_range[0], x_range[1], 500)\n    \n    # Compute neural network output\n    f_nn = two_layer_network(x, biases, output_weights, output_bias)\n    \n    # Compute target\n    f_target = target_func(x)\n    \n    # Plot\n    plt.figure(figsize=(10, 6))\n    plt.plot(x, f_target, 'r-', linewidth=2, label='Target function')\n    plt.plot(x, f_nn, 'b--', linewidth=2, label='Neural network')\n    \n    # Mark knot points\n    knots = [-b for b in biases]\n    for knot in knots:\n        if x_range[0] &lt;= knot &lt;= x_range[1]:\n            plt.axvline(knot, color='gray', linestyle=':', alpha=0.5)\n    \n    plt.grid(True, alpha=0.3)\n    plt.legend()\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('Neural Network Approximation')\n    plt.show()\n    \n    # Calculate and return error\n    error = np.max(np.abs(f_target - f_nn))\n    print(f\"Maximum error: {error:.4f}\")\n    return error\n\n# Example usage:\n# output_weights = [1, -0.3, -0.5, -0.4]\n# biases = [0, -np.pi/4, -np.pi/2, -3*np.pi/4]\n# plot_neural_network(biases, output_weights)\n\n\n\nAfter your experimentation, the following result will no longer seem so surprising:\n\nTheorem 2.1 (Universal Approximation Theorem (informal version)) A two-layer neural network with a sufficient number of neurons can approximate any continuous function on a bounded interval to arbitrary accuracy.\nMore precisely: for any continuous function \\(f: [a,b] \\to \\mathbb{R}\\) and any \\(\\epsilon &gt; 0\\), there exists a two-layer neural network \\(f_{\\text{nn}}\\) such that \\[\\begin{equation}\n\\max_{x \\in [a,b]} |f(x) - f_{\\text{nn}}(x)| &lt; \\epsilon.\n\\end{equation}\\]\n\n\n\nExercise 2.26 üí¨ Reflect on what you have learned about neural networks and compare them to Taylor series:\n\nWhat are the advantages of using neural networks for function approximation compared to Taylor series?\nWhat are the advantages of Taylor series compared to neural networks?\nFor each of the following functions, which approximation method (Taylor series or neural network) do you think would be more appropriate, and why?\n\n\\(f(x) = e^x\\) near \\(x = 0\\)\n\\(f(x) = |x|\\) near \\(x = 0\\)\n\n\\(f(x) = \\sin(100x)\\) on \\([0, 2\\pi]\\)\nA function defined by experimental data points\n\nNeural networks are determined by their weights and biases. Taylor series are determined by the derivatives of the function at a point. Which do you think is easier to compute in practice, and why?\n\n\n\n\n\n2.4.4 Deep Neural Networks\nSo far we have looked at two-layer neural networks (one hidden layer + one output layer). But we can also stack multiple layers on top of each other to create deep neural networks.\nThe idea is simple: instead of having the output layer directly combine the hidden neurons, we can feed the hidden neurons into another layer of neurons, and then another, and so on.\nFor example, a three-layer network would look like:\n\nInput: \\(x\\)\nFirst Hidden Layer: \\[\\begin{equation}\nh_i^{(1)}(x) = \\max(0, w_i^{(1)} x + b_i^{(1)}) \\quad \\text{for } i = 1, \\ldots, n_1\n\\end{equation}\\]\nSecond Hidden Layer: Each neuron in this layer takes all outputs from the first hidden layer: \\[\\begin{equation}\nh_j^{(2)} = \\max\\left(0, \\sum_{i=1}^{n_1} w_{ji}^{(2)} h_i^{(1)} + b_j^{(2)}\\right) \\quad \\text{for } j = 1, \\ldots, n_2\n\\end{equation}\\]\nOutput Layer: \\[\\begin{equation}\nf(x) = \\sum_{j=1}^{n_2} c_j h_j^{(2)} + c_0\n\\end{equation}\\]\n\nThis is called a deep neural network because it has multiple hidden layers. The term ‚Äúdeep learning‚Äù comes from using such multi-layer architectures.\n\n\n\n\n\n\n\n\n\n\n\n\n2.4.5 Why Depth Matters: The Sawtooth Example\nWhy are deep neural networks, with many layers, often more powerful than shallow networks with just one large hidden layer? In the following exercises, you will explore a classic example, the sawtooth wave, to discover a concrete answer. You‚Äôll build both a shallow and a deep network to represent the same function, and compare their efficiency as the complexity grows. Through this exploration, you‚Äôll see how deep networks can represent certain functions far more efficiently by exploiting their compositional structure.\nWe‚Äôll work with a sawtooth function that has 2 triangular peaks on the interval \\([0, 1]\\): \\[\\begin{equation}\nf(x) = \\begin{cases}\n4x & \\text{if } 0 \\leq x &lt; 0.25 \\\\\n2 - 4x & \\text{if } 0.25 \\leq x &lt; 0.5 \\\\\n4x - 2 & \\text{if } 0.5 \\leq x &lt; 0.75 \\\\\n4 - 4x & \\text{if } 0.75 \\leq x \\leq 1\n\\end{cases}\n\\end{equation}\\]\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.27 (Building a Shallow Network for the Sawtooth) You already know how to represent piecewise linear functions with a two-layer neural network. Each of the 5 ‚Äúhinge points‚Äù (where the slope changes) requires a ReLU neuron.\na) For each hinge point at position \\(x_i\\), you need a ReLU neuron with bias \\(b_i = -x_i\\).\nb) What are the output weights with which you need to combinae these neurons?\nUse this to complete the code below and use the plot to check that the resulting function has the desired form.\ndef shallow_2_peak_network(x):\n    biases = [.....]\n    output_weights = [.....]\n    return two_layer_network(x, biases, output_weights)\n\n# Plot\nplt.figure(figsize=(10, 6))\nx = np.linspace(-0.1,1.1,500)\nplt.plot(x, shallow_2_peak_network(x), 'r-', lw=3)\nplt.grid()\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.title('2-Peak Sawtooth Function')\nplt.show()\n\n\n\nExercise 2.28 (Building the Deep Network) A deeper network takes a more elegant approach. Instead of directly creating all the peaks, it uses one layer to create a single peak and then another layer to duplicate this peak.\nThe key insight is to create a single-peak function \\[g(x) = \\begin{cases}2x&0\\leq x&lt;1/2\\\\2(1-x)&1/2\\leq x\\leq1\\\\0 & x\\not\\in [0,1]\\end{cases}\\] that produces one triangular peak at \\(x = 0.5\\) with height 1, returning to 0 at \\(z &lt; 0\\) and \\(z &gt; 1\\).\n\n\n\n\n\n\n\n\n\nThen, our 2-peak sawtooth is simply \\(f(x) = g(g(x))\\).\na) First, create a single-peak as a shallow network with three hidden neurons.\ndef g(x):\n    \"\"\" Single triangular peak \"\"\"\n    biases = [.....]\n    output_weights = [.....]\n    return two_layer_network(x, biases, output_weights)\n\n# Plot\nplt.figure(figsize=(10, 6))\nx = np.linspace(-0.1,1.1,500)\nplt.plot(x, g(x), 'r-', lw=3))\nplt.grid()\nplt.xlabel('x')\nplt.ylabel('g(x)')\nplt.title('Single peak function')\nplt.show()\nb) Now comes the magic! Pass the result of your network \\(g(x)\\) through the same network again: compute \\(g(g(x))\\). Plot the result and compare it to the target.\n\nf = lambda x: g(g(x))\n\n# Plot\nplt.figure(figsize=(10, 6))\nx = np.linspace(-0.1,1.1,500)\nplt.plot(x, f(x), 'r-', lw=3)\nplt.grid()\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.title('2-Peak Sawtooth Function')\nplt.show()\n\n\n\n\n\n\n\n\nc) Explain in your own words: Why does applying \\(g\\) twice produce two peaks? Think about what happens as the first application of \\(g\\) creates a peak that rises from 0 to 1 and back to 0. What does the second application of \\(g\\) see as its input?\nd) Create a 4-peak sawtooth by composing \\(g\\) three times: \\(g(g(g(x)))\\). Plot it. How many hidden neurons does this use? Compare to the shallow network approach from the previous exercise.\n\n\n\nExercise 2.29 (Scaling Analysis: The Exponential Advantage) Now let‚Äôs compare how the two approaches scale as we increase the number of peaks.\na) üñã Complete this table by implementing both shallow and deep networks for different numbers of peaks:\n\n\n\n\n\n\n\n\n\nNumber of Peaks\nShallow Network Neurons\nDeep Network Neurons\nAdvantage (ratio)\n\n\n\n\n2\n5\n6\n0.83\n\n\n4\n?\n?\n?\n\n\n8\n?\n?\n?\n\n\n16\n?\n?\n?\n\n\n\nb) üñã Based on your table, write formulas for the number of neurons needed as a function of the number of peaks \\(n\\) for both approaches. What kind of growth do you observe (linear, logarithmic, exponential)?\nHint: For the deep network, if \\(n = 2^k\\) peaks, how many layers do you need? How many neurons per layer?\nc) üíª Visualize the scaling behavior. Plot the number of neurons (y-axis) versus the number of peaks (x-axis) for both approaches:\n# TODO: Create arrays for different numbers of peaks\npeaks = [2, 4, 8, 16, 32, 64]\nshallow_neurons = [...]  # Fill in based on your formula\ndeep_neurons = [...]     # Fill in based on your formula\n\n# Plot\nplt.figure(figsize=(8, 5))\nplt.plot(peaks, shallow_neurons, 'bo-', label='Shallow Network', lw=2)\nplt.plot(peaks, deep_neurons, 'mo-', label='Deep Network', lw=2)\nplt.xlabel('Number of Peaks')\nplt.ylabel('Number of Hidden Neurons')\nplt.title('Scaling: Shallow vs Deep Networks')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\nd) üí¨ Based on your analysis, explain why deep networks can be more efficient than shallow networks for certain types of functions. What specific property of the sawtooth function makes it particularly well-suited for a deep architecture?\ne) üí¨ The deep network achieves its efficiency by reusing the same learned ‚Äúpeak-making‚Äù module across layers through composition. Can you think of other real-world functions or patterns that might benefit from deep architectures in a similar way? Consider:\n\nFunctions with repetitive structures at different scales (e.g., fractals, wavelets)\nHierarchical patterns (e.g., language: letters ‚Üí words ‚Üí sentences)\nFunctions that can be naturally expressed as compositions of simpler functions\n\nf) üí¨ What are the limitations of this advantage? Can you think of functions where a shallow network might be more efficient than a deep one? (Hint: What if the function has no repetitive or compositional structure?)",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "nmFunctions.html#sec-functions_problems",
    "href": "nmFunctions.html#sec-functions_problems",
    "title": "2¬† Functions",
    "section": "2.5 Problems",
    "text": "2.5 Problems\n\nExercise 2.30 Find the Taylor Series for \\(f(x) = \\frac{1}{\\log(x)}\\) centred at the point \\(x_0 = e\\). Then use the Taylor Series to approximate the number \\(\\frac{1}{\\log(3)}\\) to 4 decimal places.\n\n\n\nExercise 2.31 In this problem we will use Taylor Series to build approximations for the irrational number \\(\\pi\\).\n\nWrite the Taylor series centred at \\(x_0=0\\) for the function \\[\\begin{equation}\nf(x) = \\frac{1}{1+x}.\n\\end{equation}\\]\nNow we want to get the Taylor Series for the function \\(g(x) = \\frac{1}{1+x^2}\\). It would be quite time consuming to take all of the necessary derivatives to get this Taylor Series. Instead we will use our answer from part (a) of this problem to shortcut the whole process.\n\nSubstitute \\(x^2\\) for every \\(x\\) in the Taylor Series for \\(f(x) = \\frac{1}{1+x}\\).\nMake a few plots to verify that we indeed now have a Taylor Series for the function \\(g(x) = \\frac{1}{1+x^2}\\).\n\nRecall from Calculus that \\[\\begin{equation}\n\\int \\frac{1}{1+x^2} dx = \\arctan(x).\n\\end{equation}\\] Hence, if we integrate each term of the Taylor Series that results from part (2) we should have a Taylor Series for \\(\\arctan(x)\\).1\nNow recall the following from Calculus:\n\n\\(\\tan(\\pi/4) = 1\\)\nso \\(\\arctan(1) = \\pi/4\\)\nand therefore \\(\\pi = 4\\arctan(1)\\).\n\nLet us use these facts along with the Taylor Series for \\(\\arctan(x)\\) to approximate \\(\\pi\\): we can just plug in \\(x=1\\) to the series, add up a bunch of terms, and then multiply by 4. Write a loop in Python that builds successively better and better approximations of \\(\\pi\\). Stop the loop when you have an approximation that is correct to 6 decimal places.\n\n\n\n\nExercise 2.32 In this problem we will prove the famous formula \\[\\begin{equation}\ne^{i\\theta} = \\cos(\\theta) + i \\sin(\\theta).\n\\end{equation}\\] This is known as Euler‚Äôs formula after the famous mathematician Leonard Euler. Show all of your work for the following tasks.\n\nWrite the Taylor series for the functions \\(e^x\\), \\(\\sin(x)\\), and \\(\\cos(x)\\).\nReplace \\(x\\) with \\(i\\theta\\) in the Taylor expansion of \\(e^x\\). Recall that \\(i = \\sqrt{-1}\\) so \\(i^2 = -1\\), \\(i^3 = -i\\), and \\(i^4 = 1\\). Simplify all of the powers of \\(i\\theta\\) that arise in the Taylor expansion. I will get you started: \\[\\begin{equation}\n\\begin{aligned} e^x &= 1 + x + \\frac{x^2}{2} + \\frac{x^3}{3!} + \\frac{x^4}{4!} + \\frac{x^5}{5!} + \\cdots \\\\ e^{i\\theta} &= 1 + (i\\theta) + \\frac{(i\\theta)^2}{2!} + \\frac{(i\\theta)^3}{3!} + \\frac{(i\\theta)^4}{4!} + \\frac{(i\\theta)^5}{5!} + \\cdots \\\\ &= 1 + i\\theta + i^2 \\frac{\\theta^2}{2!} + i^3 \\frac{\\theta^3}{3!} + i^4 \\frac{\\theta^4}{4!} + i^5 \\frac{\\theta^5}{5!} + \\cdots \\\\ &= \\ldots \\text{ keep simplifying ... } \\ldots \\end{aligned}\n\\end{equation}\\]\nGather all of the real terms and all of the imaginary terms together. Factor the \\(i\\) out of the imaginary terms. What do you notice?\nUse your result from part (3) to prove that \\(e^{i\\pi} + 1 = 0\\).\n\n\n\n\nExercise 2.33 In physics, the relativistic energy of an object is defined as \\[\\begin{equation}\nE_{rel} = \\gamma mc^2\n\\end{equation}\\] where \\[\\begin{equation}\n\\gamma = \\frac{1}{\\sqrt{1 - \\frac{v^2}{c^2}}}.\n\\end{equation}\\] In these equations, \\(m\\) is the mass of the object, \\(c\\) is the speed of light (\\(c \\approx 3 \\times 10^8\\)m/s), and \\(v\\) is the velocity of the object. For an object of fixed mass (m) we can expand the Taylor Series centred at \\(v=0\\) for \\(E_{rel}\\) to get \\[\\begin{equation}\nE_{rel} = mc^2 + \\frac{1}{2} mv^2 + \\frac{3}{8} \\frac{mv^4}{c^2} + \\frac{5}{16} \\frac{mv^6}{c^4} + \\cdots.\n\\end{equation}\\]\n\nWhat do we recover if we consider an object with zero velocity?\nWhy might it be completely reasonable to only use the quadratic approximation \\[\\begin{equation}\nE_{rel} = mc^2 + \\frac{1}{2} mv^2\n\\end{equation}\\] for the relativistic energy equation?2\n(some physics knowledge required) What do you notice about the second term in the Taylor Series approximation of the relativistic energy function?\nShow all of the work to derive the Taylor Series centred at \\(v = 0\\) given above.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "nmFunctions.html#footnotes",
    "href": "nmFunctions.html#footnotes",
    "title": "2¬† Functions",
    "section": "",
    "text": "There are many reasons why integrating an infinite series term by term should give you a moment of pause. For the sake of this problem we are doing this operation a little blindly, but in reality we should have verified that the infinite series actually converges uniformly.‚Ü©Ô∏é\nThis is something that people in physics and engineering do all the time ‚Äì there is some complicated nonlinear relationship that they wish to use, but the first few terms of the Taylor Series captures almost all of the behaviour since the higher-order terms are very very small.‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "nmRoots2.html",
    "href": "nmRoots2.html",
    "title": "4¬† Roots 2",
    "section": "",
    "text": "In preparation.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Roots 2</span>"
    ]
  },
  {
    "objectID": "nmCalculus.html",
    "href": "nmCalculus.html",
    "title": "5¬† Derivatives, Integrals",
    "section": "",
    "text": "In preparation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Derivatives, Integrals</span>"
    ]
  },
  {
    "objectID": "nmOptimisation.html",
    "href": "nmOptimisation.html",
    "title": "6¬† Optimisation",
    "section": "",
    "text": "In preparation.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Optimisation</span>"
    ]
  },
  {
    "objectID": "nmODE1.html",
    "href": "nmODE1.html",
    "title": "7¬† ODEs 1",
    "section": "",
    "text": "In preparation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>ODEs 1</span>"
    ]
  },
  {
    "objectID": "nmODE2.html",
    "href": "nmODE2.html",
    "title": "8¬† ODEs 2",
    "section": "",
    "text": "In preparation.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>ODEs 2</span>"
    ]
  },
  {
    "objectID": "nmPDE1.html",
    "href": "nmPDE1.html",
    "title": "9¬† PDEs 1",
    "section": "",
    "text": "In preparation.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>PDEs 1</span>"
    ]
  },
  {
    "objectID": "nmPDE2.html",
    "href": "nmPDE2.html",
    "title": "10¬† PDEs 2",
    "section": "",
    "text": "In preparation.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>PDEs 2</span>"
    ]
  },
  {
    "objectID": "nmPython.html",
    "href": "nmPython.html",
    "title": "Appendix A ‚Äî Python",
    "section": "",
    "text": "A.1 Why Python?\nIn this appendix we will walk through some of the basics of using Python ‚Äî the popular general-purpose programming language that we will use throughout this module.\nFor most of you this material will not be new. For example, you have probably seen it in your first year ‚ÄúMathematical Programming & Skills‚Äù module. But for some of you this may be entirely new. You will have some notion of what a programming language ‚Äúis‚Äù and ‚Äúdoes‚Äù, but you may never have written any code. That is all right.\nIf you are new to Python, don‚Äôt feel that you need to work through this appendix in one go. Instead, spread the work over the first two weeks of the course and interlace it with your work on the first two chapters.\nThere is a lot of material in this appendix. Do not feel that you need to learn it all by hard. The idea is just that you should have seen the various language constructs once. Your familiarity with them will come automatically later when you use them throughout the course.\nWe are going to be using Python since\nIt is important to keep in mind that Python is a general purpose language that we will be using for Scientific Computing. The purpose of Scientific Computing is not to build apps, build software, manage databases, or develop user interfaces. Instead, Scientific Computing is the use of a computer programming language (like Python) along with mathematics to solve scientific and mathematical problems. For this reason it is definitely not our purpose to write an all-encompassing guide for how to use Python. We will only cover what is necessary for our computing needs. You will learn more as the course progresses, so use this appendix just to get going with the language. To keep things as simple as possible, we will for example not use object oriented programming, so will not introduce classes and methods.\nWe are also definitely not saying that Python is the best language for scientific computing under all circumstances. The reason there are so many scientific programming languages coexisting is that each has particular strengths that make it the best option for particular applications. But we are saying that Python is so widely used that everyone should know Python.\nThere is an overwhelming abundance of information available about Python and the suite of tools that we will frequently use.\nThese tools together provide all of the computational power that we will need. And they are free!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Python</span>"
    ]
  },
  {
    "objectID": "nmPython.html#sec-why_python",
    "href": "nmPython.html#sec-why_python",
    "title": "Appendix A ‚Äî Python",
    "section": "",
    "text": "Python is free,\nPython is very widely used,\nPython is flexible,\nPython is relatively easy to learn,\nand Python is quite powerful.\n\n\n\n\n\nPython https://www.python.org/,\nnumpy (numerical Python) https://www.numpy.org/,\nmatplotlib (a suite of plotting tools) https://matplotlib.org/,\nscipy (scientific Python) https://www.scipy.org/.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Python</span>"
    ]
  },
  {
    "objectID": "nmPython.html#sec-python_basics",
    "href": "nmPython.html#sec-python_basics",
    "title": "Appendix A ‚Äî Python",
    "section": "A.2 Python Programming Basics",
    "text": "A.2 Python Programming Basics\nIf you are already very practised in using Python then you can jump straight to Section A.6 with the coding exercises. But if you are new to Python or your Python skills are a bit rusty, then you will benefit from working through all the examples and exercises below, making sure you copy and paste all the code into your Colab notebook and run it there, and then critically evaluate and understand the output.\n\nA.2.1 Variables\nVariable names in Python can contain letters (lower case or capital), numbers 0-9, and some special characters such as the underscore. Variable names must start with a letter. There are a bunch of reserved words that you can not use for your variable names because they have a special meaning in the Python syntax. Python will let you know with a syntax error if you try to use a reserved word for a variable name.\nYou can do the typical things with variables. Assignment is with an equal sign (be careful R users, we will not be using the left-pointing arrow here!).\nWarning: When defining numerical variables you do not always get floating point numbers. In some programming languages, if you write x=1 then automatically x is saved as 1.0; a floating point number, not an integer. In Python however, if you assign x=1 it is defined as an integer (with no decimal digits) but if you assign x=1.0 it is assigned as a floating point number.\n# assign some variables\nx = 7 # integer assignment of the integer 7\ny = 7.0 # floating point assignment of the decimal number 7.0\nprint(\"The variable x has the value\", x, \" and has type\", type(x), \". \\n\")\nprint(\"The variable y has the value\", y, \" and has type\", type(y), \". \\n\")\nRemember to copy each code block to your own notebook, execute it and look at the output. To copy the code from this guide to your notebook you can use the ‚ÄúCopy to Clipboard‚Äù icon that pops up in the top right corner of a code block when you hover over that code block.\n# multiplying by a float will convert an integer to a float\nx = 7 # integer assignment of the integer 7\nprint(\"Multiplying x by 1.0 gives\", 1.0*x)\nprint(\"The type of this value is\", type(1.0*x), \". \\n\")\nThe allowed mathematical operations are:\n\nAddition: +\nSubtraction: -\nMultiplication: *\nDivision: /\nInteger Division (modular division): // and %\nExponents: **\n\nThat‚Äôs right, the caret key, ^, is NOT an exponent in Python (sigh). Instead we have to get used to ** for exponents.\nx = 7.0\ny = x**2 # square the value in x\ny\n\n\nExercise A.1 Write code to define positive integers \\(a,b\\) and \\(c\\) of your own choosing. Then calculate \\(a^2, b^2\\) and \\(c^2\\). When you have all three values computed, check to see if your three values form a Pythagorean Triple so that \\(a^2 + b^2 = c^2\\). Have Python simply say True or False to verify that you do, or do not, have a Pythagorean Triple defined.¬†Hint: You will need to use the == Boolean check just like in other programming languages.\n\n\n\n\nA.2.2 Indexing and Lists\nLists are a key component to storing data in Python. Lists are exactly what the name says: lists of things (in our case, usually the entries are floating point numbers).\nWarning: Python indexing starts at 0 whereas some other programming languages have indexing starting at 1. In other words, the first entry of a list has index 0, the second entry as index 1, and so on. We just have to keep this in mind.\nWe can extract a part of a list using the syntax name[start:stop] which extracts elements between index start and stop-1. Take note that Python stops reading at the second to last index. This often catches people off guard when they first start with Python.\n\n\nExample A.1 (Lists and Indexing) Let us look at a few examples of indexing from lists. In this example we will use the list of numbers 0 through 8. This list contains 9 numbers indexed from 0 to 8.\n\nCreate the list of numbers 0 through 8\n\nmy_list = [0,1,2,3,4,5,6,7,8]\n\nOutput the list\n\nmy_list\n\nSelect only the element with index 0.\n\nmy_list[0]\n\nSelect all elements up to, but not including, the third element of my_list.\n\nmy_list[:2]\n\nSelect the last element of my_list (this is a handy trick!).\n\nmy_list[-1] \n\nSelect the elements indexed 1 through 4. Beware! This is not the first through fifth element.\n\nmy_list[1:5] \n\nSelect every other element in the list starting with the first.\n\nmy_list[0::2]\n\nSelect the last three elements of my_list\n\nmy_list[-3:]\n\n\nIn Python, elements in a list do not need to be the same type. You can mix integers, floats, strings, lists, etc.\n\nExample A.2 In this example we see a list of several items that have different data types: float, integer, string, and complex. Note that the imaginary number \\(i\\) is represented by \\(1j\\) in Python. This use of \\(j\\) instead of \\(i\\) is common in some scientific disciplines and is just another thing that we Mathematicians will need to get used to in Python.\nMixedList = [1.0, 7, 'Bob', 1-1j]\nprint(MixedList)\nprint(type(MixedList[0]))\nprint(type(MixedList[1]))\nprint(type(MixedList[2]))\nprint(type(MixedList[3])) \n# Notice that we use 1j for the imaginary number \"i\".\n\n\n\nExercise A.2 In this exercise you will put your new list skills into practice.\n\nCreate the list of the first several Fibonacci numbers: \\[\\begin{equation}\n1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89.\n\\end{equation}\\]\nPrint the first four elements of the list.\nPrint every third element of the list starting from the first.\nPrint the last element of the list.\nPrint the list in reverse order.\nPrint the list starting at the last element and counting backward by every other element.\n\n\n\n\n\nA.2.3 List Operations\nPython is awesome about allowing you to do things like appending items to lists, removing items from lists, and inserting items into lists. Note in all of the examples below that we are using the code\nvariable.method\nwhere you put the variable name, a dot, and the thing that you would like to do to that variable. For example, my_list.append(7) will append the number 7 to the list my_list. We say that append is a ‚Äúmethod‚Äù of the list my_list. This is a common programming feature in Python and we will use it often.\n\n\nExample A.3 The .append method can be used to append an element to the end of a list.\nmy_list = [0,1,2,3]\nprint(my_list)\n# Append the string 'a' to the end of the list\nmy_list.append('a') \nprint(my_list)\n# Do it again ... just for fun\nmy_list.append('a') \nprint(my_list)\n# Append the number 15 to the end of the list\nmy_list.append(15) \nprint(my_list)\n\n\n\nExample A.4 The .remove method can be used to remove an element from a list.\n# Let us remove the 3\nmy_list.remove(3)\nmy_list\n\n\n\nExample A.5 The .insert method can be used to insert an element at a location in a list.\n# insert the letter `A` at the 0-indexed spot\nmy_list.insert(0,'A') \n# insert the letter `B` at the spot with index 3 \nmy_list.insert(3,'B') \n# remember that index 3 means the fourth spot in the list\nmy_list\n\n\n\nExercise A.3 In this exercise you will go a bit further with your list operation skills.\n\nCreate the list of the first several Lucas Numbers: \\(1,3,4,7,11,18,29,47.\\)\nAdd the next three Lucas Numbers to the end of the list.\nRemove the number 3 from the list.\nInsert the 3 back into the list in the correct spot.\nPrint the list in reverse order.\nDo a few other list operations to this list and report your findings.\n\n\n\n\n\nA.2.4 Tuples\nIn Python, a ‚Äútuple‚Äù is like an ordered pair (or ordered triple, or ordered quadruple, ...) in mathematics. We will occasionally see tuples in our work in numerical analysis so for now let us just give a couple of code snippets showing how to store and read them.\nWe can define the tuple of numbers \\((10,20)\\) in Python as follows:\n\nExample A.6 ¬†\npoint = 10, 20 \nprint(point, type(point))\nWe can also define a tuple with parenthesis if we like. Python does not care.\npoint = (10, 20) # now we define the tuple with parenthesis\nprint(point, type(point))\nWe can then unpack the tuple into components if we wish:\nx, y = point\nprint(\"x = \", x)\nprint(\"y = \", y)\n\nThere are other important data structures in Python that we will not use in this module. These include dictionaries and sets. We will not cover these here because we are trying to keep things simple so that we can concentrate on Numerical Analysis instead. If you are interested in learning more about these data structures, you can find a lot of information about them in the Python documentation.\n\n\nA.2.5 Control Flow: Loops and If Statements\nAny time you need to do some repetitive task with a programming language you can use a loop. Just like in other programming languages, we can do loops and conditional statements in very easy ways in Python. The thing to keep in mind is that the Python language is very white-space-dependent. This means that your indentations need to be correct in order for a loop to work. You could get away with sloppy indention in other languages but not so in Python. Also, in some languages (like R and Java) you need to wrap your loops in curly braces. Again, not so in Python.\nCaution: Be really careful of the white space in your code when you write loops.\n\nA.2.5.1 for Loops\nA for loop is designed to do a task a certain number of times and then stop. This is a great tool for automating repetitive tasks, but it also nice numerically for building sequences, summing series, or just checking lots of examples. The following are some examples of Python for loops.\n\nExample A.7 Print the first 6 perfect squares.\nfor x in [1,2,3,4,5,6]:\n    print(x**2)\nOften instead of writing the list of integers explicitly one uses the range() function, so that this example would be written as\nfor x in range(1,7):\n    print(x**2)\nNote that range(1,7) produces the integers from 1 to 6, not from 1 to 7. This is another manifestation of Python‚Äôs weird 0-based indexing. Of course it is only weird to people who are new to Python. For Pythonists it is perfectly natural.\nYou can also use range() to generate a sequence of numbers with a specific step size. For example, range(1, 10, 2) will generate the odd integers from 1 to 9.\nfor x in range(1, 10, 2):\n    print(x)\n\n\nTake careful note of the syntax for a for loop as it is the same as for other loops and conditional statements:\n\na control statement,\na colon, a new line,\nindent four spaces,\nsome programming statements\n\nWhen you are done with the loop, just back out of the indention. There is no need for an end command or a curly brace. All of the control statements in Python are white-space-dependent.\n\n\nExample A.8 Print the names in a list.\nNamesList = ['Alice','Billy','Charlie','Dom','Enrique','Francisco']\nfor name in NamesList:\n    print(name)\n\n\nIn Python you can use a more compact notation for for loops sometimes. This takes a bit of getting used to, but is super slick!\n\n\nExample A.9 Create a list of the perfect squares from 1 to 9.\n# create a list of the perfect squares from 1 to 9\nCoolList = [x**2 for x in range(1,10)]\nprint(CoolList)\n# Then print the sum of this list\nprint(\"The sum of the first 9 perfect squares is\", sum(CoolList))\n\n\nfor loops can also be used to build sequences, as can be seen in the next couple of examples.\n\n\nExample A.10 In the following code we write a for loop that outputs a list of the first 7 iterations of the sequence \\(x_{n+1}=-0.5x_n+1\\) starting with \\(x_0=3\\). Notice that we are using the command x.append instead of \\(x[n+1]\\) to append the new term to the list. This allows us to grow the length of the list dynamically as the loop progresses.\nx=[3.0]\nfor n in range(0,7):\n    x.append(-0.5*x[n] + 1)\n    print(x) # print the whole list x at each step of the loop\n\n\n\nExample A.11 As an alternative to the code from the previous example we can pre-allocate the memory in a list of zeros. This is done with the clever code x = [0] * 10. Literally multiplying a list by some number, like 10, says to repeat that list 10 times.\nNow we will build the sequence with pre-allocated memory.\nx = [0] * 7\nx[0] = 3.0\nfor n in range(0,6):\n    x[n+1] = -0.5*x[n]+1\n    print(x) # This print statement shows x at each iteration\n\n\n\nExercise A.4 We want to sum the first 100 perfect cubes. Let us do this in two ways.\n\nStart off a variable called total at 0 and write a for loop that adds the next perfect cube to the running total.\nWrite a for loop that builds the sequence of the first 100 perfect cubes. After the list has been built find the sum with the sum() function.\n\nThe answer is: 25,502,500 so check your work.\n\n\n\nExercise A.5 Write a for loop that builds the first 20 terms of the sequence \\(x_{n+1}=1-x_n^2\\) with \\(x_0=0.1\\). Pre-allocate enough memory in your list and then fill it with the terms of the sequence. Only print the list after all of the computations have been completed.\n\n\n\n\nA.2.5.2 while Loops\nA while loop repeats some task (or sequence of tasks) while a logical condition is true. It stops when the logical condition turns from true to false. The structure in Python is the same as with for loops.\n\n\nExample A.12 Print the numbers 0 through 4 and then the word ‚Äúdone.‚Äù we will do this by starting a counter variable, i, at 0 and increment it every time we pass through the loop.\ni = 0\nwhile i &lt; 5:\n    print(i)\n    i += 1 # increment the counter\nprint(\"done\")\n\n\n\nExample A.13 Now let us use a while loop to build the sequence of Fibonacci numbers and stop when the newest number in the sequence is greater than 1000. Notice that we want to keep looping until the condition that the last term is greater than 1000 ‚Äì this is the perfect task for a while loop, instead of a for loop, since we do not know how many steps it will take before we start the task\nFib = [1,1]\nwhile Fib[-1] &lt;= 1000:\n    Fib.append(Fib[-1] + Fib[-2])\nprint(\"The last few terms in the list are:\\n\",Fib[-3:])\n\n\n\nExercise A.6 Write a while loop that sums the terms in the Fibonacci sequence until the sum is larger than 1000\n\n\n\n\nA.2.5.3 if Statements\nConditional (if) statements allow you to run a piece of code only under certain conditions. This is handy when you have different tasks to perform under different conditions.\n\n\nExample A.14 Let us look at a simple example of an if statement in Python.\nName = \"Alice\"\nif Name == \"Alice\":\n    print(\"Hello, Alice.  Isn't it a lovely day to learn Python?\")\nelse:\n    print(\"You're not Alice.  Where is Alice?\")\nName = \"Billy\"\nif Name == \"Alice\":\n    print(\"Hello, Alice.  Isn't it a lovely day to learn Python?\")\nelse:\n    print(\"You're not Alice.  Where is Alice?\")\n\n\n\nExample A.15 For another example, if we get a random number between 0 and 1 we could have Python print a different message depending on whether it was above or below 0.5. Run the code below several times and you will see different results each time.\nNote: We have to import the numpy package to get the random number generator in Python. Do not worry about that for now. We will talk about packages in a moment.\nimport numpy as np\nx = np.random.uniform() # get a random number between 0 and 1\nif x &lt; 0.5:\n    print(x,\" is less than a half\")\nelse:\n    print(x, \"is NOT less than a half\")\n(Take note that the output will change every time you run it.)\n\n\n\nExample A.16 In many programming tasks it is handy to have several different choices between tasks instead of just two choices as in the previous examples. This is a job for the elif command.\nThis is the same code as last time except we will make the decision at 0.33 and 0.67.\nimport numpy as np\nx = np.random.rand(1,1) # get a random 1x1 matrix using numpy\nx = x[0,0] # pull the entry from the first row and first column\nif x &lt; 0.33:\n    print(x,\" &lt; 1/3\")\nelif x &lt; 0.67:\n    print(\"1/3 &lt;= \",x,\"&lt; 2/3\")\nelse:\n    print(x, \"&gt;= 2/3\")\n(Take note that the output will change every time you run it.)\n\n\n\nExercise A.7 Write code to give the Collatz Sequence \\[\\begin{equation}\nx_{n+1} = \\left\\{ \\begin{array}{ll} x_n / 2, & \\text{$x_n$ is even} \\\\ 3 x_n + 1, & \\text{otherwise} \\end{array} \\right.\n\\end{equation}\\] starting with a positive integer of your choosing. The sequence will converge1 to 1 so your code should stop when the sequence reaches 1.\nHints: To test whether a number x is even you can test whether the remainder after dividing by 2 is zero with (x % 2) == 0. Also you will want to use the integer division // when calculating \\(x_n/2\\).\n\n\n\n\n\nA.2.6 Functions\nMathematicians and programmers talk about functions in very similar ways, but they are not exactly the same. When we say ‚Äúfunction‚Äù in a programming sense we are talking about a chunk of code that you can pass parameters and expect an output of some sort. This is not unlike the mathematician‚Äôs version. But unlike a mathematical function, a Python function can also have side effects, like plotting a graph for example. So Python‚Äôs definition of a function is a bit more flexible than that of a mathematician.\nIn Python, to define a function we start with def, followed by the function‚Äôs name, any input variables in parenthesis, and a colon. The indented code after the colon is what defines the actions of the function.\n\n\nExample A.17 The following code defines the polynomial \\(f(x) = x^3 + 3x^2 + 3x + 1\\) and then evaluates the function at a point \\(x=2.3\\).\ndef f(x):\n    return(x**3 + 3*x**2 + 3*x + 1)\nf(2.3)\n\n\nTake careful note of several things in the previous example:\n\nTo define the function we cannot just type it like we would see it one paper. This is not how Python recognizes functions.\nOnce we have the function defined we can call upon it just like we would on paper.\nWe cannot pass symbols into this type of function.2\n\n\n\nExercise A.8 Define the function \\(g(n) = n^2 + n + 41\\) as a Python function. Write a loop that gives the output for this function for integers from \\(n=0\\) to \\(n=39\\). Euler noticed that each of these outputs is a prime number (check this on your own). Will the function produce a prime for \\(n=40\\)? For \\(n=41\\)?\n\n\n\nExample A.18 One cool thing that you can do with functions is call them recursively. That is, you can call the same function from within the function itself. This turns out to be really handy in several mathematical situations.\nLet us define a function for the factorial. This function is naturally going to be recursive in the sense that it calls on itself!\ndef factorial(n):\n    if n==0:\n        return(1)\n    else:\n        return(n*factorial(n-1)) \n        # Note: we are calling the function recursively.\nWhen you run this code there will be no output. You have just defined the function so you can use it later, as follows:\nfactorial(12)\n\n\n\nExample A.19 For this next example let us define a function to calculate the next element in the sequence \\[\\begin{equation}\nx_{n+1} = \\left\\{ \\begin{array}{ll} 2x_n, & x_n \\in [0,0.5] \\\\ 2x_n - 1, & x_n \\in (0.5,1] \\end{array} \\right.\n\\end{equation}\\] and then build a loop to find the first several elements of the sequence starting at any real number between 0 and 1.\n# Define the function\ndef my_seq(xn):\n    if xn &lt;= 0.5:\n        return(2*xn)\n    else:\n        return(2*xn-1)\n# Now build a sequence with this function\nx = [0.125] # arbitrary starting point\nfor n in range(0,5): # Let us only build the first 5 terms\n    x.append(my_seq(x[n]))\nprint(x)\n\n\n\nExample A.20 A fun way to approximate the square root of two is to start with any positive real number and iterate over the sequence \\[\\begin{equation}\nx_{n+1} = \\frac{1}{2} x_n + \\frac{1}{x_n}\n\\end{equation}\\] until we are within any tolerance we like of the square root of \\(2\\). Write code that defines the sequence as a function and then iterates in a while loop until we are within \\(10^{-8}\\) of the square root of 2.\nWe import the math package so that we get the square root function. More about packages in the next section.\nfrom math import sqrt\ndef f(x):\n    return(0.5*x + 1/x)\nx = 1.1 # arbitrary starting point\nprint(f\"{'Approximation':&lt;20} | {'Exact':&lt;20} | {'Absolute error':&lt;20}\")\nprint(\"-\" * 68)\nwhile abs(x-sqrt(2)) &gt; 10**(-8):\n    x = f(x)\n    print(f\"{x:&lt;20} | {sqrt(2):&lt;20} | {abs(x - sqrt(2)):&lt;20}\")\nThis example also shows how to format the output of a print statement so that it is nicely aligned in columns. The :&lt;20 means that the string will be left-aligned and padded with spaces to a total width of 20 characters. The f before the string allows us to use curly braces {} to insert variables into the string. You will see an alternative way for displaying tabular data in Example¬†A.48.\n\n\n\nExercise A.9 The previous example is a special case of the Babylonian Algorithm for calculating square roots. If you want the square root of \\(S\\) then iterate the sequence \\[\\begin{equation}\nx_{n+1} = \\frac{1}{2} \\left(x_n + \\frac{S}{x_n} \\right)\n\\end{equation}\\] until you are within an appropriate tolerance.\nModify the code given in the previous example to give a list of approximations of the square roots of the natural numbers \\(2\\) through \\(20\\), each to within \\(10^{-8}\\). This problem will require that you build a function, write a ‚Äòfor‚Äô loop (for the integers \\(2\\) through \\(20\\)), and write a ‚Äòwhile‚Äô loop inside your ‚Äòfor‚Äô loop to do the iterations.\n\n\n\n\nA.2.7 Lambda Functions\nUsing def to define a function as in the previous subsection is really nice when you have a function that is complicated or requires some bit of code to evaluate. However, in the case of mathematical functions we have a convenient alternative: lambda Functions.\nThe basic idea of a lambda Function is that we just want to state what the variable is and what the rule is for evaluating the function. This is closest to the way that we write mathematical functions. For example, we can define the mathematical function \\(f(x) = x^2+3\\) in two different ways.\n\nUsing def:\n\ndef f(x):\n    return(x**2+3)\n\nUsing lambda:\n\nf = lambda x: x**2+3\nYou can see that in the Lambda Function we are explicitly stating the name of the variable immediately after the word lambda, then we put a colon, and then the function definition. This is somewhat similar to but also annoyingly different from the mathematicians notation \\(f:x \\mapsto x^2 + 3\\).\nNo matter whether we use def or lambda to define the function f, if we want to evaluate the function at a point, say \\(x=1.5\\), then we can write code just like we would mathematically: \\(f(1.5)\\)\nf(1.5) # evaluate the function at x=1.5\nWe can also define Lambda Functions of several variables. For example, if we want to define the mathematical function \\(f(x,y) = x^2 + xy + y^3\\) we could write the code\nf = lambda x, y: x**2 + x*y + y**3\nIf we wanted the value \\(f(2,4)\\) we would now write the code f(2,4).\n\n\nExercise A.10 Go back to Exercise¬†A.8 and repeat this exercise using a lambda function.\n\n\n\nExercise A.11 Go back to Exercise¬†A.9 and repeat this exercise using a lambda function.\n\n\n\n\nA.2.8 Packages\nPython was not created as a scientific programming language. The reason Python can be used for scientific computing is that there are powerful extension packages that define additional functions that are needed for scientific calculations.\nLet us start with the math package.\n\n\nExample A.21 The code below imports the math package into your instance of Python and calculates the cosine of \\(\\pi/4\\).\nimport math\nx = math.cos(math.pi / 4)\nprint(x)\nThe answer, unsurprisingly, is the decimal form of \\(\\sqrt{2}/2\\).\n\n\nYou might already see a potential disadvantage to Python‚Äôs packages: there is now more typing involved! Let us fix this. When you import a package you could just import all of the functions so they can be used by their proper names.\n\n\nExample A.22 Here we import the entire math package so we can use every one of the functions therein without having to use the math prefix.\nfrom math import * # read this as: from math import everything\nx = cos(pi / 4)\nprint(x)\nThe end result is exactly the same: the decimal form of \\(\\sqrt{2}/2\\), but now we had less typing to do.\n\n\nNow you can freely use the functions that were imported from the math package. There is a disadvantage to this, however. What if we have two packages that import functions with the same name. For example, in the math package and in the numpy package there is a cos() function. In the next block of code we will import both math and numpy, but instead we will import them with shortened names so we can type things a bit faster.\n\n\nExample A.23 Here we import math and numpy under aliases so we can use the shortened aliases and not mix up which functions belong to which packages.\nimport math as ma\nimport numpy as np\n# use the math version of the cosine function\nx = ma.cos(ma.pi / 4) \n# use the numpy version of the cosine function\ny = np.cos(np.pi / 4) \nprint(x, y)\nBoth x and y in the code give the decimal approximation of \\(\\sqrt{2}/2\\). This is clearly pretty redundant in this really simple case, but you should be able to see where you might want to use this and where you might run into troubles.\n\n\n\nExample A.24 (Contents of a package) Once you have a package imported you can see what is inside of it using the dir command. The following block of code prints a list of all of the functions inside the math package.\nimport math\nprint(dir(math))\n\n\nBy the way: you only need to import a package once in a session. The only reason we are repeating the import statement in each code block is to make it easier to come back to this material later in a new session, where you will need to import the packages again.\nOf course, there will be times when you need help with a function. You can use the help function to view the help documentation for any function. For example, you can run the code help(math.acos) to get help on the arc cosine function from the math package.\n\n\nExercise A.12 Import the math package, figure out how the log function works, and write code to calculate the logarithm of the number 8.3 in base 10, base 2, base 16, and base \\(e\\) (the natural logarithm).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Python</span>"
    ]
  },
  {
    "objectID": "nmPython.html#sec-numpy",
    "href": "nmPython.html#sec-numpy",
    "title": "Appendix A ‚Äî Python",
    "section": "A.3 Numerical Python with NumPy",
    "text": "A.3 Numerical Python with NumPy\nThe base implementation of Python includes the basic programming language, the tools to write loops, check conditions, build and manipulate lists, and all of the other things that we saw in the previous section. In this section we will explore the package numpy that contains optimized numerical routines for doing numerical computations in scientific computing.\n\n\nExample A.25 To start with, let us look at a really simple example. Say you have a list of real numbers and you want to take the sine of every element in the list. If you just try to take the sine of the list you will get an error. Try it yourself.\nfrom math import pi, sin\nmy_list = [0,pi/6, pi/4, pi/3, pi/2, 2*pi/3, 3*pi/4, 5*pi/6, pi]\nsin(my_list)\nYou could get around this error using some of the tools from base Python, but none of them are very elegant from a programming perspective.\nfrom math import pi, sin\nmy_list = [0,pi/6, pi/4, pi/3, pi/2, 2*pi/3, 3*pi/4, 5*pi/6, pi]\nsine_list = [sin(n) for n in my_list]\nsine_list\nfrom math import pi, sin\nmy_list = [0,pi/6, pi/4, pi/3, pi/2, 2*pi/3, 3*pi/4, 5*pi/6, pi]\nsine_list = [ ]\nfor n in range(0,len(my_list)):\n    sine_list.append(sin(my_list[n]))\nsine_list\nPerhaps more simply, say we wanted to square every number in a list. Just appending the code **2 to the end of the list will fail!\nmy_list = [1,2,3,4]\nmy_list**2 # This will produce an error\nIf, instead, we define the list as a numpy array instead of a Python list then everything will work mathematically exactly the way that we intend.\nimport numpy as np\nmy_list = np.array([1,2,3,4])\nmy_list**2 # This will work as expected!  \n\n\n\nExercise A.13 See if you can take the sine of a full list of numbers that are stored in a numpy array.\nHint: you will now see why the numpy package provides its own version of the sine function.\n\n\nThe package numpy is used in many (most) mathematical computations in numerical analysis using Python. It provides algorithms for matrix and vector arithmetic. Furthermore, it is optimized to be able to do these computations in the most efficient possible way (both in terms of memory and in terms of speed).\nTypically when we import numpy we use import numpy as np. This is the standard way to name the numpy package. This means that we will have lots of function with the prefix ‚Äúnp‚Äù in order to call on the numpy functions. Let us first see what is inside the package\nimport numpy as np\ndir(np)\nA brief glimpse through the list reveals a huge wealth of mathematical functions that are optimized to work in the best possible way with the Python language. (We are intentionally not showing the output here since it is quite extensive, run it so you can see.)\n\nA.3.1 Numpy Arrays, Array Operations, and Matrix Operations\nIn the previous section you worked with Python lists. As we pointed out, the shortcoming of Python lists is that they do not behave well when we want to apply mathematical functions to the vector as a whole. The ‚Äúnumpy array‚Äù, np.array, is essentially the same as a Python list with the notable exceptions that\n\nIn a numpy array every entry is a floating point number\nIn a numpy array the memory usage is more efficient (mostly since Python is expecting data of all the same type)\nWith a numpy array there are ready-made functions that can act directly on the array as a matrix or a vector\n\nLet us just look at a few examples using numpy. What we are going to do is to define a matrix \\(A\\) and vectors \\(v\\) and \\(w\\) as \\[\\begin{equation}\nA = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}, \\quad v = \\begin{pmatrix} 5\\\\6 \\end{pmatrix} \\quad \\text{and} \\quad w = v^T = \\begin{pmatrix} 5 & 6 \\end{pmatrix}.\n\\end{equation}\\] Then we will do the following\n\nGet the size and shape of these arrays\nGet individual elements, rows, and columns from these arrays\nTreat these arrays as with linear algebra to\n\ndo element-wise multiplication\ndo matrix a vector products\ndo scalar multiplication\ntake the transpose of matrices\ntake the inverse of matrices\n\n\n\n\nExample A.26 (numpy Matrices) The first thing to note is that a matrix is a list of lists (each row is a list).\nimport numpy as np\nA = np.array([[1,2],[3,4]])\nprint(\"The matrix A is:\\n\",A)\nv = np.array([[5],[6]]) # this creates a column vector\nprint(\"The vector v is:\\n\",v)\nw = np.array([[5,6]]) # this creates a row vector\nprint(\"The vector w is:\\n\",w)\n\n\n\nExample A.27 (.shape) The .shape attribute can be used to give the shape of a numpy array. Notice that the output is a tuple showing the size (rows, columns).\nprint(\"The shape of the matrix A is \", A.shape)\nprint(\"The shape of the column vector v is \", v.shape)\nprint(\"The shape of the row vector w is \", w.shape)\n\n\n\nExample A.28 (.size) The .size attribute can be used to give the size of a numpy array. The size of a matrix or vector will be the total number of elements in the array. You can think of this as the product of the values in the tuple coming from the shape method.\nprint(\"The size of the matrix A is \", A.size)\nprint(\"The size of the column vector v is \", v.size)\nprint(\"The size of the row vector w is \", w.size)\n\n\nReading individual elements from a numpy array is the same, essentially, as reading elements from a Python list. We will use square brackets to get the row and column. Remember that the indexing all starts from 0, not 1!\n\nExample A.29 Let us read the top left and bottom right entries of the matrix \\(A\\).\nimport numpy as np\nA = np.array([[1,2],[3,4]])\nprint(A[0,0]) # top left\nprint(A[1,1]) # bottom right\n\n\n\nExample A.30 Let us read the first row from that matrix \\(A\\).\nimport numpy as np\nA = np.array([[1,2],[3,4]])\nprint(A[0,:])\n\n\n\nExample A.31 Let us read the second column from the matrix \\(A\\).\nimport numpy as np\nA = np.array([[1,2],[3,4]])\nprint(A[:,1])\nNotice when we read the column it was displayed as a row. Be careful. Reading a row or a column from a matrix will automatically flatten it into a 1-dimensional array.\n\n\nIf we try to multiply either \\(A\\) and \\(v\\) or \\(A\\) and \\(A\\) we will get some funky results. Unlike in some programming languages like MATLAB, the default notion of multiplication is NOT matrix multiplication. Instead, the default is element-wise multiplication. You may be familiar with this from R.\n\n\nExample A.32 If we write the code A*A we do NOT do matrix multiplication. Instead we do element-by-element multiplication. This is a common source of issues when dealing with matrices and Linear Algebra in Python.\nimport numpy as np\nA = np.array([[1,2],[3,4]])\nprint(\"Element-wise multiplication:\\n\", A * A)\nprint(\"Matrix multiplication:\\n\", A @ A)\n\n\n\nExample A.33 If we write A * v Python will do element-wise multiplication across each column since \\(v\\) is a column vector. If we want the matrix A to act on v we write A @ v.\nimport numpy as np\nA = np.array([[1,2],[3,4]])\nv = np.array([[5],[6]])\nprint(\"Element-wise multiplication on each column:\\n\", A * v) \n# A @ v will do proper matrix multiplication\nprint(\"Matrix A acting on vector v:\\n\", A @ v)\n\nIt is up to you to check that these products are indeed correct from the definitions of matrix multiplication from Linear Algebra.\nIt remains to show some of the other basic linear algebra operations: inverses, determinants, the trace, and the transpose.\n\n\nExample A.34 (Transpose) Taking the transpose of a matrix (swapping the rows and columns) is done with the .T attribute.\nA.T # The transpose is relatively simple\n\n\n\nExample A.35 (Trace) The trace is done with matrix.trace()\nA.trace() # The trace is pretty darn easy too\nOddly enough, the trace returns a matrix, not a scalar Therefore you will have to read the first entry (index [0,0]) from the answer to just get the trace.\n\n\n\nExample A.36 (Determinant) The determinant function is hiding under the linalg subpackage inside numpy. Therefore we need to call it as such.\nnp.linalg.det(A)\nYou notice an interesting numerical error here. You can do the determinant easily by hand and so know that it should be exactly \\(-2\\). We‚Äôll discuss the source of these kinds of errors in Chapter 1.\n\n\n\nExample A.37 (Inverse) In the linalg subpackage there is also a function for taking the inverse of a matrix.\nA_inv = np.linalg.inv(A)\nA_inv\nWe can check that we get the identity matrix back:\nA @ A_inv\n\n\n\nExercise A.14 Now that we can do some basic linear algebra with numpy it is your turn. Define the matrix \\(B\\) and the vector \\(u\\) as\n\n\\[\\begin{equation}\nB = \\begin{pmatrix} 1 & 4 & 8 \\\\ 2 & 3 & -1 \\\\ 0 & 9 & -3 \\end{pmatrix} \\quad \\text{and} \\quad u = \\begin{pmatrix} 6 \\\\ 3 \\\\ -7 \\end{pmatrix}.\n\\end{equation}\\]\nThen find\n\n\\(Bu\\)\n\\(B^2\\) (in the traditional linear algebra sense)\nThe size and shape of \\(B\\)\n\\(B^T u\\)\nThe element-by-element product of \\(B\\) with itself\nThe dot product of \\(u\\) with the first row of \\(B\\)\n\n\n\n\n\nA.3.2 arange, linspace, zeros, ones, and meshgrid\nThere are a few built-in ways to build arrays in numpy that save a bit of time in many scientific computing settings.\n\n\nExample A.38 The np.arange (array range) function is great for building sequences.\nimport numpy as np\nx = np.arange(0,0.6,0.1)\nx\nnp.arange builds an array of floating point numbers with the arguments start, stop, and step. Note that the stop value itself is not included in the result.\n\n\n\nExample A.39 The np.linspace function builds an array of floating point numbers starting at one point, ending at the next point, and have exactly the number of points specified with equal spacing in between: start, stop, number of points.\nimport numpy as np\ny = np.linspace(0,5,11)\ny\nIn a linear space you are always guaranteed to hit the stop point exactly, but you do not have direct control over the step size.\n\n\n\nExample A.40 The np.zeros function builds an array of zeros. This is handy for pre-allocating memory.\nimport numpy as np\nz = np.zeros((3,5)) # create a 3x5 matrix of zeros\nz\nIf you already have an array and want to create an array of zeros with the same shape, you can use np.zeros_like(array).\nimport numpy as np\nx = np.linspace(0,5,11)\nz = np.zeros_like(x)\nz\nSimilarly there are the functions np.ones and np.ones_like to build arrays of ones.\n\n\n\nExample A.41 The np.meshgrid function builds two arrays that when paired make up the ordered pairs for a 2D (or higher D) mesh grid of points. This is handy for building 2D (or higher dimensional) arrays of data for multi-variable functions. Notice that the output is defined as a tuple.\nimport numpy as np\nx, y = np.meshgrid(np.linspace(0,5,6), np.linspace(0,5,6))\nprint(\"x = \", x)\nprint(\"y = \", y)\nThe thing to notice with the np.meshgrid() function is that when you lay the two arrays on top of each other, the matching entries give every ordered pair in the domain.\nIf the purpose of this is not clear to you yet, don‚Äôt worry. You will see it used a lot later in the module.\n\n\n\nExercise A.15 Now it is time to practice with some of these numpy functions.\n\nCreate a numpy array of the numbers 1 through 10 and square every entry in the list without using a loop.\nCreate a \\(10 \\times 10\\) identity matrix and change the top right corner to a 5. Hint: np.identity()\nFind the matrix-vector product of the answer to part (b) and the answer to part (a).\nChange the bottom row of your matrix from part (b) to all \\(3\\)‚Äôs, then change the third column to all \\(7\\)‚Äôs, and then find the \\(5^{th}\\) power of this matrix.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Python</span>"
    ]
  },
  {
    "objectID": "nmPython.html#sec-matplotlib",
    "href": "nmPython.html#sec-matplotlib",
    "title": "Appendix A ‚Äî Python",
    "section": "A.4 Plotting with Matplotlib",
    "text": "A.4 Plotting with Matplotlib\nA key part of scientific computing is plotting your results or your data. The tool in Python best-suited to this task is the package matplotlib. As with all of the other packages in Python, it is best to learn just the basics first and then to dig deeper later. One advantage to using matplotlib in Python is that it is modelled off of MATLAB‚Äôs plotting tools. People coming from a MATLAB background should feel pretty comfortable here, but there are some differences to be aware of.\n\nA.4.1 Basics with plt.plot()\nWe are going to start right away with an example. In this example, however, we will walk through each of the code chunks one-by-one so that we understand how to set up a proper plot.\nBelow we will mention some tricks for getting the plots to render that only apply to Jupyter Notebooks. If you are using Google Colab then you may not need some of these little tricks.\n\n\nExample A.42 (Plotting with matplotlib) In the first example we want to simply plot the sine function on the domain \\(x \\in [0,2\\pi]\\), colour it green, put a grid on it, and give a meaningful legend and axis labels. To do so we first need to take care of a couple of housekeeping items.\n\nImport numpy so we can take advantage of some good numerical routines.\nImport matplotlib‚Äôs pyplot module. The standard way to pull it in is with the nickname plt (just like with numpy when we import it as np).\n\n\nimport numpy as np \nimport matplotlib.pyplot as plt\n\nIn Jupyter Notebooks the plots will not show up unless you tell the notebook to put them ‚Äúinline.‚Äù Usually we will use the following command to get the plots to show up. You do not need to do this in Google Colab. The percent sign is called a magic command in Jupyter Notebooks. This is not a Python command, but it is a command for controlling the Jupyter IDE specifically.\n%matplotlib inline\nNow we will build a numpy array of \\(x\\) values (using the np.linspace function) and a numpy array of \\(y\\) values from the sine function.\n\n# 100 equally spaced points from 0 to 2pi\nx = np.linspace(0,2*np.pi,100) \ny = np.sin(x)\n\n\nNext, build the plot with plt.plot(). The syntax is: plt.plot(x, y, ‚Äôcolor‚Äô, ...)¬†where you have several options that you can pass (more on that later).\nWe send the plot label directly to the plot function. This is optional and we could set the legend up separately if we like.\nThen we will add the grid with plt.grid()\nThen we will add the legend to the plot\nFinally we will add the axis labels\nWe end the plotting code with plt.show() to tell Python to finally show the plot. This line of code tells Python that you are done building that plot.\n\n\nplt.plot(x,y, 'green', label='The Sine Function')\nplt.grid()\nplt.legend()\nplt.xlabel(\"x axis\")\nplt.ylabel(\"y axis\")\nplt.show()\n\n\n\n\n\n\n\nFigure¬†A.1: The sine function\n\n\n\n\n\n\n\n\nExample A.43 Now let us do a second example, but this time we want to show four different plots on top of each other. When you start a figure, matplotlib is expecting all of those plots to be layered on top of each other. (Note:For MATLAB users, this means that you do not need the hold on command since it is automatically ‚Äúon.‚Äù)\nIn this example we will plot \\[\\begin{equation}\ny_0 = \\sin(2\\pi x) \\quad y_1 = \\cos(2 \\pi x) \\quad y_2 = y_0 + y_1 \\quad \\text{and} \\quad y_3 = y_0 - y_1\n\\end{equation}\\] on the domain \\(x \\in [0,1]\\) with 100 equally spaced points. we will give each of the plots a different line style, built a legend, put a grid on the plot, and give axis labels.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n# %matplotlib inline # you may need this in Jupyter Notebooks\n\n# build the x and y values\nx = np.linspace(0,1,100)\ny0 = np.sin(2*np.pi*x)\ny1 = np.cos(2*np.pi*x)\ny2 = y0 + y1\ny3 = y0 - y1\n\n# plot each of the functions \n# (notice that they will be on the same axes)\nplt.plot(x, y0, 'b-.', label=r\"$y_0 = \\sin(2\\pi x)$\")\nplt.plot(x, y1, 'r--', label=r\"$y_1 = \\cos(2\\pi x)$\")\nplt.plot(x, y2, 'g:', label=r\"$y_2 = y_0 + y_1$\")\nplt.plot(x, y3, 'k-', label=r\"$y_3 = y_0 - y_1$\")\n\n# put in a grid, legend, title, and axis labels\nplt.grid()\nplt.legend()\nplt.title(\"Awesome Graph\")\nplt.xlabel('x axis label')\nplt.ylabel('y axis label')\nplt.show()\n\n\n\n\n\n\n\nFigure¬†A.2: Plots of the sine, cosine, and sums and differences.\n\n\n\n\n\nNotice the r in front of the strings defining the legend. This prevents the backslash that is used a lot in LaTeX to be interpreted as an escape character. These strings are referred to as raw strings.\nThe legend was placed automatically at the lower left of the plot. There are ways to control the placement of the legend if you wish, but for now just let Python and matplotlib have control over the placement.\n\n\n\nExample A.44 Now let us create the same plot with slightly different code. The plot function can take several \\((x, y)\\) pairs in the same line of code. This can really shrink the amount of coding that you have to do when plotting several functions on top of each other.\n\n# The next line of code does all of the plotting of all \n# of the functions.  Notice the order: x, y, color and \n# line style, repeat\nimport numpy as np\nimport matplotlib.pyplot as plt\nx = np.linspace(0,1,100)\ny0 = np.sin(2*np.pi*x)\ny1 = np.cos(2*np.pi*x)\ny2 = y0 + y1\ny3 = y0 - y1\nplt.plot(x, y0, 'b-.', x, y1, 'r--', x, y2, 'g:', x, y3, 'k-')\n\nplt.grid()\nplt.legend([r\"$y_0 = \\sin(2\\pi x)$\",r\"$y_1 = \\cos(2\\pi x)$\",\\\n            r\"$y_2 = y_0 + y_1$\",r\"$y_3 = y_0 - y_1$\"])\nplt.title(\"Awesome Graph\")\nplt.xlabel('x axis label')\nplt.ylabel('y axis label')\nplt.show()\n\n\n\n\n\n\n\nFigure¬†A.3: A second plot of the sine, cosine, and sums and differences.\n\n\n\n\n\n\n\n\nExercise A.16 Plot the functions \\(f(x) = x^2\\), \\(g(x) = x^3\\), and \\(h(x) = x^4\\) on the same axes. Use the domain \\(x \\in [0,1]\\). Put a grid, a legend, a title, and appropriate labels on the axes.\n\n\n\n\nA.4.2 Subplots\nIt is often very handy to place plots side-by-side or as some array of plots. The subplots command allows us that control. The main idea is that we are setting up a matrix of blank plots and then populating the axes with the plots that we want.\n\n\nExample A.45 Let us repeat the previous exercise, but this time we will put each of the plots in its own subplot. There are a few extra coding quirks that come along with building subplots so we will highlight each block of code separately.\n\nFirst we set up the plot area with plt.subplots(). The first two inputs to the subplots command are the number of rows and the number of columns in your plot array. For the first example we will do 2 rows of plots with 2 columns ‚Äì so there are four plots total.\nThen we build each plot individually telling matplotlib which axes to use for each of the things in the plots.\nNotice the small differences in how we set the titles and labels\nIn this example we are setting the \\(y\\)-axis to the interval \\([-2,2]\\) for consistency across all of the plots.\n\n\n# set up the blank matrix of plots\nimport numpy as np\nimport matplotlib.pyplot as plt\nx = np.linspace(0,1,100)\ny0 = np.sin(2*np.pi*x)\ny1 = np.cos(2*np.pi*x)\ny2 = y0 + y1\ny3 = y0 - y1\n\nfig, axes = plt.subplots(nrows = 2, ncols = 2)\n\n# Build the first plot\naxes[0,0].plot(x, y0, 'b-.')\naxes[0,0].grid()\naxes[0,0].set_title(r\"$y_0 = \\sin(2\\pi x)$\")\naxes[0,0].set_ylim(-2,2)\naxes[0,0].set_xlabel(\"x\")\naxes[0,0].set_ylabel(\"y\")\n\n# Build the second plot\naxes[0,1].plot(x, y1, 'r--')\naxes[0,1].grid()\naxes[0,1].set_title(r\"$y_1 = \\cos(2\\pi x)$\")\naxes[0,1].set_ylim(-2,2)\naxes[0,1].set_xlabel(\"x\")\naxes[0,1].set_ylabel(\"y\")\n\n# Build the first plot\naxes[1,0].plot(x, y2, 'g:')\naxes[1,0].grid()\naxes[1,0].set_title(r\"$y_2 = y_0 + y_1$\")\naxes[1,0].set_ylim(-2,2)\naxes[1,0].set_xlabel(\"x\")\naxes[1,0].set_ylabel(\"y\")\n\n# Build the first plot\naxes[1,1].plot(x, y3, 'k-')\naxes[1,1].grid()\naxes[1,1].set_title(r\"$y_3 = y_0 - y_1$\")\naxes[1,1].set_ylim(-2,2)\naxes[1,1].set_xlabel(\"x\")\naxes[1,1].set_ylabel(\"y\")\n\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure¬†A.4: An example of subplots\n\n\n\n\n\nThe fig.tight_layout() command makes the plot labels a bit more readable in this instance (again, something you can play with).\n\n\n\nExercise A.17 Put the functions \\(f(x) = x^2\\), \\(g(x) = x^3\\) and \\(h(x) = x^4\\) in a subplot environment with 1 row and 3 columns of plots. Use the unit interval as the domain and range for all three plot. Make sure that each plot has a grid, appropriate labels, an appropriate title, and the overall figure has a title.\n\nA.4.3 Logarithmic Scaling with semilogy, semilogx, and loglog\nIt is occasionally useful to scale an axis logarithmically. This arises most often when we are examining an exponential function, or some other function, that is close to zero for much of the domain. Scaling logarithmically allows us to see how small the function is getting in orders of magnitude instead of as a raw real number. we will use this often in numerical methods.\n\n\n\n\nExample A.46 In this example we will plot the function \\(y = 10^{-0.01x}\\) on a regular (linear) scale and on a logarithmic scale on the \\(y\\) axis. We use the interval \\([0,500]\\) on the \\(x\\) axis.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nx = np.linspace(0,500,1000)\ny = 10**(-0.01*x)\nfig, axis = plt.subplots(1,2)\n\naxis[0].plot(x,y, 'r')\naxis[0].grid()\naxis[0].set_title(\"Linearly scaled y axis\")\naxis[0].set_xlabel(\"x\")\naxis[0].set_ylabel(\"y\")\n\naxis[1].semilogy(x,y, 'r')\naxis[1].grid()\naxis[1].set_title(\"Logarithmically scaled y axis\")\naxis[1].set_xlabel(\"x\")\naxis[1].set_ylabel(\"Log(y)\")\n\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure¬†A.5: An example of using logarithmic scaling.\n\n\n\n\n\nIt should be noted that the same result can be achieved using the yscale command along with the plot command instead of using the semilogy command. So you could replace\naxis[1].semilogy(x,y, 'r')\nby\naxis[1].plot(x,y, 'r')\naxis[1].set_yscale(\"log\")\nto produce identical results.\n\n\n\nExercise A.18 Plot the function \\(f(x) = x^3\\) for \\(x \\in [0,1]\\) on linearly scaled axes, logarithmic axis in the \\(y\\) direction, logarithmically scaled axes in the \\(x\\) direction, and a log-log plot with logarithmic scaling on both axes. Use subplots to put your plots side-by-side. Give appropriate labels, titles, etc.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Python</span>"
    ]
  },
  {
    "objectID": "nmPython.html#sec-pandas",
    "href": "nmPython.html#sec-pandas",
    "title": "Appendix A ‚Äî Python",
    "section": "A.5 Dataframes with Pandas",
    "text": "A.5 Dataframes with Pandas\nThe Pandas package provides Python with the ability to work with tables of data similar to what R provides via its dataframes. As we will not work much with data in this module, we do not need to dive deep into the Pandas package. In some of the optional exercises you will load in data from files using pd.read_csv().\n\nExample A.47 ¬†\n\nimport pandas as pd\ndf = pd.read_csv('https://github.com/gustavdelius/NumericalAnalysis2025/raw/main/data/Calculus/bikespeed.csv')\ndf\n\n\n\n\n\n\n\n\nTime (sec)\nSpeed (ft/sec)\n\n\n\n\n0\n0\n34\n\n\n1\n10\n32\n\n\n2\n20\n29\n\n\n3\n30\n33\n\n\n4\n40\n37\n\n\n5\n50\n40\n\n\n6\n60\n41\n\n\n7\n70\n36\n\n\n8\n80\n38\n\n\n9\n90\n39\n\n\n\n\n\n\n\n\n\n\nExample A.48 Pandas can also be useful to us for collecting computational results into tables for easier display. In this example we will build a table of the first 10 natural numbers and their squares and cubes. We then display the table.\n\nimport numpy as np\nimport pandas as pd\n\n# Calculate the columns for the table\nn = np.arange(1,11)\nn2 = n**2\nn3 = n**3\n\n# Combine the columns into a data frame with headers\ndf = pd.DataFrame({'n': n, 'n^2': n2, 'n^3': n3})\ndf\n\n\n\n\n\n\n\n\nn\nn^2\nn^3\n\n\n\n\n0\n1\n1\n1\n\n\n1\n2\n4\n8\n\n\n2\n3\n9\n27\n\n\n3\n4\n16\n64\n\n\n4\n5\n25\n125\n\n\n5\n6\n36\n216\n\n\n6\n7\n49\n343\n\n\n7\n8\n64\n512\n\n\n8\n9\n81\n729\n\n\n9\n10\n100\n1000\n\n\n\n\n\n\n\nThis provides an alternative to how we created a tabular display with the print() function in Example¬†A.20.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Python</span>"
    ]
  },
  {
    "objectID": "nmPython.html#sec-python_exercises",
    "href": "nmPython.html#sec-python_exercises",
    "title": "Appendix A ‚Äî Python",
    "section": "A.6 Problems",
    "text": "A.6 Problems\nThese problem exercises here are meant for you to practice and improve your coding skills. Please refrain from relying too much on Gemini or any other AI for solving these exercises. The point is to struggle through the code, get it wrong many times, debug, and then to eventually have working code. So I recommend switching off the AI features in Google Colab for the purpose of these exercises.\nYou do not need to do all the exercises. Do only as many as you have time for and you feel is useful. It might be a good idea to split the exercises up among your group members and then share the code with each other.\n\n\nExercise A.19 (This problem is modified from (‚ÄúProject Euler‚Äù n.d.))\nIf we list all of the numbers below 10 that are multiples of 3 or 5 we get 3, 5, 6, and 9. The sum of these multiples is 23. Write code to find the sum of all the multiples of 3 or 5 below 1000. Your code needs to run error free and output only the sum. There are of course many ways you could approach this exercise. Compare your approach to that of others in your group.\n\n\n\nExercise A.20 (This problem is modified from (‚ÄúProject Euler‚Äù n.d.))\nEach new term in the Fibonacci sequence is generated by adding the previous two terms. By starting with 1 and 2, the first 10 terms will be: \\[\\begin{equation}\n1, 1, 2, 3, 5, 8, 13, 21, 34, 55, \\dots\n\\end{equation}\\] By considering the terms in the Fibonacci sequence whose values do not exceed four million, write code to find the sum of the even-valued terms. Your code needs to run error free and output only the sum.\n\n\n\nExercise A.21 Write computer code that will draw random numbers from the unit interval \\([0,1]\\), distributed uniformly (using Python‚Äôs np.random.rand()), until the sum of the numbers that you draw is greater than 1. Keep track of how many numbers you draw. Then write a loop that does this process many many times. On average, how many numbers do you have to draw until your sum is larger than 1?\n\nHint #1:\n\nUse the np.random.rand()command to draw a single number from a uniform distribution with bounds \\((0,1)\\).\n\nHint #2:\n\nYou should do this more than 1,000,000 times to get a good average ‚Ä¶ and the number that you get should be familiar!\n\n\n\n\n\nExercise A.22 (This problem is modified from (‚ÄúProject Euler‚Äù n.d.))\nThe sum of the squares of the first ten natural numbers is, \\[\\begin{equation}\n1^2 + 2^2 + \\dots + 10^2 = 385\n\\end{equation}\\] The square of the sum of the first ten natural numbers is, \\[\\begin{equation}\n(1 + 2 + \\dots + 10)^2 = 55^2 = 3025\n\\end{equation}\\] Hence the difference between the square of the sum of the first ten natural numbers and the sum of the squares is \\(3025 - 385 = 2640\\).\nWrite code to find the difference between the square of the sum of the first one hundred natural numbers and the sum of the squares. Your code needs to run error free and output only the difference.\n\n\n\nExercise A.23 (This problem is modified from (‚ÄúProject Euler‚Äù n.d.))\nThe prime factors of \\(13195\\) are \\(5, 7, 13\\) and \\(29\\). Write code to find the largest prime factor of the number \\(600851475143\\)? Your code needs to run error free and output only the largest prime factor.\n\n\n\nExercise A.24 (This problem is modified from (‚ÄúProject Euler‚Äù n.d.))\nThe number 2520 is the smallest number that can be divided by each of the numbers from 1 to 10 without any remainder. Write code to find the smallest positive number that is evenly divisible by all of the numbers from 1 to 20?\nHint: You will likely want to use modular division for this problem.\n\n\n\nExercise A.25 The following iterative sequence is defined for the set of positive integers: \\[\\begin{equation}\n\\begin{aligned} & n \\to \\frac{n}{2} \\quad (n \\text{ is even}) \\\\ & n \\to 3n + 1 \\quad (n \\text{ is odd}) \\end{aligned}\n\\end{equation}\\] Using the rule above and starting with \\(13\\), we generate the following sequence: \\[\\begin{equation}\n13 \\to 40 \\to 20 \\to 10 \\to 5 \\to 16 \\to 8 \\to 4 \\to 2 \\to 1\n\\end{equation}\\] It can be seen that this sequence (starting at 13 and finishing at 1) contains 10 terms. Although it has not been proved yet (Collatz Problem), it is thought that all starting numbers finish at 1. This has been verified on computers for massively large starting numbers, but this does not constitute a proof that it will work this way for all starting numbers.\nWrite code to determine which starting number, under one million, produces the longest chain. NOTE: Once the chain starts, the terms are allowed to go above one million.\n\nFootnotes\n\n\n\n\n‚ÄúProject Euler.‚Äù n.d. Accessed December 14, 2023. https://projecteuler.net/.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Python</span>"
    ]
  },
  {
    "objectID": "nmPython.html#footnotes",
    "href": "nmPython.html#footnotes",
    "title": "Appendix A ‚Äî Python",
    "section": "",
    "text": "Actually, it is still an open mathematical question whether every integer seed will converge to 1. The Collatz sequence has been checked for many millions of initial seeds and they all converge to 1, but there is no mathematical proof that it will always happen. You will check the conjecture numerically in Exercise¬†A.25‚Ü©Ô∏é\nThere is the sympy package if you want to do symbolic computations, but we will not use that in this module.‚Ü©Ô∏é",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Python</span>"
    ]
  },
  {
    "objectID": "ex_exam_solns.html",
    "href": "ex_exam_solns.html",
    "title": "Appendix B ‚Äî Exam solutions",
    "section": "",
    "text": "B.1 Numbers\nIn this appendix you find example solutions to the exam-style questions.\n(a) Machine Precision\nMachine precision is the gap between the number 1 and the next larger floating point number. With 4 bits for the mantissa, the smallest number greater than \\(1\\) that can be represented is \\(1.0001_2\\). So machine precision is \\(\\epsilon = 0.0001_2=1/16\\). The machine precision is the upper bound for the relative rounding error.\n(b) Conversion of 13.5\n(c) Addition: 13.5 + 0.25\n(d) Loss of Significance",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Exam solutions</span>"
    ]
  },
  {
    "objectID": "ex_exam_solns.html#numbers",
    "href": "ex_exam_solns.html#numbers",
    "title": "Appendix B ‚Äî Exam solutions",
    "section": "",
    "text": "Sign: Positive (\\(+13.5\\)), so \\(s=0\\).\nConvert \\(13.5\\) to binary:\n\n\\(13_{10} = 1101_2\\).\n\\(0.5_{10} = 0.1_2\\).\nResult: \\(1101.1_2\\).\n\nNormalize: \\(1.1011 \\times 2^3\\).\nExponent (\\(E=3\\)):\n\nStored exponent \\(e = E + 3 = 3 + 3 = 6\\).\n\\(6_{10} = 110_2\\).\n\nMantissa (\\(m\\)): Drop the leading 1 \\(\\rightarrow\\) 1011.\nFinal Bit Pattern: 0 110 1011.\n\n\n\nConvert second number (0.25):\n\n\\(0.25 = 1/4 = 1.0 \\times 2^{-2}\\).\n\nAlign Exponents:\n\n\\(x_1 = 1.1011 \\times 2^3\\)\n\\(x_2 = 1.0000 \\times 2^{-2}\\)\nShift \\(x_2\\) to match exponent \\(3\\): Shift right by \\(3 - (-2) = 5\\) positions.\n\\(x_2 \\rightarrow 0.00001 \\times 2^3\\).\n\nAdd Significands:\n  1.1011      (13.5)\n+ 0.00001     (0.25 aligned)\n-----------\n  1.10111\nRounding:\n\nThe result \\(1.10111_2\\) has 5 fractional bits, but we can only store 4.\nIt lies exactly halfway between \\(1.1011\\) (\\(13.5\\)) and \\(1.1100\\) (\\(14.0\\)).\nTie-breaking rule: ‚ÄúTies to Even‚Äù.\nThe Least Significant Bit (4th bit) is 1 (odd). To make it even, we round up (add 1 to the LSB).\n\\(1.1011 + 0.0001 = 1.1100\\).\n\nFinal Result:\n\nSignificand: \\(1.1100\\)\nValue: \\(1.1100_2 \\times 2^3 = 1110.0_2 = \\mathbf{14.0}\\).\n\nError:\n\nExact sum: \\(13.75\\).\nStored sum: \\(14.0\\).\nAbsolute Error: \\(|13.75 - 14.0| = \\mathbf{0.25}\\).\n\n\n\n\nError Type: Catastrophic Cancellation (or Loss of Significant Digits).\nExplanation: When \\(x\\) is very large (\\(10^8\\)), \\(x^2 + 1 \\approx x^2\\), so \\(\\sqrt{x^2+1} \\approx x\\). Subtracting two extremely close numbers causes the cancellation of the leading digits, leaving only the random ‚Äúnoise‚Äù from the least significant bits.\nImproved (Stable) Formula: Multiply by the conjugate: \\[f(x) = \\frac{(\\sqrt{x^2+1} - x)(\\sqrt{x^2+1} + x)}{\\sqrt{x^2+1} + x} = \\frac{1}{\\sqrt{x^2+1} + x}.\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Exam solutions</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Numerical Analysis",
    "section": "",
    "text": "Introduction\nMathematics is not just an abstract pursuit; it is an essential tool that powers a vast array of applications. From weather forecasting to black hole simulations, from urban planning to medical research, from ecology to epidemiology, the application of mathematics has become indispensable. Central to this applied force is Numerical Analysis.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#sec-what_is_numerical_analysis",
    "href": "index.html#sec-what_is_numerical_analysis",
    "title": "Numerical Analysis",
    "section": "What Is Numerical Analysis?",
    "text": "What Is Numerical Analysis?\nNumerical Analysis is the discipline that bridges continuous mathematical theories with their concrete implementation on digital computers. These computers, by design, work with discrete quantities, and translating continuous problems into this discrete realm is not always straightforward.\nIn this module, we will explore some key techniques, algorithms, and principles of Numerical Analysis that enable us to translate mathematical problems into computational solutions. We will delve into the challenges that arise in this translation, the strategies to overcome them, and the interaction of theory and practice.\nMany mathematical problems cannot be solved analytically in closed form. In Numerical Analysis, we aim to find approximation algorithms for mathematical problems, i.e., schemes that allow us to compute the solution approximately. These algorithms use only elementary operations that computers know how to do (\\(+,-,\\times,/\\)), but often a long sequence of them, so that in practice they need to be run on computers.\n\nExample from Algebra\nSolve the equation \\[\\log(x) = \\sin(x)\\] for \\(x\\) in the interval \\(x \\in (0,\\pi)\\). Stop and try using all of the algebra that you ever learned to find \\(x\\). You will quickly realize that there are no by-hand techniques that can solve this problem! A numerical approximation, however, is not so hard to come by. The following graph shows that there is a solution to this equation somewhere between \\(x=2\\) and \\(x=2.5\\).\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nx = np.linspace(1, 4, 100)\nplt.plot(x, np.log(x), label=\"log(x)\")\nplt.plot(x, np.sin(x), label=\"sin(x)\")\nplt.xlabel(\"x\")\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†1: The graphs of the real functions \\(\\log(x)\\) and \\(\\sin(x)\\) intersect at exactly one point, giving the solution to the equation \\(\\log(x) = \\sin(x)\\).\n\n\n\n\n\n\n\nExample from Calculus\nWhat if we want to evaluate\n\\[\n    \\int_0^\\pi \\sin(x^2) dx?\n\\]\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef f(x):\n    return np.sin(x**2)\n\na = 0\nb = np.pi\nn = 1000  # Number of points for numerical integration\n\nx = np.linspace(a, b, n)\ny = f(x)\n\n# Calculate the numerical integral using the trapezoidal rule\nintegral = np.trapezoid(y, x)\n\n# Shade the positive and negative regions differently\nplt.fill_between(x, y, where=y&gt;=0, color='green', alpha=0.5, label=\"Positive\")\nplt.fill_between(x, y, where=y&lt;0, color='red', alpha=0.5, label=\"Negative\")\n\n# Plot the curve\nplt.plot(x, y, color='black', label=r\"$\\sin(x^2)$\")\n\n# Set labels and title\nplt.xlabel(\"x\")\n\n# Add legend\nplt.legend()\n\n# Show the plot\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†2: Visual representation of the integral of \\(\\sin(x^2)\\) from \\(0\\) to \\(\\pi\\).\n\n\n\n\n\nAgain, trying to use any of the possible techniques for using the Fundamental Theorem of Calculus, and hence finding an antiderivative, on the function \\(\\sin(x^2)\\) is completely hopeless. Substitution, integration by parts, and all of the other techniques that you know will all fail. Again, a numerical approximation is not so difficult and is very fast and gives the value\n\n\nCode\n# Use Simpson's rule to approximate the integral of sin(x^2) from 0 to pi\nfrom scipy.integrate import simpson\nprint(simpson(y, x = x))\n\n\n0.7726517138019184\n\n\nBy the way, this integral (called the Fresnel Sine Integral) actually shows up naturally in the field of optics and electromagnetism, so it is not just some arbitrary integral that was cooked up just for fun.\n\n\nExample from Differential Equations\nSay we needed to solve the differential equation\n\\[\\frac{dy}{dt} = \\sin(y^2) + t.\\]\nThe nonlinear nature of the problem precludes us from using most of the typical techniques (e.g.¬†separation of variables, undetermined coefficients, Laplace Transforms, etc). However, computational methods that result in a plot of an approximate solution can be made very quickly. Here is a plot of the solution up to time \\(t=2.5\\) with initial condition \\(y(0)=0.1\\):\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef f(t, y):\n    return np.sin(y**2) + t\n\n# Initial condition\ny0 = 0.1\n\n# Time span for the solution\nt_span = (0, 2.5)\n\n# Solve the differential equation using SciPy's solver\nsol = solve_ivp(f, t_span, [y0], max_step=0.1, dense_output=True)\n\n# Extract the time values and solution\nt = sol.t\ny = sol.sol(t)[0]  \n\n# Plot the numerical solution\nplt.plot(t, y)\n\n# Labels and title\nplt.xlabel('t')\nplt.ylabel('y')\n\n# Show the plot\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†3: Plot of numerical solution of \\(dy/dt=\\sin(y^2)+t\\) with \\(y(0)=0.1\\).\n\n\n\n\n\nThis was an artificial example, but differential equations are central to modelling the real world in order to predict the future. They are the closest thing we have to a crystal ball. Here is a plot of a numerical solution of the SIR model of the evolution of an epidemic over time:\n\n\nCode\nimport numpy as np\nfrom scipy.integrate import odeint\nimport matplotlib.pyplot as plt\n\n# SIR model differential equations\ndef sir_model(y, t, N, beta, gamma):\n    S, I, R = y\n    dSdt = -beta * S * I / N\n    dIdt = beta * S * I / N - gamma * I\n    dRdt = gamma * I\n    return dSdt, dIdt, dRdt\n\n# Total population, N\nN = 1000\n# Initial number of infected and recovered individuals\nI0, R0 = 1, 0\n# Everyone else is susceptible to infection initially\nS0 = N - I0 - R0\n# Contact rate, beta, and mean recovery rate, gamma, (in 1/days)\nbeta, gamma = 0.25, 1./20 \n# A grid of time points (in days)\nt = np.linspace(0, 160, 160)\n\n# Initial conditions vector\ny0 = S0, I0, R0\n# Integrate the SIR equations over the time grid, t\nret = odeint(sir_model, y0, t, args=(N, beta, gamma))\nS, I, R = ret.T\n\n# Plot the data on three separate curves for S(t), I(t) and R(t)\nplt.figure(figsize=(10,6))\nplt.plot(t, S, 'b', alpha=0.7, linewidth=2, label='Susceptible')\nplt.plot(t, I, 'y', alpha=0.7, linewidth=2, label='Infected')\nplt.plot(t, R, 'g', alpha=0.7, linewidth=2, label='Recovered')\nplt.xlabel('Time /days')\nplt.ylabel('Number (1000s)')\nplt.ylim(0, N)\nplt.title('SIR Model Simulation')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†4: Plot of a numerical solution of the SIR model\n\n\n\n\n\n\n\nReasons to study Numerical Analysis\nSo why should you want to venture into Numerical Analysis rather than just use the computer as a black box?\n\nPrecision and Stability: Computers, despite their power, can introduce significant errors if mathematical problems are implemented without care. Numerical Analysis offers techniques to ensure we obtain results that are both accurate and stable.\nEfficiency: Real-world applications often demand not just correctness, but efficiency. By grasping the methods of Numerical Analysis, we can design algorithms that are both accurate and resource-efficient.\nBroad Applications: Whether your interest lies in physics, engineering, biology, finance, or many other scientific fields, Numerical Analysis provides the computational tools to tackle complex problems in these areas.\nBasis for Modern Technologies: Core principles of Numerical Analysis are foundational in emerging fields such as artificial intelligence, quantum computing, and data science.\n\nThe prerequisites for this material include a firm understanding of calculus and linear algebra and a good understanding of the basics of differential equations.\nBy the end of this module, you will not merely understand the methods of Numerical Analysis; you will be equipped to apply them efficiently and effectively in diverse scenarios: you will be able to tackle problems in physics, engineering, biology, finance, and many other fields; you will be able to design algorithms that are both accurate and resource-efficient; you will be able to ensure that your computational solutions are both accurate and stable; you will be able to leverage the power of computers to solve complex problems.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#sec-how_this_module_works",
    "href": "index.html#sec-how_this_module_works",
    "title": "Numerical Analysis",
    "section": "How this module works",
    "text": "How this module works\nThere are 4 one-hour whole-class sessions every week. Three of these are listed on your timetable as ‚ÄúLecture‚Äù and one as ‚ÄúComputer Practical‚Äù. However, in all these sessions you, the student, are the one that is doing the work; discovering methods, writing code, working problems, leading discussions, and pushing the pace. I, the lecturer, will act as a guide who only steps in to redirect conversations or to provide necessary insight. You will use the whole-class sessions to share and discuss your work with the other members of your group. There will also be some whole-class discussions moderated by me.\nYou will find that this text is not a set of lecture notes. Instead it mostly just contains collections of exercises with minimal interweaving exposition. It is expected that you do every one of the exercises in the main body of each chapter and use the sequencing of the exercises to guide your learning and understanding.\nTherefore the whole-class sessions form only a very small part of your work on this module. For each hour of whole-class work you should timetable yourself about two and a half hours of work outside class for working through the exercises on your own. I strongly recommend that you put those two and a half hours (ten hours spread throughout the week) into your timetable.\nIn order to enable you to get immediate feedback on your work also in between class sessions, I have made feedback quizzes where you can test your understanding of the material and your results from some of the exercises. Exercises that have an associated question in the feedback quiz are marked with a üéì.\nThere are no traditional problem sheets in this module. In this module you will be working on exercises continuously throughout the week rather than working through a problem sheet only every other week.\nAt the end of each chapter there is a section entitled ‚ÄúProblems‚Äù that contains additional exercises aimed at consolidating your new understanding and skills. These are optional. Many of the chapters also have a section entitled ‚ÄúProjects‚Äù. These projects are more open-ended investigations, designed to encourage creative mathematics, to push your coding skills and to require you to write and communicate mathematics. These projects are entirely optional and perhaps you will like to return to one of these even after the module has finished. If you do work on one of the projects, be sure to share your work with me at gustav.delius@york.ac.uk because will be very interested, also after the end of the module.\nIf you notice any mistakes or unclear things in the learning guide, please point them out to me in the class sessions or at gustav.delius@york.ac.uk. Many thanks go to Ben Mason and Toby Cheshire for the corrections they had sent in for a previous version of this guide.\nYou will need two notebooks for working through the exercises in this guide: one in paper form and one electronic. Some of the exercises are pen-and-paper exercises (and often these are marked with a pen icon ‚úèÔ∏è) while others are coding exercises (often marked with a computer icon üíª) and some require both writing or sketching and coding. You can keep your two notebooks linked through the numbering of the exercises. There are also some exercises that are marked with a Discussion icon üí¨. These are exercises that you should be discussing with other members of your group.\nYou will keep your coding notebooks in Google Colab, which we will discuss below. Most students find it easiest to have one dedicated Colab notebook per section, but some students will want to have one per chapter. You are highly encouraged to write explanatory text into your Google Colab notebooks as you go so that future-you can tell what it is that you were doing, which problem(s) you were solving, and what your thought processes were.\nIn the end, your collection of notebooks will contain solutions to every exercise in the guide and can serve as a reference manual for future numerical analysis problems. At the end of each of your notebooks you may also want to add a summary of what you have learned, which will both consolidate your learning and make it easier for you to remind yourself of your new skills later.\nOne request: do not share your notebooks publicly on the internet because that would create temptation for future students to not put in the work themselves, thereby robbing them of the learning experience.\nIf you have a computer, bring it along to the class sessions. However this is not a requirement. I will bring along some spare machines to make sure that every group has at least one computer to use during every session. The only requirements for a computer to be useful for this module is that it can connect to the campus WiFi, can run a web browser, and has a physical keyboard (typing code on virtual keyboards is too slow). The ‚ÄúComputer Practical‚Äù takes place in a PC classroom, so there will of course be plenty of machines available then.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#sec-assessment",
    "href": "index.html#sec-assessment",
    "title": "Numerical Analysis",
    "section": "Assessment",
    "text": "Assessment\nUnfortunately, your learning in the module also needs to be assessed. The final mark will be made up of 40% coursework and 60% final exam.\nThe 40% coursework mark will come from 10 short quizzes that will take place during the ‚ÄúComputer practical‚Äù in weeks 2 to 11. Answering each quiz should take less than 5 minutes but you will be given 16 minutes each to complete the quizzes in order to give you a large safety margin and remove stress. Late answers will not be accepted. The quizzes will be based on exercises that you will already have worked through and for which you will have had time to discuss them in class, so they will be really easy if you have engaged with the exercises as intended. Each quiz will be worth 5 points. There will be a practice quiz in the computer practical in week 1.\nWhile working on the assessment quizzes you can already check your answers. If one of your answers is incorrect you can correct it and submit it again. However this will attract a penalty of 30% of the total mark for that answer. For multiple choice questions the penalty for a wrong answer may be even higher to discourage guessing. So it pays to be careful.\nDuring the assessment quizzes you will be required to work exclusively on one of the PCs in the computer practical room rather than your own machine. This means in particular that you need to be physically present for the assessment. While working on the quiz you are only allowed to use a web browser, and the only pages you are allowed to have open are this guide, the quiz page on Moodle and any of your notebooks on Google Colab, with the AI features switched off. You are not allowed to use any AI assistants or other web pages. Besides your digital notebooks on Google Colab you may also use any hand-written notes as long as you have written them yourself.\nTo allow for the fact that there may be weeks in which you are ill or otherwise prevented from performing to your best in the assessment quizzes, your final coursework mark will be calculated as the average over your 8 best marks. If exceptional circumstances affect more than two of the 10 quizzes then you would need to submit an exceptional circumstances claim.\nThe 60% final exam will be a 2 hour exam of the usual closed-book form in an exam room during the exam period. To prepare yourself for the final exam, there will be an exam style question at the end of each chapter, and I will make a practice exam available at the end of the module.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#sec-textbooks",
    "href": "index.html#sec-textbooks",
    "title": "Numerical Analysis",
    "section": "Textbooks",
    "text": "Textbooks\nIn this module we will only scratch the surface of the vast subject that is Numerical Analysis. The aim is for you at the end of this module to be familiar with some key ideas and to have the confidence to engage with new methods when they become relevant to you.\nThere are many textbooks on Numerical Analysis. Standard textbooks are (Burden and Faires 2010) and (Kincaid and Cheney 2009). They contain much of the material from this module. A less structured and more opinionated account can be found in (Acton 1990). Another well known reference that researchers often turn to for solutions to specific tasks is (Press et al. 2007). You will find many others in the library. They may go also under alternative names like ‚ÄúNumerical Methods‚Äù or ‚ÄúScientific Computing‚Äù.\nYou may also want to look at textbooks for specific topics covered in this module, like for example (Butcher 2016) for methods for ordinary differential equations.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#sec-your_jobs",
    "href": "index.html#sec-your_jobs",
    "title": "Numerical Analysis",
    "section": "Your jobs",
    "text": "Your jobs\nYou have the following jobs as a student in this module:\n\nFight! You will have to fight hard to work through this material. The fight is exactly what we are after since it is ultimately what leads to innovative thinking.\nScrew Up! More accurately, do not be afraid to screw up. You should write code, work problems, and develop methods, then be completely unafraid to scrap what you have done and redo it from scratch.\nCollaborate! You should collaborate with your peers, both within your group and across the whole class. Discuss exercises, ask questions, help others.\nEnjoy! Part of the fun of inquiry-based learning is that you get to experience what it is like to think like a true mathematician / scientist. It takes hard work but ultimately this should be fun!",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#python",
    "href": "index.html#python",
    "title": "Numerical Analysis",
    "section": "Python",
    "text": "Python\nTo properly understand numerical analysis, you will need to write code in order to experiment with the methods we discuss. You will be using Python for this purpose. or most of you, coding in Python will not be new. For example, you have probably seen it in your first year ‚ÄúMathematical Programming & Skills‚Äù module. But perhaps you have never seen Python before. You will have some notion of what a programming language ‚Äúis‚Äù and ‚Äúdoes‚Äù, but you may never have written any code. That is all right. You will pick it up as you go along.\nAppendix A ‚Äî Python covers some of the basics of Python programming that we will use in this module. If you are new to Python, don‚Äôt feel that you need to work through this appendix in one go. Instead, spread the work over the first two weeks of the course and interlace it with your work on the first two chapters.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#sec-colab",
    "href": "index.html#sec-colab",
    "title": "Numerical Analysis",
    "section": "Google Colab",
    "text": "Google Colab\nEvery computer is its own unique flower with its own unique requirements. Hence, we will not spend time here giving you all of the ways that you can install Python and all of the associated packages necessary for this module. Instead I ask that you use the Google Colab notebook tool for writing and running your Python code: https://colab.research.google.com.\nGoogle Colab allows you to keep all of your Python code on your Google Drive. The Colab environment is a free and collaborative version of the popular Jupyter notebook project. Jupyter notebooks allow you to write and test code as well as to mix writing (including LaTeX formatting) in along with your code and your output. I recommend that if you are new to Google Colab, you start by watching the brief introductory video.\nNow the time has come for the first two exercises of this module.\n\nExercise 1 üíª Spend a bit of time poking around in Colab. Figure out how to\n\nCreate new Colab notebooks.\nAdd and delete code cells.\nType a simple calculation like 1+1 into a code cell and evaluate it.\nAdd and delete text cells.\nAdd an equation to a text cell using LaTeX notation.\nSave a notebook to your Google Drive.\nOpen a notebook from Google Drive.\nShare a notebook with other members of your group and see if you can collaboratively edit it.\n\n\n\n\nExercise 2 üíª Click on this link to a Colab notebook. It should open it in Colab. Then save a copy of it to your Google Drive. You need a copy because you will not have permission to edit the original. Follow the instructions in the notebook.\n\n\n\nThe use of AI\nYou will have gathered from the previous exercise that in this module you are not only allowed to use AI, you are encouraged to use AI. However you have probably already discovered that you get more out of an AI if you are already familiar with the basic concepts of a subject. You will need to be able to understand and check any answer an AI gives you. If there is something in an AI answer that is not totally clear or not obviously correct, always ask the AI to explain the details of its answer and ask follow-on questions until everything is crystal-clear.\nDuring the 10 assessment quizzes you will not be allowed to use any AI. In particular you will be required to switch off the AI features in Google Colab. It is thus a good idea when working on practice exercises to also switch off the AI features to make sure you know what you are doing even when there is no AI assistance. To switch off the AI features you should tick the ‚ÄúHide generative AI‚Äù checkbox on the ‚ÄúAI assistance‚Äù tab of the ‚ÄúSettings‚Äù page in Google Colab.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#about-you",
    "href": "index.html#about-you",
    "title": "Numerical Analysis",
    "section": "About you",
    "text": "About you\n\nExercise 3 üíª I am interested to learn more about you to help me make this module work well for you. It would be helpful if you would answer the questionnaire at https://forms.gle/YHHExaAnVzcLtBcU7. Of course you are free to share as much or as little as you like.\n\n\n¬© Gustav Delius. This learning guide is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. You may copy, distribute, display, remix, rework, and perform this copyrighted work, as long as you give credit to Gustav Delius gustav.delius@york.ac.uk and to the late Eric Sullivan, formerly Mathematics Faculty at Carroll College, on whose learning guide this one is based.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Numerical Analysis",
    "section": "References",
    "text": "References\n\n\n\n\nActon, Forman S. 1990. Numerical Methods That Work. 1St Edition edition. Washington, D.C: The Mathematical Association of America.\n\n\nBurden, Richard L., and J. Douglas Faires. 2010. Numerical Analysis. 9th ed. Brooks Cole.\n\n\nButcher, J. C. 2016. Numerical Methods for Ordinary Differential Equations. Third edition. Wiley. https://yorsearch.york.ac.uk/permalink/f/1kq3a7l/44YORK_ALMA_DS51336126850001381.\n\n\nKincaid, D. R., and E. W. Cheney. 2009. Numerical Analysis: Mathematics of Scientific Computing. Pure and Applied Undergraduate Texts. American Mathematical Society.\n\n\nPress, William H., Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. 2007. Numerical Recipes: The Art of Scientific Computing. Cambridge University Press. https://numerical.recipes/.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "nmLinAlg.html",
    "href": "nmLinAlg.html",
    "title": "Appendix C ‚Äî Linear Algebra",
    "section": "",
    "text": "C.1 Intro to Numerical Linear Algebra\nThe preceding comment says it all ‚Äì linear algebra is the most important of all of the mathematical tools that you can learn as a practitioner of the mathematical sciences. The theorems, proofs, conjectures, and big ideas in almost every other mathematical field find their roots in linear algebra. Numerical Linear Algebra is the study of algorithms for solving problems in linear algebra. This subject has a somewhat different flavour than the numerical analysis we are studying in the main text and hence we present it as an optional appendix.\nOur goal in this appendix is to explore numerical algorithms for the primary questions of linear algebra:\nTake careful note that in our current digital age, numerical linear algebra and its fast algorithms are behind the scenes for wide varieties of computing applications. Applications of numerical linear algebra include:\nWhat‚Äôs more, researchers have found provably optimal ways to perform most of the typical tasks of linear algebra so most scientific software works very well and very quickly with linear algebra.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>¬† <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "nmLinAlg.html#intro-to-numerical-linear-algebra",
    "href": "nmLinAlg.html#intro-to-numerical-linear-algebra",
    "title": "Appendix C ‚Äî Linear Algebra",
    "section": "",
    "text": "solving systems of equations,\nfinding eigenvalue-eigenvector pairs for a matrix.\n\n\n\nbuilding neural networks and AI algorithms,\ndetermining the most important web page in a Google search,\nmodelling realistic 3D environments in video games,\ndigital image processing,\nand many many more.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>¬† <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "nmLinAlg.html#notation",
    "href": "nmLinAlg.html#notation",
    "title": "Appendix C ‚Äî Linear Algebra",
    "section": "C.2 Notation",
    "text": "C.2 Notation\nThroughout this chapter we will use the following notation conventions.\n\nA bold mathematical symbol such as \\(\\boldsymbol{x}\\) or \\(\\boldsymbol{u}\\) will represent a vector.\nIf \\(\\boldsymbol{u}\\) is a vector then \\(u_j\\) will be the \\(j^{th}\\) entry of the vector.\nVectors will typically be written vertically with parenthesis as delimiters such as \\[\\begin{equation}\n\\boldsymbol{u} = \\begin{pmatrix} 1\\\\2\\\\3 \\end{pmatrix}.\n\\end{equation}\\]\nTwo bold symbols separated by a centred dot such as \\(\\boldsymbol{u} \\cdot \\boldsymbol{v}\\) will represent the dot product of two vectors.\nA capital mathematical symbol such as \\(A\\) or \\(X\\) will represent a matrix\nIf \\(A\\) is a matrix then \\(A_{ij}\\) will be the element in the \\(i^{th}\\) row and \\(j^{th}\\) column of the matrix.\nA matrix will typically be written with parenthesis as delimiters such as \\[\\begin{equation}\nA = \\begin{pmatrix} 1 & 2 & 3 \\\\ 4 & 5 & \\pi \\end{pmatrix}.\n\\end{equation}\\]\nThe juxtaposition of a capital symbol and a bold symbol such as \\(A\\boldsymbol{x}\\) will represent matrix-vector multiplication.\nA lower case or Greek mathematical symbol such as \\(x\\), \\(c\\), or \\(\\lambda\\) will represent a scalar.\nThe scalar field of real numbers is given as \\(\\mathbb{R}\\) and the scalar field of complex numbers is given as \\(\\mathbb{C}\\).\nThe symbol \\(\\mathbb{R}^n\\) represents the collection of all \\(n\\)-dimensional vectors where the elements are drawn from the real numbers.\nThe symbol \\(\\mathbb{C}^n\\) represents the collection of all \\(n\\)-dimensional vectors where the elements are drawn from the complex numbers.\n\nIt is an important part of learning to read and write linear algebra to give special attention to the symbolic language so you can communicate your work easily and efficiently.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>¬† <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "nmLinAlg.html#vectors-and-matrices-in-python",
    "href": "nmLinAlg.html#vectors-and-matrices-in-python",
    "title": "Appendix C ‚Äî Linear Algebra",
    "section": "C.3 Vectors and Matrices in Python",
    "text": "C.3 Vectors and Matrices in Python\nWe first need to understand how Python‚Äôs numpy library builds and stores vectors and matrices. The following exercises will give you some experience building and working with these data structures and will point out some common pitfalls that mathematicians fall into when using Python for linear algebra.\n\n\nExample C.1 (numpy Arrays) In Python you can build a list using square brackets such as [1,2,3]. This is called a ‚ÄúPython list‚Äù and is NOT a vector in the way that we think about it mathematically. It is simply an ordered collection of objects. To build mathematical vectors in Python we need to use numpy arrays with np.array(). For example, the vector \\[\\begin{equation}\n\\boldsymbol{u} = \\begin{pmatrix} 1\\\\2\\\\3\\end{pmatrix}\n\\end{equation}\\] would be built with the following code.\nimport numpy as np\nu = np.array([1,2,3])\nprint(u)\nNotice that Python defines the vector u as a matrix with only one dimension. You can see that in the following code.\nimport numpy as np\nu = np.array([1,2,3])\nprint(\"The length of the u vector is \\n\",len(u))\nprint(\"The shape of the u vector is \\n\",u.shape)\n\n\n\nExample C.2 (numpy Matrices) In numpy, a matrix is validly built as a list of lists. For example, the matrix \\[\\begin{equation}\nA = \\begin{pmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{pmatrix}\n\\end{equation}\\] is defined using np.array() where each row is an individual list, and the matrix is a collection of these lists.\nimport numpy as np\nA = np.array([[1,2,3],[4,5,6],[7,8,9]])\nprint(A)\nMoreover, we can extract the shape, the number of rows, and the number of columns of \\(A\\) using the A.shape command. To be a bit more clear on this one we will use the matrix \\[\\begin{equation}\nA = \\begin{pmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{pmatrix}\n\\end{equation}\\]\nimport numpy as np\nA = np.array([[1,2,3],[4,5,6]])\nprint(\"The shape of the A matrix is \\n\",A.shape)\nprint(\"Number of rows in A is \\n\",A.shape[0])\nprint(\"Number of columns in A is \\n\",A.shape[1])\n\n\n\nExample C.3 (Row and Column Vectors in Python) You can more specifically build row or column vectors in Python using the np.array() command and then only specifying one row or column. But take careful note that numpy treats a 1D array (like np.array([1,2,3])) differently from a 2D array (like np.array([[1,2,3]])). For example, if you want the vectors \\[\\begin{equation}\n\\boldsymbol{u} = \\begin{pmatrix} 1\\\\2\\\\3\\end{pmatrix} \\quad \\text{and} \\quad \\boldsymbol{v} = \\begin{pmatrix} 4 & 5 & 6 \\end{pmatrix}\n\\end{equation}\\] then we would use the following Python code.\nimport numpy as np\nu = np.array([[1],[2],[3]])\nprint(\"The column vector u is \\n\",u)\nv = np.array([[1,2,3]])\nprint(\"The row vector v is \\n\",v)\nAlternatively, if you want to define a column vector you can define a row vector (since there are far fewer brackets to keep track of) and then transpose the matrix to turn it into a column. Note that the .transpose() method (or .T) only swaps dimensions; it won‚Äôt turn a 1D array into a 2D column vector unless it was already 2D.\nimport numpy as np\nu = np.array([[1,2,3]])\nu = u.transpose()\nprint(\"The column vector u is \\n\",u)\n\n\n\nExample C.4 (Matrix Indexing) Python indexes all arrays, vectors, lists, and matrices starting from index 0. Let us get used to this fact.\nConsider the matrix \\(A\\) defined in the previous problem. Mathematically we know that the entry in row 1 column 1 is a 1, the entry in row 1 column 2 is a 2, and so on. However, with Python we need to shift the way that we enumerate the rows and columns of a matrix. Hence we would say that the entry in row 0 column 0 is a 1, the entry in row 0 column 1 is a 2, and so on.\nMathematically we can view all Python matrices as follows. If \\(A\\) is an \\(n \\times n\\) matrix then \\[\\begin{equation}\nA = \\begin{pmatrix} A_{0,0} & A_{0,1} & A_{0,2} & \\cdots & A_{0,n-1} \\\\ A_{1,0} & A_{1,1} & A_{1,2} & \\cdots & A_{1,n-1} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ A_{n-1,0} & A_{n-1,1} & A_{n-1,2} & \\cdots & A_{n-1,n-1} \\end{pmatrix}\n\\end{equation}\\]\nSimilarly, we can view all vectors as follows. If \\(\\boldsymbol{u}\\) is an \\(n \\times 1\\) vector then \\[\\begin{equation}\n\\boldsymbol{u} = \\begin{pmatrix} u_0 \\\\ u_1 \\\\ \\vdots \\\\ u_{n-1} \\end{pmatrix}\n\\end{equation}\\]\nThe following code should help to illustrate this indexing convention.\nimport numpy as np\nA = np.array([[1,2,3],[4,5,6],[7,8,9]])\nprint(\"Entry in row 0 column 0 is\",A[0,0])\nprint(\"Entry in row 0 column 1 is\",A[0,1])\nprint(\"Entry in the bottom right corner\",A[2,2])\n\n\n\nExercise C.1 Build your own matrix in Python and practice choosing individual entries from the matrix.\n\n\n\nExample C.5 (Matrix Slicing) The last thing that we need to be familiar with is slicing a matrix. The term ‚Äúslicing‚Äù generally refers to pulling out individual rows, columns, entries, or blocks from a list, array, or matrix in Python. Examine the code below to see how to slice parts out of a numpy matrix.\nimport numpy as np\nA = np.array([[1,2,3],[4,5,6],[7,8,9]])\nprint(A)\nprint(\"The first column of A is \\n\",A[:,0])\nprint(\"The second row of A is \\n\",A[1,:])\nprint(\"The top left 2x2 sub matrix of A is \\n\",A[:-1,:-1])\nprint(\"The bottom right 2x2 sub matrix of A is \\n\",A[1:,1:])\nu = np.array([1,2,3,4,5,6])\nprint(\"The first 3 entries of the vector u are \\n\",u[:3])\nprint(\"The last entry of the vector u is \\n\",u[-1])\nprint(\"The last two entries of the vector u are \\n\",u[-2:])\n\n\n\nExercise C.2 Define the matrix \\(A\\) and the vector \\(u\\) in Python. Then perform all of the tasks below. \\[\\begin{equation}\nA = \\begin{pmatrix} 1 & 3 & 5 & 7 \\\\ 2 & 4 & 6 & 8 \\\\ -3 & -2 & -1 & 0 \\end{pmatrix} \\quad \\text{and} \\quad \\boldsymbol{u} = \\begin{pmatrix} 10\\\\20\\\\30 \\end{pmatrix}\n\\end{equation}\\]\n\nPrint the matrix \\(A\\), the vector \\(\\boldsymbol{u}\\), the shape of \\(A\\), and the shape of \\(\\boldsymbol{u}\\).\nPrint the first column of \\(A\\).\nPrint the first two rows of \\(A\\).\nPrint the first two entries of \\(\\boldsymbol{u}\\).\nPrint the last two entries of \\(\\boldsymbol{u}\\).\nPrint the bottom left \\(2 \\times 2\\) submatrix of \\(A\\).\nPrint the middle two elements of the middle row of \\(A\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>¬† <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "nmLinAlg.html#matrix-and-vector-operations",
    "href": "nmLinAlg.html#matrix-and-vector-operations",
    "title": "Appendix C ‚Äî Linear Algebra",
    "section": "C.4 Matrix and Vector Operations",
    "text": "C.4 Matrix and Vector Operations\nNow let us start doing some numerical linear algebra. We start our discussion with the basics: the dot product and matrix multiplication. The numerical routines in Python‚Äôs numpy packages are designed to do these tasks in very efficient ways but it is a good coding exercise to build your own dot product and matrix multiplication routines just to further cement the way that Python deals with these data structures and to remind you of the mathematical algorithms. What you will find in numerical linear algebra is that the indexing and the housekeeping in the codes is the hardest part. So why do not we start ‚Äúeasy.‚Äù\n\nC.4.1 The Dot Product\n\nExercise C.3 This problem is meant to jog your memory about dot products, how to compute them, and what you might use them for. If your linear algebra is a bit rusty then read ahead a bit and then come back to this problem.\nConsider two vectors \\(\\boldsymbol{u}\\) and \\(\\boldsymbol{v}\\) defined as \\[\\begin{equation}\n\\boldsymbol{u} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} \\quad \\text{and} \\quad \\boldsymbol{v} = \\begin{pmatrix} 3\\\\4 \\end{pmatrix}.\n\\end{equation}\\]\n\nDraw a picture showing both \\(\\boldsymbol{u}\\) and \\(\\boldsymbol{v}\\).\nWhat is \\(\\boldsymbol{u} \\cdot \\boldsymbol{v}\\)?\nWhat is \\(\\|\\boldsymbol{u}\\|\\)?\nWhat is \\(\\|\\boldsymbol{v}\\|\\)?\nWhat is the angle between \\(\\boldsymbol{u}\\) and \\(\\boldsymbol{v}\\)?\nGive two reasons why we know that \\(\\boldsymbol{u}\\) is not perpendicular to \\(\\boldsymbol{v}\\).\nWhat is the scalar projection of \\(\\boldsymbol{u}\\) onto \\(\\boldsymbol{v}\\)? Draw this scalar projections on your picture from part (1).\nWhat is the scalar projection of \\(\\boldsymbol{v}\\) onto \\(\\boldsymbol{u}\\)? Draw this scalar projections on your picture from part (1).\n\n\n\nNow let us get the formal definitions of the dot product on the table.\n\nDefinition C.1 (Dot product) The dot product of two vectors \\(\\boldsymbol{u}, \\boldsymbol{v} \\in \\mathbb{R}^n\\) is \\[\\begin{equation}\n\\boldsymbol{u} \\cdot \\boldsymbol{v} = \\sum_{j=1}^n u_j v_j.\n\\end{equation}\\] Without summation notation the dot product of two vectors is , \\[\\begin{equation}\n\\boldsymbol{u} \\cdot \\boldsymbol{v} = u_1 v_1 + u_2 v_2 + \\cdots + u_n v_n.\n\\end{equation}\\]\nYou may also recall that the dot product of two vectors is given geometrically as \\[\\begin{equation}\n\\boldsymbol{u} \\cdot \\boldsymbol{v} = \\|\\boldsymbol{u} \\| \\|\\boldsymbol{v}\\| \\cos \\theta\n\\end{equation}\\] where \\(\\|\\boldsymbol{u}\\|\\) and \\(\\|\\boldsymbol{v}\\|\\) are the magnitudes (or lengths) of \\(\\boldsymbol{u}\\) and \\(\\boldsymbol{v}\\) respectively, and \\(\\theta\\) is the angle between the two vectors. In physical applications the dot product is often used to find the angle between two vectors (e.g.¬†between two forces). Hence, the last form of the dot product is often rewritten as \\[\\begin{equation}\n\\theta = \\cos^{-1}\\left( \\frac{\\boldsymbol{u} \\cdot \\boldsymbol{v}}{ \\|\\boldsymbol{u} \\| \\| \\boldsymbol{v} \\|} \\right).\n\\end{equation}\\]\n\n\n\nDefinition C.2 (Magnitude of a Vector) The magnitude of a vector \\(\\boldsymbol{u} \\in \\mathbb{R}^n\\) is defined as 1 \\[\\begin{equation}\n\\| \\boldsymbol{u} \\| = \\sqrt{\\boldsymbol{u} \\cdot \\boldsymbol{u}}.\n\\end{equation}\\]\n\n\n\nExercise C.4 Write a Python function that accepts two vectors (defined as numpy arrays) and returns the dot product. Write this code without the use any loops.\nimport numpy as np\ndef myDotProduct(u,v):\n    return # the dot product formula uses a product inside a sum.\n\n\n\nExercise C.5 Test your myDotProduct() function on several dot products to make sure that it works. Example code to find the dot product between \\[\\begin{equation}\n\\boldsymbol{u} = \\begin{pmatrix} 1 \\\\ 2\\\\ 3\\end{pmatrix} \\quad \\text{and} \\quad \\boldsymbol{v} = \\begin{pmatrix} 4\\\\5\\\\6\\end{pmatrix}\n\\end{equation}\\] is given below. Test your code on other vectors. Then implement an error catch into your code to catch the case where the two input vectors are not the same size. You will want to use the len() command to find the length of the vectors.\nu = np.array([1,2,3])\nv = np.array([4,5,6])\nmyDotProduct(u,v)\n\n\n\nExercise C.6 Try sending Python lists instead of numpy arrays into your myDotProduct function. What happens? Why does it happen? What is the cautionary tale here? Modify your myDotProduct() function one more time so that it starts by converting the input vectors into numpy arrays.\nu = [1,2,3]\nv = [4,5,6]\nmyDotProduct(u,v)\n\n\n\nExercise C.7 The numpy library in Python has a built-in command for doing the dot product: np.dot(). Test the np.dot() command and be sure that it does the same thing as your myDotProduct() function.\n\n\n\n\nC.4.2 Matrix Multiplication\nNext we will blow the dust off of your matrix multiplication skills.\n\nExercise C.8 Verify that the product of \\(A\\) and \\(B\\) is indeed what we show below. Work out all of the details by hand. \\[\\begin{equation}\nA = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{pmatrix} \\qquad B = \\begin{pmatrix} 7 & 8 & 9 \\\\ 10 & 11 & 12 \\end{pmatrix}\n\\end{equation}\\]\n\\[\\begin{equation}\nAB = \\begin{pmatrix} 27 & 30 & 33 \\\\ 61 & 68 & 75 \\\\ 95 & 106 & 117 \\end{pmatrix}\n\\end{equation}\\]\n\nNow that you have practised the algorithm for matrix multiplication we can formalize the definition and then turn the algorithm into a Python function.\n\n\n\nDefinition C.3 (Matrix Multiplication) If \\(A\\) and \\(B\\) are matrices with \\(A \\in \\mathbb{R}^{n \\times p}\\) and \\(B \\in \\mathbb{R}^{p \\times m}\\) then the product \\(AB\\) is defined as \\[\\begin{equation}\n\\left( AB \\right)_{ij} = \\sum_{k=1}^p A_{ik} B_{kj}.\n\\end{equation}\\]\nA moment‚Äôs reflection reveals that each entry in the matrix product is actually a dot product, \\[\\begin{equation}\n\\left( \\text{Entry in row $i$ column $j$ of $AB$} \\right) = \\left( \\text{Row $i$ of matrix $A$} \\right) \\cdot \\left( \\text{Column $j$ of matrix $B$} \\right).\n\\end{equation}\\]\n\n\n\nExercise C.9 The definition of matrix multiplication above contains the cryptic phrase a moment‚Äôs reflection reveals that each entry in the matrix product is actually a dot product. Let us go back to the matrices \\(A\\) and \\(B\\) defined in Exercise¬†C.8 above and re-evaluate the matrix multiplication algorithm to make sure that you see each entry as the end result of a dot product.\nWe want to find the product of matrices \\(A\\) and \\(B\\) using dot products. \\[\\begin{equation}\nA = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{pmatrix} \\qquad B = \\begin{pmatrix} 7 & 8 & 9 \\\\ 10 & 11 & 12 \\end{pmatrix}\n\\end{equation}\\]\n\nWhy will the product \\(AB\\) clear be a \\(3 \\times 3\\) matrix?\nWhen we do matrix multiplication we take the product of a row from the first matrix times a column from the second matrix ‚Ä¶ at least that‚Äôs how many people think of it when they perform the operation by hand.\n\nThe rows of \\(A\\) can be written as the vectors \\[\\begin{equation}\n\\boldsymbol{a}_0 = \\begin{pmatrix} 1 & 2 \\end{pmatrix}\n\\end{equation}\\]\n\n\\[\\begin{equation}\n\\boldsymbol{a}_1 = \\begin{pmatrix} \\underline{\\hspace{0.5in}} & \\underline{\\hspace{0.5in}} \\end{pmatrix}\n\\end{equation}\\]\n\\[\\begin{equation}\n\\boldsymbol{a}_2 = \\begin{pmatrix} \\underline{\\hspace{0.5in}} & \\underline{\\hspace{0.5in}} \\end{pmatrix}\n\\end{equation}\\]\n\nThe columns of \\(B\\) can be written as the vectors \\[\\begin{equation}\n\\boldsymbol{b}_0 = \\begin{pmatrix} 7 \\\\ 10 \\end{pmatrix}\n\\end{equation}\\]\n\n\\[\\begin{equation}\n\\boldsymbol{b}_1 = \\begin{pmatrix} \\underline{\\hspace{0.5in}} \\\\ \\underline{\\hspace{0.5in}} \\end{pmatrix}\n\\end{equation}\\]\n\\[\\begin{equation}\n\\boldsymbol{b}_2 = \\begin{pmatrix} \\underline{\\hspace{0.5in}} \\\\ \\underline{\\hspace{0.5in}} \\end{pmatrix}\n\\end{equation}\\]\nNow let us write each entry in the product \\(AB\\) as a dot product. \\[\\begin{equation}\nAB = \\begin{pmatrix} \\boldsymbol{a}_0 \\cdot \\boldsymbol{b}_0 & \\underline{\\hspace{0.25in}} \\cdot \\underline{\\hspace{0.25in}} & \\underline{\\hspace{0.25in}} \\cdot \\underline{\\hspace{0.25in}} \\\\ \\underline{\\hspace{0.25in}} \\cdot \\underline{\\hspace{0.25in}} & \\underline{\\hspace{0.25in}} \\cdot \\underline{\\hspace{0.25in}} & \\underline{\\hspace{0.25in}} \\cdot \\underline{\\hspace{0.25in}} \\\\ \\underline{\\hspace{0.25in}} \\cdot \\underline{\\hspace{0.25in}} & \\underline{\\hspace{0.25in}} \\cdot \\underline{\\hspace{0.25in}} & \\underline{\\hspace{0.25in}} \\cdot \\underline{\\hspace{0.25in}} \\end{pmatrix}\n\\end{equation}\\]\nVerify that you get \\[\\begin{equation}\nAB = \\begin{pmatrix} 27 & 30 & 33 \\\\ 61 & 68 & 75 \\\\ 95 & 106 & 117 \\end{pmatrix}\n\\end{equation}\\] when you perform all of the dot products from part (3).\n\n\n\n\nExercise C.10 The observation that matrix multiplication is just a bunch of dot products is what makes the code for doing matrix multiplication very fast and very streamlined. We want to write a Python function that accepts two numpy matrices and returns the product of the two matrices. Inside the code we will leverage the np.dot() command to do the appropriate dot products.\nPartial code is given below. Fill in all of the details and give ample comments showing what each line does.\nimport numpy as np\ndef myMatrixMult(A,B):\n    # Get the shapes of the matrices A and B.\n    # Then write an if statement that catches size mismatches \n    # in the matrices.  Next build a zeros matrix that is the \n    # correct size for the product of A and B. \n    AB = ??? \n    # AB is a zeros matix that will be filled with the values \n    # from the product\n    # \n    # Next we do a double for-loop that loops through all of \n    # the indices of the product\n    for i in range(n): # loop over the rows of AB\n        for j in range(m): # loop over the columns of AB\n            # use the np.dot() command to take the dot product\n            AB[i,j] = ???\n    return AB\nUse the following test code to determine if you actually get the correct matrix product out of your code.\n``` python         \nA = np.array([[1,2],[3,4],[5,6]])\nB = np.array([[7,8,9],[10,11,12]])\nAB = myMatrixMult(A,B)\nprint(AB)\n\n\n\nExercise C.11 Try your myMatrixMult() function on several other matrix multiplication problems.\n\n\n\nExercise C.12 Build in an error catch so that your myMatrixMult() function catches when the input matrices do not have compatible sizes for multiplication. Write your code so that it returns an appropriate error message in this special case.\n\n\nNow that you have been through the exercise of building a matrix multiplication function we will admit that using it inside larger coding problems would be a bit cumbersome (and perhaps annoying). It would be nice to just type @ and have Python just know that you mean to do matrix multiplication. This is where numpy arrays come in quite handy.\n\n\nExercise C.13 (Matrix Multiplication with Python) Python will handle matrix multiplication easily so long as the matrices are defined as numpy arrays. For example, with the matrices \\(A\\) and \\(B\\) from above if you can just type A @ B (or np.dot(A, B)) in Python and you will get the correct result. Pretty nice!! Let us take another moment to notice, though, that regular Python lists do not behave in the same way. Can you guess what happens if you run the following Python code? (Note: * does strictly element-wise multiplication for numpy arrays, which we will see in a moment).\nA = [[1,2],[3,4],[5,6]] # a Python list of lists\nB = [[7,8,9],[10,11,12]] # a Python list of lists\nA @ B\n\n\n\nExample C.6 (Element-by-Element Multiplication) Sometimes it is convenient to do naive multiplication of matrices when you code. That is, if you have two matrices that are the same size, ‚Äúnaive multiplication‚Äù would just line up the matrices on top of each other and multiply the corresponding entries.2 In Python you can do this with the * operator (or np.multiply()). The code below demonstrates this tool with the matrices \\[\\begin{equation}\nA = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{pmatrix} \\quad \\text{and} \\quad B = \\begin{pmatrix} 7 & 8 \\\\ 9 & 10 \\\\ 11 & 12 \\end{pmatrix}.\n\\end{equation}\\]\n(Note that the product \\(AB\\) does not make sense under the mathematical definition of matrix multiplication, but it does make sense in terms of element-by-element (‚Äúnaive‚Äù) multiplication.)\nimport numpy as np\nA = np.array([[1,2],[3,4],[5,6]])\nB = np.array([[7,8],[9,10],[11,12]])\nprint(A * B)\n\n\nThe key takeaways for doing matrix multiplication in Python are as follows:\n\nIf you are doing linear algebra in Python then you should define vectors and matrices with np.array().\nIf your matrices are defined with np.array() then @ (or np.dot()) does regular matrix multiplication and * does element-by-element multiplication.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>¬† <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "nmLinAlg.html#the-lu-factorization",
    "href": "nmLinAlg.html#the-lu-factorization",
    "title": "Appendix C ‚Äî Linear Algebra",
    "section": "C.5 The LU Factorization",
    "text": "C.5 The LU Factorization\nOne of the many classic problems of linear algebra is to solve the linear system \\(A \\boldsymbol{x} = \\boldsymbol{b}\\) where \\(A\\) is a matrix of coefficients and \\(\\boldsymbol{b}\\) is a vector of right-hand sides. You likely recall your go-to technique for solving systems was row reduction (or Gaussian Elimination). Furthermore, you likely rarely actually did row reduction by hand, and instead you relied on a computer to do most of the computations for you. Just what was the computer doing, exactly? Do you think that it was actually following the same algorithm that you did by hand?\n\nC.5.1 A Recap of Row Reduction\nLet us blow the dust off your row reduction skills before we look at something better.\n\n\nExercise C.14 Solve the following system of equations by hand. \\[\\begin{equation}\n\\begin{array}{rl} x_0 + 2x_1 + 3x_2 &= 1 \\\\ 4x_0 + 5x_1 + 6x_2 &= 0 \\\\ 7x_0 + 8x_1 &= 2 \\end{array}\n\\end{equation}\\] Note that the system of equations can also be written in the matrix form \\[\\begin{equation}\n\\begin{pmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 0 \\end{pmatrix} \\begin{pmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 2\\end{pmatrix}\n\\end{equation}\\]\nIf you need a nudge to get started then jump ahead to the next problem.\n\n\n\nExercise C.15 We want to solve the system of equations \\[\\begin{equation}\n\\begin{pmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 0 \\end{pmatrix} \\begin{pmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 2\\end{pmatrix}\n\\end{equation}\\]\nRow Reduction Process:\nNote: Throughout this discussion we use Python-type indexing so the rows and columns are enumerated starting at 0. That is to say, we will talk about row 0, row 1, and row 2 of a matrix instead of rows 1, 2, and 3.\n\nAugment the coefficient matrix and the vector on the right-hand side to get \\[\\begin{equation}\n\\left( \\begin{array}{ccc|c} 1 & 2 & 3 & 1 \\\\ 4 & 5 & 6 & 0 \\\\ 7 & 8 & 0 & 2 \\end{array} \\right)\n\\end{equation}\\]\nThe goal of row reduction is to perform elementary row operations until our augmented matrix gets to (or at least gets as close as possible to) \\[\\begin{equation}\n\\left( \\begin{array}{ccc|c} 1 & 0 & 0 & \\star \\\\ 0 & 1 & 0 & \\star \\\\ 0 & 0 & 1 & \\star \\end{array} \\right)\n\\end{equation}\\] The allowed elementary row operations are:\n\nWe are allowed to scale any row.\nWe can add two rows.\nWe can interchange two rows.\n\nWe are going to start with column 0. We already have the ‚Äú\\(1\\)‚Äù in the top left corner so we can use it to eliminate all of the other values in the first column of the matrix.\n\nFor example, if we multiply the \\(0^{th}\\) row by \\(-4\\) and add it to the first row we get \\[\\begin{equation}\n\\left( \\begin{array}{ccc|c} 1 & 2 & 3 & 1 \\\\ 0 & -3 & -6 & -4 \\\\ 7 & 8 & 0 & 2 \\end{array} \\right).\n\\end{equation}\\]\nMultiply row 0 by a scalar and add it to row 2. Your end result should be \\[\\begin{equation}\n\\left( \\begin{array}{ccc|c} 1 & 2 & 3 & 1 \\\\ 0 & -3 & -6 & -4 \\\\ 0 & -6 & -21 & -5 \\end{array} \\right).\n\\end{equation}\\] What did you multiply by? Why?\n\nNow we should deal with column 1.\n\nWe want to get a 1 in row 1 column 1. We can do this by scaling row 1. What did you scale by? Why? Your end result should be \\[\\begin{equation}\n\\left( \\begin{array}{ccc|c} 1 & 2 & 3 & 1 \\\\ 0 & 1 & 2 & \\frac{4}{3} \\\\ 0 & -6 & -21 & -5 \\end{array} \\right).\n\\end{equation}\\]\nNow scale row 1 by something and add it to row 0 so that the entry in row 0 column 1 becomes a 0.\nNext scale row 1 by something and add it to row 2 so that the entry in row 2 column 1 becomes a 0.\nAt this point you should have the augmented system \\[\\begin{equation}\n\\left( \\begin{array}{ccc|c} 1 & 0 & -1 & -\\frac{5}{3} \\\\ 0 & 1 & 2 & \\frac{4}{3} \\\\ 0 & 0 & -9 & 3 \\end{array} \\right).\n\\end{equation}\\]\n\nFinally we need to work with column 2.\n\nMake the value in row 2 column 2 a 1 by scaling row 2. What did you scale by? Why?\nScale row 2 by something and add it to row 1 so that the entry in row 1 column 2 becomes a 0. What did you scale by? Why?\nScale row 2 by something and add it to row 0 so that the entry in row 0 column 2 becomes a 0. What did you scale by? Why?\nBy the time you have made it this far you should have the system \\[\\begin{equation}\n\\left( \\begin{array}{ccc|c} 1 & 0 & 0 & -2 \\\\ 0 & 1 & 0 & 2 \\\\ 0 & 0 & 1 & -\\frac{1}{3} \\end{array} \\right)\n\\end{equation}\\] and you should be able to read off the solution to the system.\n\nYou should verify your answer in two different ways:\n\nIf you substitute your values into the original system then all of the equal signs should be true. Verify this.\nIf you substitute your values into the matrix equation and perform the matrix-vector multiplication on the left-hand side of the equation you should get the right-hand side of the equation. Verify this.\n\n\n\n\n\nExercise C.16 Summarize the process for doing Gaussian Elimination to solve a square system of linear equations.\n\n\n\n\nC.5.2 The LU Decomposition\nYou may have used the rref() command either on a calculator in other software to perform row reduction in the past. You will be surprised to learn that there is no rref() command in Python‚Äôs numpy library! That‚Äôs because there are far more efficient and stable ways to solve a linear system on a computer. There is an rref command in Python‚Äôs sympy (symbolic Python) library, but given that it works with symbolic algebra it is quite slow.\nIn solving systems of equations we are interested in equations of the form \\(A \\boldsymbol{x} = \\boldsymbol{b}\\). Notice that the \\(\\boldsymbol{b}\\) vector is just along for the ride, so to speak, in the row reduction process since none of the values in \\(\\boldsymbol{b}\\) actually cause you to make different decisions in the row reduction algorithm. Hence, we only really need to focus on the matrix \\(A\\). Furthermore, let us change our awfully restrictive view of always seeking a matrix of the form \\[\\begin{equation}\n\\left( \\begin{array}{cccc|c} 1 & 0 & \\cdots & 0 & \\star \\\\ 0 & 1 & \\cdots & 0 & \\star \\\\ \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ 0 & 0 & \\cdots & 1 & \\star \\end{array} \\right)\n\\end{equation}\\] and instead say:\n\nWhat if we just row reduce until the system is simple enough to solve by hand?\n\nThat‚Äôs what the next several exercises are going to lead you to. Our goal here is to develop an algorithm that is fast to implement on a computer and simultaneously performs the same basic operations as row reduction for solving systems of linear equations.\n\n\nExercise C.17 Let \\(A\\) be defined as \\[\\begin{equation}\nA = \\begin{pmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 0 \\end{pmatrix}.\n\\end{equation}\\]\n\nThe first step in row reducing \\(A\\) would be to multiply row 0 by \\(-4\\) and add it to row 1. Do this operation by hand so that you know what the result is supposed to be. Check out the following amazing observation. Define the matrix \\(L_1\\) as follows: \\[\\begin{equation}\nL_1 = \\begin{pmatrix} 1 & 0 & 0 \\\\ -4 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}.\n\\end{equation}\\] Now multiply \\(L_1\\) and \\(A\\). \\[\\begin{equation}\nL_1 A = \\begin{pmatrix} \\underline{\\hspace{0.25in}} & \\underline{\\hspace{0.25in}} & \\underline{\\hspace{0.25in}} \\\\ \\underline{\\hspace{0.25in}} & \\underline{\\hspace{0.25in}} & \\underline{\\hspace{0.25in}} \\\\ \\underline{\\hspace{0.25in}} & \\underline{\\hspace{0.25in}} & \\underline{\\hspace{0.25in}} \\end{pmatrix}\n\\end{equation}\\] What just happened?!\nLet us do it again. The next step in the row reduction of your result from part (1) would be to multiply row 0 by \\(-7\\) and add to row 2. Again, do this by hand so you know what the result should be. Then define the matrix \\(L_2\\) as \\[\\begin{equation}\nL_2 = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ -7 & 0 & 1 \\end{pmatrix}\n\\end{equation}\\] and find the product \\(L_2 \\left( L_1 A \\right)\\). \\[\\begin{equation}\nL_2 \\left( L_1 A \\right) = \\begin{pmatrix} \\underline{\\hspace{0.25in}} & \\underline{\\hspace{0.25in}} & \\underline{\\hspace{0.25in}} \\\\ \\underline{\\hspace{0.25in}} & \\underline{\\hspace{0.25in}} & \\underline{\\hspace{0.25in}} \\\\ \\underline{\\hspace{0.25in}} & \\underline{\\hspace{0.25in}} & \\underline{\\hspace{0.25in}} \\end{pmatrix}\n\\end{equation}\\] Pure insanity!!\nNow let us say that you want to make the entry in row 2 column 1 into a 0 by scaling row 1 by something and then adding to row 2. Determine what the scalar would be and then determine which matrix, call it \\(L_3\\), would do the trick so that \\(L_3 (L_2 L_1 A)\\) would be the next row reduced step. \\[\\begin{equation}\nL_3 = \\begin{pmatrix} 1 & \\underline{\\hspace{0.25in}} & \\underline{\\hspace{0.25in}} \\\\ \\underline{\\hspace{0.25in}} & 1 & \\underline{\\hspace{0.25in}} \\\\ \\underline{\\hspace{0.25in}} & \\underline{\\hspace{0.25in}} & 1 \\end{pmatrix}\n\\end{equation}\\]\n\n\\[\\begin{equation}\nL_3 \\left( L_2 L_1 A \\right) = \\begin{pmatrix} \\underline{\\hspace{0.25in}} & \\underline{\\hspace{0.25in}} & \\underline{\\hspace{0.25in}} \\\\ \\underline{\\hspace{0.25in}} & \\underline{\\hspace{0.25in}} & \\underline{\\hspace{0.25in}} \\\\ \\underline{\\hspace{0.25in}} & \\underline{\\hspace{0.25in}} & \\underline{\\hspace{0.25in}} \\end{pmatrix}\n\\end{equation}\\]\n\n\n\nExercise C.18 Apply the same idea from the previous problem to do the first three steps of row reduction to the matrix \\[\\begin{equation}\nA = \\begin{pmatrix} 2 & 6 & 9 \\\\ -6 & 8 & 1 \\\\ 2 & 2 & 10 \\end{pmatrix}\n\\end{equation}\\]\n\n\n\nExercise C.19 Now let us make a few observations about the two previous problems.\n\nWhat will multiplying \\(A\\) by a matrix of the form \\[\\begin{equation}\n\\begin{pmatrix} 1 & 0 & 0 \\\\ c & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n\\end{equation}\\] do?\nWhat will multiplying \\(A\\) by a matrix of the form \\[\\begin{equation}\n\\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ c & 0 & 1 \\end{pmatrix}\n\\end{equation}\\] do?\nWhat will multiplying \\(A\\) by a matrix of the form \\[\\begin{equation}\n\\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & c & 1 \\end{pmatrix}\n\\end{equation}\\] do?\nMore generally: If you wanted to multiply row \\(j\\) of an \\(n\\times n\\) matrix by \\(c\\) and add it to row \\(k\\), that is the same as multiplying by what matrix?\n\n\n\n\nExercise C.20 After doing all of the matrix products, \\(L_3 L_2 L_1 A\\), the resulting matrix will have zeros in the entire lower triangle. That is, all of the non-zero entries of the resulting matrix will be on the main diagonal or above. We call this matrix \\(U\\), for upper-triangular. Hence, we have formed a matrix \\[\\begin{equation}\nL_3 L_2 L_1 A = U\n\\end{equation}\\] and if we want to solve for \\(A\\) we would get \\[\\begin{equation}\nA = (\\underline{\\hspace{0.5in}})^{-1} (\\underline{\\hspace{0.5in}})^{-1} (\\underline{\\hspace{0.5in}})^{-1} U\n\\end{equation}\\] (Take care that everything is in the right order in your answer.)\n\n\n\nExercise C.21 It would be nice, now, if the inverses of the \\(L\\) matrices were easy to find. Use np.linalg.inv() to directly compute the inverse of \\(L_1\\), \\(L_2\\), and \\(L_3\\) for each of the example matrices. Then complete the statement: If \\(L_k\\) is an identity matrix with some non-zero \\(c\\) in row \\(i\\) and column \\(j\\) then \\(L_k^{-1}\\) is what matrix?\n\n\n\nExercise C.22 We started this discussion with \\(A\\) as \\[\\begin{equation}\nA = \\begin{pmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 0 \\end{pmatrix}\n\\end{equation}\\] and we defined \\[\\begin{equation}\nL_1 = \\begin{pmatrix} 1 & 0 & 0 \\\\ -4 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}, \\quad L_2 = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ -7 & 0 & 1 \\end{pmatrix}, \\quad \\text{and} \\quad L_3 = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & -2 & 1 \\end{pmatrix}.\n\\end{equation}\\] Based on your answer to the previous exercises we know that \\[\\begin{equation}\nA = L_1^{-1} L_2^{-1} L_3^{-1} U.\n\\end{equation}\\] Explicitly write down the matrices \\(L_1^{-1}\\), \\(L_2^{-1}\\), and \\(L_3^{-1}\\).\nNow explicitly find the product \\(L_1^{-1} L_2^{-1} L_3^{-1}\\) and call this product \\(L\\). Verify that \\(L\\) itself is also a lower-triangular matrix with ones on the main diagonal. Moreover, take note of exactly the form of the matrix. The answer should be super surprising to you!!\n\n\nThroughout all of the preceding exercises, our final result is that we have factored the matrix \\(A\\) into the product of a lower-triangular matrix and an upper-triangular matrix. Stop and think about that for a minute ‚Ä¶ we just factored a matrix!\nLet us return now to our discussion of solving the system of equations \\(A \\boldsymbol{x} = \\boldsymbol{b}\\). If \\(A\\) can be factored into \\(A = LU\\) then the system of equations can be rewritten as \\(LU \\boldsymbol{x} = \\boldsymbol{b}\\). As we will see in the next subsection, solving systems of equations with triangular matrices is super fast and relatively simple! Hence, we have partially achieved our modified goal of reducing the row reduction into some simpler case.3\nIt remains to implement the \\(LU\\) decomposition (also called the \\(LU\\) factorization) in Python.\n\n\nExample C.7 (The LU Factorization) The following Python function takes a square matrix \\(A\\) and outputs the matrices \\(L\\) and \\(U\\) such that \\(A = LU\\). The entire code is given to you. It will be up to you in the next exercise to pick apart every step of the function.\ndef myLU(A):\n    n = A.shape[0] # get the dimension of the matrix A\n    L = np.eye(n) # Build the identity part of L\n    U = np.copy(A) # start the U matrix as a copy of A\n    for j in range(0,n-1):\n        for i in range(j+1,n):\n            mult = U[i,j] / U[j,j]\n            U[i, :] = U[i, :] - mult * U[j,:]\n            L[i,j] = mult\n    return L,U\n\n\n\nExercise C.23 Go to Example¬†C.7 and go through every iteration of every loop by hand starting with the matrix \\[\\begin{equation}\nA = \\begin{pmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 0 \\end{pmatrix}.\n\\end{equation}\\] Give details of what happens at every step of the algorithm. I will get you started.\n\nn=3, L starts as an identity matrix of the correct size, and U starts as a copy of A.\nStart the outer loop: j=0: (j is the counter for the column)\n\nStart the inner loop: i=1: (i is the counter for the row)\n\nmult = A[1,0] / A[0,0] so mult\\(=4/1\\).\nA[1, 1:3] = A[1, 1:3] - 4 * A[0,1:3]. Translated, this states that columns 1 and 2 of matrix \\(A\\) took their original value minus 4 times the corresponding values in row 0.\nU[1, 1:3] = A[1, 1:3]. Now we replace the locations in \\(U\\) with the updated information from our first step of row reduction.\nL[1,0]=4. We now fill the \\(L\\) matrix with the proper value.\nU[1,0]=0. Finally, we zero out the lower triangle piece of the \\(U\\) matrix which we have now taken care of.\n\ni=2:\n\n‚Ä¶ keep going from here ‚Ä¶\n\n\n\n\n\n\nExercise C.24 Apply your new myLU code to other square matrices and verify that indeed \\(A\\) is the product of the resulting \\(L\\) and \\(U\\) matrices. You can produce a random matrix with np.random.randn(n,n) where n is the number of rows and columns of the matrix. For example, np.random.randn(10,10) will produce a random \\(10 \\times 10\\) matrix with entries chosen from the normal distribution with centre 0 and standard deviation 1. Random matrices are just as good as any other when testing your algorithm.\n\n\n\n\nC.5.3 Solving Triangular Systems\nWe now know that row reduction is just a collection of sneaky matrix multiplications. In the previous exercises we saw that we can often turn our system of equations \\(A \\boldsymbol{x} = \\boldsymbol{b}\\) into the system \\(LU \\boldsymbol{x} = \\boldsymbol{b}\\) where \\(L\\) is lower-triangular (with ones on the main diagonal) and \\(U\\) is upper-triangular. But why was this important?\nWell, if \\(LU \\boldsymbol{x} = \\boldsymbol{b}\\) then we can rewrite our system of equations as two systems: \\[\\begin{equation}\n\\text{An upper-triangular system: } U \\boldsymbol{x} = \\boldsymbol{y}\n\\end{equation}\\] and \\[\\begin{equation}\n\\text{A lower-triangular system: } L \\boldsymbol{y} = \\boldsymbol{b}.\n\\end{equation}\\]\nIn the following exercises we will devise algorithms for solving triangular systems. After we know how to work with triangular systems we will put all of the pieces together and show how to leverage the \\(LU\\) decomposition and the solution techniques for triangular systems to quickly and efficiently solve linear systems.\n\n\nExercise C.25 Outline a fast algorithm (without formal row reduction) for solving the lower-triangular system \\[\\begin{equation}\n\\begin{pmatrix} 1 & 0 & 0 \\\\ 4 & 1 & 0 \\\\ 7 & 2 & 1 \\end{pmatrix} \\begin{pmatrix} y_0 \\\\ y_1 \\\\ y_2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 2\\end{pmatrix}.\n\\end{equation}\\]\n\n\n\nExercise C.26 As a convention we will always write our lower-triangular matrices with ones on the main diagonal. Generalize your steps from the previous exercise so that you have an algorithm for solving any lower-triangular system. The most natural algorithm that most people devise here is called forward substitution.\n\n\n\nDefinition C.4 (Forward Substutition Algorithm (lsolve)) The general statement of the Forward Substitution Algorithm is:\nSolve \\(L \\boldsymbol{y} = \\boldsymbol{b}\\) for \\(\\boldsymbol{y}\\), where the matrix \\(L\\) is assumed to be lower-triangular with ones on the main diagonal.\nThe code below gives a full implementation of the Forward Substitution algorithm (also called the lsolve algorithm).\ndef lsolve(L, b):\n    # L is assumed to be a lower-triangular np.array\n    n = b.size # what does this do?\n    y = np.zeros(n) # what does this do?\n    for i in range(n):\n        # start the loop by assigning y to the value on the right\n        y[i] = b[i] \n        for j in range(i): # now adjust y \n            y[i] = y[i] - L[i,j] * y[j]\n    return(y)\n\n\n\nExercise C.27 Work with your partner(s) to apply the lsolve() code to the lower-triangular system\n\n\\[\\begin{equation}\n\\begin{pmatrix} 1 & 0 & 0 \\\\ 4 & 1 & 0 \\\\ 7 & 2 & 1 \\end{pmatrix} \\begin{pmatrix} y_0 \\\\ y_1 \\\\ y_2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 2\\end{pmatrix}\n\\end{equation}\\] by hand. It is incredibly important to implement numerical linear algebra routines by hand a few times so that you truly understand how everything is being tracked and calculated.\nI will get you started.\n\nStart: i=0:\n\ny[0]=1 since b[0]=1.\nThe next for loop does not start since range(0) has no elements (stop and think about why this is).\n\nNext step in the loop: i=1:\n\ny[1] is initialized as 0 since b[1]=0.\nNow we enter the inner loop at j=0:\n\nWhat does y[1] become when j=0?\n\nDoes j increment to anything larger?\n\nFinally we increment i to i=2:\n\nWhat does y[2] get initialized to?\nEnter the inner loop at j=0:\n\nWhat does y[2] become when j=0?\n\nIncrement the inner loop to j=1:\n\nWhat does y[2] become when j=1?\n\n\nStop\n\n\n\n\nExercise C.28 Copy the code from Definition¬†C.4 into a Python function but in your code write a comment on every line stating what it is doing. Write a test script that creates a lower-triangular matrix of the correct form and a right-hand side \\(\\boldsymbol{b}\\) and solve for \\(\\boldsymbol{y}\\). Test your code by giving it a large lower-triangular system.\n\n\nNow that we have a method for solving lower-triangular systems, let us build a similar method for solving upper-triangular systems. The merging of lower and upper-triangular systems will play an important role in solving systems of equations.\n\n\nExercise C.29 Outline a fast algorithm (without formal row reduction) for solving the upper-triangular system \\[\\begin{equation}\n\\begin{pmatrix} 1 & 2 & 3 \\\\ 0 & -3 & -6 \\\\ 0 & 0 & -9 \\end{pmatrix} \\begin{pmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -4 \\\\ 3\\end{pmatrix}\n\\end{equation}\\] The most natural algorithm that most people devise here is called backward substitution. Notice that in our upper-triangular matrix we do not have a diagonal containing all ones.\n\n\n\nExercise C.30 Generalize your backward substitution algorithm from the previous problem so that it could be applied to any upper-triangular system.\n\n\n\nDefinition C.5 (Backward Substitution Algorithm) The following code solves the problem \\(U \\boldsymbol{x} = \\boldsymbol{y}\\) using backward substitution. The matrix \\(U\\) is assumed to be upper-triangular. You will notice that most of this code is incomplete. It is your job to complete this code, and the next exercise should help.\ndef usolve(U, y):\n    # U is assumed to be an upper-triangular np.array\n    n = y.size\n    x = np.zeros(n)\n    for i in range( ??? ):     # what should we be looping over?\n        x[i] = y[i] / ???      # what should we be dividing by?\n        for j in range( ??? ): # what should we be looping over:\n            x[i] = x[i] - U[i,j] * x[j] / ??? # complete this line \n            # ... what does the previous line do?\n    return(x)\n\n\n\nExercise C.31 Now we will work through the backward substitution algorithm to help fill in the blanks in the code. Consider the upper-triangular system \\[\\begin{equation}\n\\begin{pmatrix} 1 & 2 & 3 \\\\ 0 & -3 & -6 \\\\ 0 & 0 & -9 \\end{pmatrix} \\begin{pmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -4 \\\\ 3\\end{pmatrix}\n\\end{equation}\\]\nWork the code from Definition¬†C.5 to solve the system. Keep track of all of the indices as you work through the code. You may want to work this problem in conjunction with the previous two problems to unpack all of the parts of the backward substitution algorithm.\nI will get you started.\n\nIn your backward substitution algorithm you should have started with the last row, therefore the outer loop starts at n-1 and reads backward to 0. (Why are we starting at n-1 and not n?)\nOuter loop: i=2:\n\nWe want to solve the equation \\(-9x_2 = 3\\) so the clear solution is to divide by \\(-9\\). In code this means that x[2]=y[2]/U[2,2].\nThere is nothing else to do for row 3 of the matrix, so we should not enter the inner loop. How can we keep from entering the inner loop?\n\nOuter loop: i=1:\n\nNow we are solving the algebraic equation \\(-3x_1 - 6x_2 = -4\\). If we follow the high school algebra we see that \\(x_1 = \\frac{-4 - (-6)x_2}{-3}\\) but this can be rearranged to \\[\\begin{equation}\nx_1 = \\frac{-4}{-3} - \\frac{-6x_2}{-3}.\n\\end{equation}\\] So we can initialize \\(x_1\\) with \\(x_1 = \\frac{-4}{-3}\\). In code, this means that we initialize with x[1] = y[1] / U[1,1].\nNow we need to enter the inner loop at j=2: (why are we entering the loop at j=2?)\n\nTo complete the algebra we need to take our initialized value of x[1] and subtract off \\(\\frac{-6x_2}{-3}\\). In code this is x[1] = x[1] - U[1,2] * x[2] / U[1,1]\n\nThere is nothing else to do so the inner loop should end.\n\nOuter loop: i=0:\n\nFinally, we are solving the algebraic equation \\(x_0 + 2x_1 + 3 x_2 = 1\\) for \\(x_0\\). The clear and obvious solution is \\(x_0 = \\frac{1 - 2x_1 - 3x_2}{1}\\) (why am I explicitly showing the division by \\(1\\) here?).\nInitialize \\(x_0\\) at x[0] = ???\nEnter the inner loop at j=2:\n\nAdjust the value of x[0] by subtracting off \\(\\frac{3x_2}{1}\\). In code we have x[0] = x[0] - ??? * ??? / ???\n\nIncrement j to j=1:\n\nAdjust the value of x[0] by subtracting off \\(\\frac{2x_1}{1}\\). In code we have x[0] = x[0] - ??? * ??? / ???\n\n\nStop.\nYou should now have a solution to the equation \\(U \\boldsymbol{x} = \\boldsymbol{y}\\). Substitute your solution in and verify that your solution is correct.\n\n\n\n\nExercise C.32 Copy the code from Definition¬†C.5 into a Python function but in your code write a comment on every line stating what it is doing. Write a test script that creates an upper-triangular matrix of the correct form and a right-hand side \\(\\boldsymbol{y}\\) and solve for \\(\\boldsymbol{x}\\). Your code needs to work on systems of arbitrarily large size.\n\n\n\n\nC.5.4 Solving Systems with LU Decomposition\nWe are finally ready for the punch line of this whole \\(LU\\) and triangular systems business!\n\n\nExercise C.33 If we want to solve \\(A \\boldsymbol{x} = \\boldsymbol{b}\\) then\n\nIf we can, write the system of equations as \\(LU \\boldsymbol{x} = \\boldsymbol{b}\\).\nSolve \\(L \\boldsymbol{y} = \\boldsymbol{b}\\) for \\(\\boldsymbol{y}\\) using forward substitution.\nSolve \\(U \\boldsymbol{x} = \\boldsymbol{y}\\) for \\(\\boldsymbol{x}\\) using backward substitution.\n\nPick a matrix \\(A\\) and a right-hand side \\(\\boldsymbol{b}\\) and solve the system using this process.\n\n\n\nExercise C.34 Try the process again on the \\(3\\times 3\\) system of equations \\[\\begin{equation}\n\\begin{pmatrix} 3 & 6 & 8\\\\ 2 & 7 & -1 \\\\ 5 & 2 & 2 \\end{pmatrix} \\begin{pmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} -13 \\\\ 4 \\\\ 1 \\end{pmatrix}\n\\end{equation}\\] That is: Find matrices \\(L\\) and \\(U\\) such that \\(A \\boldsymbol{x} = \\boldsymbol{b}\\) can be written as \\(LU\\boldsymbol{x} = \\boldsymbol{b}\\). Then do two triangular solves to determine \\(\\boldsymbol{x}\\).\n\n\nLet us take stock of what we have done so far.\n\nSolving lower-triangular systems is super fast and easy!\nSolving upper-triangular systems is super fast and easy (so long as we never divide by zero).\nIt is often possible to rewrite the matrix \\(A\\) as the product of a lower-triangular matrix \\(L\\) and an upper-triangular matrix \\(U\\) so \\(A=LU\\).\nNow we can re-frame the equation \\(A \\boldsymbol{x} = \\boldsymbol{b}\\) as \\(LU \\boldsymbol{x} = \\boldsymbol{b}\\).\nSubstitute \\(\\boldsymbol{y} = U \\boldsymbol{x}\\) so the system becomes \\(L \\boldsymbol{y} = \\boldsymbol{b}\\). Solve for \\(\\boldsymbol{y}\\) with forward substitution.\nNow solve \\(U \\boldsymbol{x} = \\boldsymbol{y}\\) using backward substitution.\n\nWe have successfully take row reduction and turned into some fast matrix multiplications and then two very quick triangular solves. Ultimately this will be a faster algorithm for solving a system of linear equations.\n\n\nDefinition C.6 (Solving Linear Systems with the LU Decomposition) Let \\(A\\) be a square matrix in \\(\\mathbb{R}^{n \\times n}\\) and let \\(\\boldsymbol{x}, \\boldsymbol{b} \\in \\mathbb{R}^n\\). To solve the problem \\(A \\boldsymbol{x} =\\boldsymbol{b}\\),\n\nFactor \\(A\\) into lower and upper-triangular matrices \\(A = LU\\).\nL, U = myLU(A)\nThe system can now be written as \\(LU \\boldsymbol{x} = \\boldsymbol{b}\\). Substitute \\(U \\boldsymbol{x} = \\boldsymbol{y}\\) and solve the system \\(L \\boldsymbol{y} = \\boldsymbol{b}\\) with forward substitution.¬† y = lsolve(L,b)\nFinally, solve the system \\(U \\boldsymbol{x} = \\boldsymbol{y}\\) with backward substitution.\nx = usolve(U,y)\n\n\n\n\nExercise C.35 The \\(LU\\) decomposition is not perfect. Discuss where the algorithm will fail.\n\n\n\nExercise C.36 What happens when you try to solve the system of equations \\[\\begin{equation}\n\\begin{pmatrix} 0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 7 \\\\ 9\\\\-3\\end{pmatrix}\n\\end{equation}\\] with the \\(LU\\) decomposition algorithm? Discuss.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>¬† <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "nmLinAlg.html#the-qr-factorization",
    "href": "nmLinAlg.html#the-qr-factorization",
    "title": "Appendix C ‚Äî Linear Algebra",
    "section": "C.6 The QR Factorization",
    "text": "C.6 The QR Factorization\nIn this section we will try to find an improvement on the \\(LU\\) factorization scheme from the previous section. What we will do here is leverage the geometry of the column space of the \\(A\\) matrix instead of leveraging the row reduction process.\n\n\nExercise C.37 We want to solve the system of equations \\[\\begin{equation}\n\\begin{pmatrix} 1/3 & 2/3 & 2/3 \\\\ 2/3 & 1/3 & -2/3 \\\\ -2/3 & 2/3 & -1/3 \\end{pmatrix} \\begin{pmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 12\\\\ -9 \\end{pmatrix}.\n\\end{equation}\\]\n\nWe could do row reduction by hand ‚Ä¶ yuck ‚Ä¶ do not do this.\nWe could apply our new-found skills with the \\(LU\\) decomposition to solve the system, so go ahead and do that with your Python code.\nWhat do you get if you compute the product \\(A^T A\\)?\n\nWhy do you get what you get? In other words, what was special about \\(A\\) that gave such an nice result?\nWhat does this mean about the matrices \\(A\\) and \\(A^T\\)?\n\nNow let us leverage what we found in part (3) to solve the system of equations \\(A \\boldsymbol{x} = \\boldsymbol{b}\\) much faster. Multiply both sides of the matrix equation by \\(A^T\\), and now you should be able to just read off the solution. This seems amazing!!\nWhat was it about this particular problem that made part (4) so elegant and easy?\n\n\n\nThe previous exercise tells us something amazing:\n\nTheorem C.1 (Orthonomal Matrices) If \\(A\\) is an orthonormal matrix where the columns are mutually orthogonal and every column is a unit vector, then \\(A^T = A^{-1}\\) and to solve the system of equation \\(A\\boldsymbol{x} = \\boldsymbol{b}\\) we simply need to multiply both sides of the equation by \\(A^T\\). Hence, the solution to \\(A \\boldsymbol{x} = \\boldsymbol{b}\\) is just \\(\\boldsymbol{x} = A^T \\boldsymbol{b}\\) in this special case.\n\n\nTheorem¬†C.1 begs an obvious question: Is there a way to turn any matrix \\(A\\) into an orthogonal matrix so that we can solve \\(A \\boldsymbol{x} = \\boldsymbol{b}\\) in this same very efficient and fast way?\nThe answer: Yes. Kind of.\nIn essence, if we can factor our coefficient matrix into an orthonormal matrix and some other nicely formatted matrix (like a triangular matrix, perhaps) then the job of solving the linear system of equations comes down to matrix multiplication and a quick triangular solve ‚Äì both of which are extremely fast!\nWhat we will study in this section is a new matrix factorization called the \\(QR\\) factorization whose goal is to convert the matrix \\(A\\) into a product of two matrices, \\(Q\\) and \\(R\\), where \\(Q\\) is orthonormal and \\(R\\) is upper-triangular.\n\n\nExercise C.38 Let us say that we have a matrix \\(A\\) and we know that it can be factored into \\(A = QR\\) where \\(Q\\) is an orthonormal matrix and \\(R\\) is an upper-triangular matrix. How would we then leverage this factorization to solve the system of equation \\(A \\boldsymbol{x} = \\boldsymbol{b}\\) for \\(\\boldsymbol{x}\\)?\n\n\nBefore proceeding to the algorithm for the \\(QR\\) factorization let us pause for a moment and review scalar and vector projections from Linear Algebra. In Figure¬†C.1 we see a graphical depiction of the vector \\(\\boldsymbol{u}\\) projected onto vector \\(\\boldsymbol{v}\\). Notice that the projection is indeed the perpendicular projection as this is what seems natural geometrically.\nThe vector projection of \\(\\boldsymbol{u}\\) onto \\(\\boldsymbol{v}\\) is the vector \\(c \\boldsymbol{v}\\). That is, the vector projection of \\(\\boldsymbol{u}\\) onto \\(\\boldsymbol{v}\\) is a scalar multiple of the vector \\(\\boldsymbol{v}\\). The value of the scalar \\(c\\) is called the scalar projection of \\(\\boldsymbol{u}\\) onto \\(\\boldsymbol{v}\\).\n\n\n\n\n\n\nFigure¬†C.1: Projection of one vector onto another.\n\n\n\nFigure 4.1: Projection of one vector onto another.\nWe can arrive at a formula for the scalar projection rather easily if we consider that the vector \\(\\boldsymbol{w}\\) in Figure¬†C.1 must be perpendicular to \\(c\\boldsymbol{v}\\). Hence \\[\\begin{equation}\n\\boldsymbol{w} \\cdot \\left( c\\boldsymbol{v} \\right) = 0.\n\\end{equation}\\] From vector geometry we also know that \\(\\boldsymbol{w} = \\boldsymbol{u}-c\\boldsymbol{v}\\). Therefore \\[\\begin{equation}\n\\left( \\boldsymbol{u} - c\\boldsymbol{v} \\right) \\cdot \\left( c \\boldsymbol{v} \\right) = 0.\n\\end{equation}\\] If we distribute we can see that \\[\\begin{equation}\nc \\boldsymbol{u} \\cdot \\boldsymbol{v} - c^2 \\boldsymbol{v} \\cdot \\boldsymbol{v} = 0\n\\end{equation}\\] and therefore either \\(c=0\\), which is only true if \\(\\boldsymbol{u} \\perp \\boldsymbol{v}\\), or \\[\\begin{equation}\nc = \\frac{\\boldsymbol{u} \\cdot \\boldsymbol{v}}{\\boldsymbol{v} \\cdot \\boldsymbol{v}} = \\frac{\\boldsymbol{u} \\cdot \\boldsymbol{v}}{\\|\\boldsymbol{v} \\|^2}.\n\\end{equation}\\]\nTherefore,\n\nthe scalar projection of \\(\\boldsymbol{u}\\) onto \\(\\boldsymbol{v}\\) is \\[\\begin{equation}\nc = \\frac{\\boldsymbol{u} \\cdot \\boldsymbol{v}}{\\|\\boldsymbol{v} \\|^2}\n\\end{equation}\\]\nthe vector projection of \\(\\boldsymbol{u}\\) onto \\(\\boldsymbol{v}\\) is \\[\\begin{equation}\nc \\boldsymbol{v} = \\left( \\frac{\\boldsymbol{u} \\cdot \\boldsymbol{v}}{\\|\\boldsymbol{v} \\|^2} \\right) \\boldsymbol{v}\n\\end{equation}\\]\n\nAnother problem related to scalar and vector projections is to take a basis for the column space of a matrix and transform that basis into an orthogonal (or orthonormal) basis. Indeed, in Figure¬†C.1 if we have the matrix \\[\\begin{equation}\nA = \\begin{pmatrix} | & | \\\\ \\boldsymbol{u} & \\boldsymbol{v} \\\\ | & | \\end{pmatrix}\n\\end{equation}\\] it should be clear from the picture that the columns of this matrix are not perpendicular. However, if we take the vector \\(\\boldsymbol{v}\\) and the vector \\(\\boldsymbol{w}\\) we do arrive at two orthogonal vector that form a basis for the same space. Moreover, if we normalize these vectors (by dividing by their respective lengths) then we can easily transform the original basis for the column space of \\(A\\) into an orthonormal basis. This process is called the Gram-Schmidt process, and you have encountered it in your Linear Algebra module.\nNow we return to our goal of finding a way to factor a matrix \\(A\\) into an orthonormal matrix \\(Q\\) and an upper-triangular matrix \\(R\\). The algorithm that we are about to build depends greatly on the ideas of scalar and vector projections.\n\n\nExercise C.39 We want to build a \\(QR\\) factorization of the matrix \\(A\\) in the matrix equation \\(A \\boldsymbol{x} = \\boldsymbol{b}\\) so that we can leverage the fact that solving the equation \\(QR\\boldsymbol{x} = \\boldsymbol{b}\\) is easy. Consider the matrix \\(A\\) defined as \\[\\begin{equation}\nA = \\begin{pmatrix} 3 & 1 \\\\ 4 & 1 \\end{pmatrix}.\n\\end{equation}\\] Notice that the columns of \\(A\\) are NOT orthonormal (they are not unit vectors and they are not perpendicular to each other).\n\nDraw a picture of the two column vectors of \\(A\\) in \\(\\mathbb{R}^2\\). we will use this picture to build geometric intuition for the rest of the \\(QR\\) factorization process.\nDefine \\(\\boldsymbol{a}_0\\) as the first column of \\(A\\) and \\(\\boldsymbol{a}_1\\) as the second column of \\(A\\). That is \\[\\begin{equation}\n\\boldsymbol{a}_0 = \\begin{pmatrix} 3\\\\4\\end{pmatrix} \\quad \\text{and} \\quad \\boldsymbol{a}_1 = \\begin{pmatrix} 1\\\\1\\end{pmatrix}.\n\\end{equation}\\] Turn \\(\\boldsymbol{a}_0\\) into a unit vector and call this unit vector \\(\\boldsymbol{q}_0\\) \\[\\begin{equation}\n\\boldsymbol{q}_0 = \\frac{\\boldsymbol{a}_0}{\\|\\boldsymbol{a}_0\\|} = \\begin{pmatrix} \\underline{\\hspace{0.5in}} \\\\ \\underline{\\hspace{0.5in}} \\end{pmatrix}.\n\\end{equation}\\] This vector \\(\\boldsymbol{q}_0\\) will be the first column of the \\(2 \\times 2\\) matrix \\(Q\\). Why is this a nice place to start building the \\(Q\\) matrix (think about the desired structure of \\(Q\\))?\nIn your picture of \\(\\boldsymbol{a}_0\\) and \\(\\boldsymbol{a}_1\\) mark where \\(\\boldsymbol{q}_0\\) is. Then draw the orthogonal projection from \\(\\boldsymbol{a}_1\\) onto \\(\\boldsymbol{q}_0\\). In your picture you should now see a right triangle with \\(\\boldsymbol{a}_1\\) on the hypotenuse, the projection of \\(\\boldsymbol{a}_1\\) onto \\(\\boldsymbol{q}_0\\) on one leg, and the second leg is the vector difference of the hypotenuse and the first leg. Simplify the projection formula for leg 1 and write the formula for leg 2.\n\n\n\\[\\begin{equation}\n\\text{hypotenuse } = \\boldsymbol{a}_1\n\\end{equation}\\]\n\\[\\begin{equation}\n\\text{leg 1 } = \\left( \\frac{\\boldsymbol{a}_1 \\cdot \\boldsymbol{q}_0}{\\boldsymbol{q}_0 \\cdot \\boldsymbol{q}_0} \\right) \\boldsymbol{q}_0 = \\underline{\\hspace{1in}}\n\\end{equation}\\]\n\\[\\begin{equation}\n\\text{leg 2 } = \\underline{\\hspace{1in}} - \\underline{\\hspace{1in}}.\n\\end{equation}\\]\n\nCompute the vector for leg 2 and then normalize it to turn it into a unit vector. Call this vector \\(\\boldsymbol{q}_1\\) and put it in the second column of \\(Q\\).\nVerify that the columns of \\(Q\\) are now orthogonal and are both unit vectors.\nThe matrix \\(R\\) is supposed to complete the matrix factorization \\(A = QR\\). We have built \\(Q\\) as an orthonormal matrix. How can we use this fact to solve for the matrix \\(R\\)?\nYou should now have an orthonormal matrix \\(Q\\) and an upper-triangular matrix \\(R\\). Verify that \\(A = QR\\).\nAn alternate way to build the \\(R\\) matrix is to observe that \\[\\begin{equation}\nR = \\begin{pmatrix} \\boldsymbol{a}_0 \\cdot \\boldsymbol{q}_0 & \\boldsymbol{a}_1 \\cdot \\boldsymbol{q}_0 \\\\ 0 & \\boldsymbol{a}_1 \\cdot \\boldsymbol{q}_1 \\end{pmatrix}.\n\\end{equation}\\] Show that this is indeed true for the matrix \\(A\\) from this problem.\n\n\n\n\nExercise C.40 Keeping track of all of the arithmetic in the \\(QR\\) factorization process is quite challenging, so let us leverage Python to do some of the work for us. The following block of code walks through the previous exercise without any looping (that way we can see every step transparently). Some of the code is missing so you will need to fill it in.\nimport numpy as np\n# Define the matrix $A$\nA = np.array([[3,1],[4,1]])\nn = A.shape[0]\n# Build the vectors a0 and a1\na0 = A[??? , ???] # ... write code to get column 0 from A\na1 = A[??? , ???] # ... write code to get column 1 from A\n# Set up storage for Q\nQ = np.zeros( (n,n) )\n# build the vector q0 by normalizing a0\nq0 = a0 / np.linalg.norm(a0)\n# Put q0 as the first column of Q\nQ[:,0] = q0\n# Calculate the lengths of the two legs of the triangle\nleg1 = # write code to get the vector for leg 1 of the triangle\nleg2 = # write code to get the vector for leg 2 of the triangle\n# normalize leg2 and call it q1\nq1 = # write code to normalize leg2\nq1 = q1 / np.linalg.norm(q1) # Just to be safe with normalization if not implied\nQ[:,1] = q1 # What does this line do?\nR = # ... build the R matrix out of A and Q\n\nprint(\"The Q matrix is \\n\",Q,\"\\n\")\nprint(\"The R matrix is \\n\",R,\"\\n\")\nprint(\"The A matrix is \\n\",A,\"\\n\")\nprint(\"The product QR is\\n\",Q @ R)\n\n\n\nExercise C.41 You should notice that the code in the previous exercise does not depend on the specific matrix \\(A\\) that we used? Put in a different \\(2 \\times 2\\) matrix and verify that the process still works. That is, verify that \\(Q\\) is orthonormal, \\(R\\) is upper-triangular, and \\(A = QR\\). Be sure, however, that your matrix \\(A\\) is full rank.\n\n\n\nExercise C.42 Draw two generic vectors in \\(\\mathbb{R}^2\\) and demonstrate the process outlined in the previous problem to build the vectors for the \\(Q\\) matrix starting from your generic vectors.\n\n\n\nExercise C.43 Now we will extend the process from the previous exercises to three dimensions. This time we will seek a matrix \\(Q\\) that has three orthonormal vectors starting from the three original columns of a \\(3 \\times 3\\) matrix \\(A\\). Perform each of the following steps by hand on the matrix \\[\\begin{equation}\nA = \\begin{pmatrix} 1 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix}.\n\\end{equation}\\] In the end you should end up with an orthonormal matrix \\(Q\\) and an upper-triangular matrix \\(R\\).\n\nStep 1: Pick column \\(\\boldsymbol{a}_0\\) from the matrix \\(A\\) and normalize it. Call this new vector \\(\\boldsymbol{q}_0\\) and make that the first column of the matrix \\(Q\\).\nStep 2: Project column \\(\\boldsymbol{a}_1\\) of \\(A\\) onto \\(\\boldsymbol{q}_0\\). This forms a right triangle with \\(\\boldsymbol{a}_1\\) as the hypotenuse, the projection of \\(\\boldsymbol{a}_1\\) onto \\(\\boldsymbol{q}_0\\) as one of the legs, and the vector difference between these two as the second leg. Notice that the second leg of the newly formed right triangle is perpendicular to \\(\\boldsymbol{q}_0\\) by design. If we normalize this vector then we have the second column of \\(Q\\), \\(\\boldsymbol{q}_1\\).\nStep 3: Now we need a vector that is perpendicular to both \\(\\boldsymbol{q}_0\\) AND \\(\\boldsymbol{q}_1\\). To achieve this we are going to project column \\(\\boldsymbol{a}_2\\) from \\(A\\) onto the plane formed by \\(\\boldsymbol{q}_0\\) and \\(\\boldsymbol{q}_1\\). we will do this in two steps:\n\nStep 3a: We first project \\(\\boldsymbol{a}_2\\) down onto both \\(\\boldsymbol{q}_0\\) and \\(\\boldsymbol{q}_1\\).\nStep 3b: The vector that is perpendicular to both \\(\\boldsymbol{q}_0\\) and \\(\\boldsymbol{q}_1\\) will be the difference between \\(\\boldsymbol{a}_2\\) the projection of \\(\\boldsymbol{a}_2\\) onto \\(\\boldsymbol{q}_0\\) and the projection of \\(\\boldsymbol{a}_2\\) onto \\(\\boldsymbol{q}_1\\). That is, we form the vector \\(\\boldsymbol{w} = \\boldsymbol{a}_2 - (\\boldsymbol{a}_2 \\cdot \\boldsymbol{q}_0 ) \\boldsymbol{q}_0 - (\\boldsymbol{a}_2 \\cdot \\boldsymbol{q}_1) \\boldsymbol{q}_1.\\) Normalizing this vector will give us \\(\\boldsymbol{q}_2\\). (Stop now and prove that \\(\\boldsymbol{q}_2\\) is indeed perpendicular to both \\(\\boldsymbol{q}_1\\) and \\(\\boldsymbol{q}_0\\).)\n\n\nThe result should be the matrix \\(Q\\) which contains orthonormal columns. To build the matrix \\(R\\) we simply recall that \\(A = QR\\) and \\(Q^{-1} = Q^T\\) so \\(R = Q^T A\\).\n\n\n\nExercise C.44 Repeat the previous exercise but write code for each step so that Python can handle all of the computations. Again use the matrix \\[\\begin{equation}\nA = \\begin{pmatrix} 1 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix}.\n\\end{equation}\\]\n\n\n\nExample C.8 (QR for \\(n=3\\)) For the sake of clarity let us now write down the full \\(QR\\) factorization for a \\(3 \\times 3\\) matrix.\nIf the columns of \\(A\\) are \\(\\boldsymbol{a}_0\\), \\(\\boldsymbol{a}_1\\), and \\(\\boldsymbol{a}_2\\) then \\[\\begin{equation}\n\\boldsymbol{q}_0 = \\frac{\\boldsymbol{a}_0}{\\|\\boldsymbol{a}_0\\|}\n\\end{equation}\\]\n\\[\\begin{equation}\n\\boldsymbol{q}_1 = \\frac{ \\boldsymbol{a}_1 - \\left( \\boldsymbol{a}_1 \\cdot \\boldsymbol{q}_0 \\right) \\boldsymbol{q}_0 }{\\| \\boldsymbol{a}_1 - \\left( \\boldsymbol{a}_1 \\cdot \\boldsymbol{q}_0 \\right) \\boldsymbol{q}_0 \\|}\n\\end{equation}\\]\n\\[\\begin{equation}\n\\boldsymbol{q}_2 = \\frac{ \\boldsymbol{a}_2 - \\left( \\boldsymbol{a}_2 \\cdot \\boldsymbol{q}_0 \\right) \\boldsymbol{q}_0 - \\left( \\boldsymbol{a}_2 \\cdot \\boldsymbol{q}_1 \\right) \\boldsymbol{q}_1}{\\| \\boldsymbol{a}_2 - \\left( \\boldsymbol{a}_2 \\cdot \\boldsymbol{q}_0 \\right) \\boldsymbol{q}_0 - \\left( \\boldsymbol{a}_2 \\cdot \\boldsymbol{q}_1 \\right) \\boldsymbol{q}_1 \\|}\n\\end{equation}\\]\nand \\[\\begin{equation}\nR = \\begin{pmatrix} \\boldsymbol{a}_0 \\cdot \\boldsymbol{q}_0 & \\boldsymbol{a}_1 \\cdot \\boldsymbol{q}_0 & \\boldsymbol{a}_2 \\cdot \\boldsymbol{q}_0 \\\\ 0 & \\boldsymbol{a}_1 \\cdot \\boldsymbol{q}_1 & \\boldsymbol{a}_2 \\cdot \\boldsymbol{q}_1 \\\\ 0 & 0 & \\boldsymbol{a}_2 \\cdot \\boldsymbol{q}_2 \\end{pmatrix}\n\\end{equation}\\]\n\n\n\nExercise C.45 (The QR Factorization) Now we are ready to build general code for the \\(QR\\) factorization. The following Python function definition is partially complete. Fill in the missing pieces of code and then test your code on square matrices of many different sizes. The easiest way to check if you have an error is to find the normed difference between \\(A\\) and \\(QR\\) with np.linalg.norm(A - Q*R).\nimport numpy as np\ndef myQR(A):\n    n = A.shape[0]\n    Q = np.zeros( (n,n) )\n    for j in range( ??? ): # The outer loop goes over the columns\n        q = A[:,j]\n        # The next loop is meant to do all of the projections.\n        # When do you start the inner loop and how far do you go?\n        # Hint: You do not need to enter this loop the first time \n        for i in range( ??? ): \n            length_of_leg = np.dot(A[:,j], Q[:,i])\n            q = q -  ??? * ??? # This is where we do projections\n        Q[:,j] = q / np.linalg.norm(q)\n    R = # finally build the R matrix\n    return Q, R\n# Test Code\nA = np.array( ... )\n# or you can build A with use np.random.randn() \n# Often time random matrices are good test cases\nQ, R = myQR(A)\nerror = np.linalg.norm(A - Q @ R)\nprint(error)\n\n\nWe now have a robust algorithm for doing \\(QR\\) factorization of square matrices we can finally return to solving systems of equations.\n\nTheorem C.2 (Solving Systems with \\(QR\\)) Remember that we want to solve \\(A \\boldsymbol{x} = \\boldsymbol{b}\\) and since \\(A = QR\\) we can rewrite it with \\(QR \\boldsymbol{x} = \\boldsymbol{b}\\). Since we know that \\(Q\\) is orthonormal by design we can multiply both sides of the equation by \\(Q^T\\) to get \\(R \\boldsymbol{x} = Q^T \\boldsymbol{b}\\). Finally, since \\(R\\) is upper-triangular we can use our usolve code from the previous section to solve the resulting triangular system.\n\n\n\nExercise C.46 Solve the system of equations \\[\\begin{equation}\n\\begin{pmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 0 \\end{pmatrix} \\begin{pmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\end{pmatrix}\n\\end{equation}\\] by first computing the \\(QR\\) factorization of \\(A\\) and then solving the resulting upper-triangular system.\n\n\n\nExercise C.47 Write code that builds a random \\(n \\times n\\) matrix and a random \\(n \\times 1\\) vector. Solve the equation \\(A \\boldsymbol{x} = \\boldsymbol{b}\\) using the \\(QR\\) factorization and compare the answer to what we find from np.linalg.solve(). Do this many times for various values of \\(n\\) and create a plot with \\(n\\) on the horizontal axis and the normed error between Python‚Äôs answer and your answer from the \\(QR\\) algorithm on the vertical axis. It would be wise to use a plt.semilogy() plot. To find the normed difference you should use np.linalg.norm(). What do you notice?",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>¬† <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "nmLinAlg.html#over-determined-systems-and-curve-fitting",
    "href": "nmLinAlg.html#over-determined-systems-and-curve-fitting",
    "title": "Appendix C ‚Äî Linear Algebra",
    "section": "C.7 Over-determined Systems and Curve Fitting",
    "text": "C.7 Over-determined Systems and Curve Fitting\n\nExercise C.48 Consider the problem of finding the quadratic function \\(f(x) = ax^2 + bx + c\\) that best fits the points \\[\\begin{equation}\n(0, 1.07), (1, 3.9), (2, 14.8), (3, 26.8).\n\\end{equation}\\] We do not know the values of \\(a\\), \\(b\\), or \\(c\\) but we do have four different \\((x,y)\\) ordered pairs. Hence, we have four equations: \\[\\begin{equation}\n1.07 = a(0)^2 + b(0) + c\n\\end{equation}\\]\n\\[\\begin{equation}\n3.9 = a(1)^2 + b(1) + c\n\\end{equation}\\]\n\\[\\begin{equation}\n14.8 = a(2)^2 + b(2) + c\n\\end{equation}\\]\n\\[\\begin{equation}\n26.8 = a(3)^2 + b(3) + c.\n\\end{equation}\\]\nThere are four equations and only three unknowns. This is what is called an over determined systems ‚Äì when there are more equations than unknowns. Let us play with this problem.\n\nFirst turn the system of equations into a matrix equation. \\[\\begin{equation}\n\\begin{pmatrix} 0 & 0 & 1 \\\\ \\underline{\\hspace{0.25in}} & \\underline{\\hspace{0.25in}} & \\underline{\\hspace{0.25in}} \\\\ \\underline{\\hspace{0.25in}} & \\underline{\\hspace{0.25in}} & \\underline{\\hspace{0.25in}} \\\\ \\underline{\\hspace{0.25in}} & \\underline{\\hspace{0.25in}} & \\underline{\\hspace{0.25in}} \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\\\ c \\end{pmatrix} = \\begin{pmatrix} 1.07 \\\\ 3.9 \\\\ 14.8 \\\\ 26.8 \\end{pmatrix}.\n\\end{equation}\\]\nNone of our techniques for solving systems will likely work here since it is highly unlikely that the vector on the right-hand side of the equation is in the column space of the coefficient matrix. Discuss this.\nOne solution to the unfortunate fact from part (b) is that we can project the vector on the right-hand side into the subspace spanned by the columns of the coefficient matrix. Think of this as casting the shadow of the right-hand vector down onto the space spanned by the columns. If we do this projection we will be able to solve the equation for the values of \\(a\\), \\(b\\), and \\(c\\) that will create the projection exactly ‚Äì and hence be as close as we can get to the actual right-hand side. Draw a picture of what we have said here.\nNow we need to project the right-hand side, call it \\(\\boldsymbol{b}\\), onto the column space of the the coefficient matrix \\(A\\). Recall the following facts:\n\nProjections are dot products\nMatrix multiplication is nothing but a bunch of dot products.\nThe projections of \\(\\boldsymbol{b}\\) onto the columns of \\(A\\) are the dot products of \\(\\boldsymbol{b}\\) with each of the columns of \\(A\\).\nWhat matrix can we multiply both sides of the equation \\(A \\boldsymbol{x} = \\boldsymbol{b}\\) by in order for the right-hand side to become the projection that we want? (Now do the projection in Python)\n\nIf you have done part (d) correctly then you should now have a square system (i.e.¬†the matrix on the left-hand side should now be square). Solve this system for \\(a\\), \\(b\\), and \\(c\\).\n\n\n\n\nTheorem C.3 (Solving Overdetermined Systems) If \\(A \\boldsymbol{x} = \\boldsymbol{b}\\) is an overdetermined system (i.e.¬†\\(A\\) has more rows than columns) then we first multiply both sides of the equation by \\(A^T\\) (why do we do this?) and then solve the square system of equations \\((A^T A) \\boldsymbol{x} = A^T \\boldsymbol{b}\\) using a system solving like \\(LU\\) or \\(QR\\). The answer to this new system is interpreted as the vector \\(\\boldsymbol{x}\\) which solves exactly for the projection of \\(\\boldsymbol{b}\\) onto the column space of \\(A\\).\nThe equation \\((A^T A) \\boldsymbol{x} = A^T \\boldsymbol{b}\\) is called the normal equations and arises often in Statistics and Machine Learning.\n\n\n\nExercise C.49 Fit a linear function to the following data. Solve for the slope and intercept using the technique outlined in Theorem¬†C.3. Make a plot of the points along with your best fit curve.\n\n\n\n\\(x\\)\n\\(y\\)\n\n\n\n\n0\n4.6\n\n\n1\n11\n\n\n2\n12\n\n\n3\n19.1\n\n\n4\n18.8\n\n\n5\n39.5\n\n\n6\n31.1\n\n\n7\n43.4\n\n\n8\n40.3\n\n\n9\n41.5\n\n\n10\n41.6\n\n\n\n\n\n\nExercise C.50 Fit a quadratic function to the following data using the technique outlined in Theorem¬†C.3. Make a plot of the points along with your best fit curve.\n\n\n\n\\(x\\)\n\\(y\\)\n\n\n\n\n0\n-6.8\n\n\n1\n11.8\n\n\n2\n50.6\n\n\n3\n94\n\n\n4\n224.3\n\n\n5\n301.7\n\n\n6\n499.2\n\n\n7\n454.7\n\n\n8\n578.5\n\n\n9\n1102\n\n\n10\n1203.2\n\n\n\nCode to download the data directly is given below.\nimport numpy as np\nimport pandas as pd\nURL1 = 'https://raw.githubusercontent.com/NumericalMethodsSullivan'\nURL2 = '/NumericalMethodsSullivan.github.io/master/data/'\nURL = URL1+URL2\ndata = np.array( pd.read_csv(URL+'Exercise4_52.csv') )\n# Exercise4_52.csv\n\n\n\nExercise C.51 The Statistical technique of curve fitting is often called ‚Äúlinear regression.‚Äù This even holds when we are fitting quadratic functions, cubic functions, etc to the data ‚Ä¶ we still call that linear regression! Why?\n\n\nThis section of the text on solving over determined systems is just a bit of a teaser for a bit of higher-level statistics, data science, and machine learning. The normal equations and solving systems via projections is the starting point of many modern machine learning algorithms.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>¬† <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "nmLinAlg.html#the-eigenvalue-eigenvector-problem",
    "href": "nmLinAlg.html#the-eigenvalue-eigenvector-problem",
    "title": "Appendix C ‚Äî Linear Algebra",
    "section": "C.8 The Eigenvalue-Eigenvector Problem",
    "text": "C.8 The Eigenvalue-Eigenvector Problem\nWe finally turn our attention to another major topic in numerical linear algebra.4\n\nDefinition C.7 (The Eigenvalue Problem) Recall that the eigenvectors, \\(\\boldsymbol{x}\\), and the eigenvalues, \\(\\lambda\\) of a square matrix satisfy the equation \\(A\\boldsymbol{x}=\\lambda \\boldsymbol{x}\\). Geometrically, the eigen-problem is the task of finding the special vectors \\(\\boldsymbol{x}\\) such that multiplication by the matrix \\(A\\) only produces a scalar multiple of \\(\\boldsymbol{x}\\).\n\n\nThinking about matrix multiplication, the geometric notion of the eigenvalue problem is rather peculiar since matrix-vector multiplication usually results in a scaling and a rotation of the vector \\(\\boldsymbol{x}\\). Therefore, in some sense the eigenvectors are the only special vectors which avoid geometric rotation under matrix multiplication. For a graphical exploration of this idea see:\nhttps://www.geogebra.org/m/JP2XZpzV.\n\nRecall that to solve the eigen-problem for a square matrix \\(A\\) we complete the following steps:\n\nFirst rearrange the definition of the eigenvalue-eigenvector pair to \\[\\begin{equation}\n(A\\boldsymbol{x}-\\lambda \\boldsymbol{x})=\\boldsymbol{0}.\n\\end{equation}\\]\nNext, factor the \\(\\boldsymbol{x}\\) on the right to get \\[\\begin{equation}\n(A-\\lambda I) \\boldsymbol{x}=\\boldsymbol{0}.\n\\end{equation}\\]\nNow observe that since \\(\\boldsymbol{x} \\ne 0\\) the matrix \\(A-\\lambda I\\) must NOT have an inverse. Therefore, \\[\\begin{equation}\n\\det(A-\\lambda I)=0.\n\\end{equation}\\]\nSolve the equation \\(\\det(A-\\lambda I)=0\\) for all of the values of \\(\\lambda\\).\nFor each \\(\\lambda\\), find a solution to the equation \\((A-\\lambda I) \\boldsymbol{x}=\\boldsymbol{0}\\). Note that there will be infinitely many solutions so you will need to make wise choices for the free variables.\n\n\n\nExercise C.52 Find the eigenvalues and eigenvectors of \\[\\begin{equation}\nA = \\begin{pmatrix} 1 & 2 \\\\ 4 & 3 \\end{pmatrix}.\n\\end{equation}\\]\n\n\n\nExercise C.53 In the matrix \\[\\begin{equation}\nA = \\begin{pmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{pmatrix}\n\\end{equation}\\] one of the eigenvalues is \\(\\lambda_1 = 0\\).\n\nWhat does that tell us about the matrix \\(A\\)?\nWhat is the eigenvector \\(\\boldsymbol{v}_1\\) associated with \\(\\lambda_1 = 0\\)?\nWhat is the null space of the matrix \\(A\\)?\n\n\n\nOK. Now that you have recalled some of the basics, let us play with a little limit problem. The following exercises are going to work us toward the power method for finding certain eigen-structures of a matrix.\n\n\nExercise C.54 Consider the matrix \\[\\begin{equation}\nA = \\begin{pmatrix} 8 & 5 & -6 \\\\ -12 & -9 & 12 \\\\ -3 & -3 & 5 \\end{pmatrix}.\n\\end{equation}\\] This matrix has the following eigen-structure: \\[\\begin{equation}\n\\boldsymbol{v}_1 = \\begin{pmatrix} 1\\\\-1\\\\0\\end{pmatrix} \\quad \\text{with} \\quad \\lambda_1 = 3\n\\end{equation}\\]\n\\[\\begin{equation}\n\\boldsymbol{v}_2 = \\begin{pmatrix} 2\\\\0\\\\2\\end{pmatrix} \\quad \\text{with} \\quad \\lambda_2 = 2\n\\end{equation}\\]\n\\[\\begin{equation}\n\\boldsymbol{v}_3 = \\begin{pmatrix} -1\\\\3\\\\1\\end{pmatrix} \\quad \\text{with} \\quad \\lambda_3 = -1\n\\end{equation}\\]\nIf we have \\[\\begin{equation}\n\\boldsymbol{x} = -2 \\boldsymbol{v}_1 + 1 \\boldsymbol{v}_2 - 3 \\boldsymbol{v}_3 = \\begin{pmatrix} 3 \\\\ -7 \\\\ -1 \\end{pmatrix}\n\\end{equation}\\] then we want to do a bit of an experiment. What happens when we iteratively multiply \\(\\boldsymbol{x}\\) by \\(A\\) but at the same time divide by the largest eigenvalue. Let us see:\n\nWhat is \\(A^1 \\boldsymbol{x} / 3^1\\)?\nWhat is \\(A^2 \\boldsymbol{x} / 3^2\\)?\nWhat is \\(A^3 \\boldsymbol{x} / 3^3\\)?\nWhat is \\(A^4 \\boldsymbol{x} / 3^4\\)?\n‚Ä¶\n\nIt might be nice now to go to some Python code to do the computations (if you have not already). Use your code to conjecture about the following limit. \\[\\begin{equation}\n\\lim_{k \\to \\infty} \\frac{A^k \\boldsymbol{x}}{\\lambda_{max}^k} = ???.\n\\end{equation}\\] In this limit we are really interested in the direction of the resulting vector, not the magnitude. Therefore, in the code below you will see that we normalize the resulting vector so that it is a unit vector.\nNote: be careful, computers do not do infinity, so for powers that are too large you will not get any results.\nimport numpy as np\nA = np.array([[8,5,-6],[-12,-9,12],[-3,-3,5]])\nx = np.array([[3],[-7],[-1]])\neigval_max = 3\n\nk = 4\n# Note: For numpy arrays, A**k is element-wise power. \n# We must use np.linalg.matrix_power for matrix power.\nresult = np.linalg.matrix_power(A, k) @ x / eigval_max**k\nprint(result / np.linalg.norm(result) )\n\n\n\nExercise C.55 If a matrix \\(A\\) has eigenvectors \\(\\boldsymbol{v}_1\\), \\(\\boldsymbol{v}_2\\), \\(\\boldsymbol{v}_3\\), \\(\\cdots,\\) \\(\\boldsymbol{v}_n\\) with eigenvalues \\(\\lambda_1, \\lambda_2, \\lambda_3, \\ldots, \\lambda_n\\) and \\(\\boldsymbol{x}\\) is in the column space of \\(A\\) then what will we get, approximately, if we evaluate \\(A^k \\boldsymbol{x} / \\max_{j}(\\lambda_j)^k\\) for very large values of \\(k\\)?\nDiscuss your conjecture with your peers. Then try to verify it with several numerical examples.\n\n\n\nExercise C.56 Explain your result from the previous exercise geometrically.\n\n\n\nExercise C.57 The algorithm that we have been toying with will find the dominant eigenvector of a matrix fairly quickly. Why might you be only interested in the dominant eigenvector of a matrix? Discuss.\n\n\n\nExercise C.58 In this problem we will formally prove the conjecture that you just made. This conjecture will lead us to the power method for finding the dominant eigenvector and eigenvalue of a matrix.\n\nAssume that \\(A\\) has \\(n\\) linearly independent eigenvectors \\(\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\dots, \\boldsymbol{v}_n\\) and choose \\(\\boldsymbol{x} = \\sum_{j=1}^n c_j \\boldsymbol{v}_j\\). You have proved in the past that \\[\\begin{equation}\nA^k \\boldsymbol{x} = c_1 \\lambda_1^k \\boldsymbol{v}_1 + c_2 \\lambda_2^k \\boldsymbol{v}_2 + \\cdots c_n \\lambda_n^k \\boldsymbol{v}_n.\n\\end{equation}\\] Stop and sketch out the details of this proof now.\nIf we factor \\(\\lambda_1^k\\) out of the right-hand side we get \\[\\begin{equation}\nA^k \\boldsymbol{x} = \\lambda_1^k \\left( c_1 ??? + c_2 \\left( \\frac{???}{???} \\right)^k \\boldsymbol{v}_2 + c_3 \\left( \\frac{???}{???} \\right)^k \\boldsymbol{v}_3 + \\cdots + c_n \\left( \\frac{???}{???} \\right)^k \\boldsymbol{v}_n \\right)\n\\end{equation}\\] (fill in the question marks)\nIf \\(|\\lambda_1| &gt; |\\lambda_2| \\ge |\\lambda_3| \\ge \\cdots \\ge |\\lambda_n|\\) then what happens to each of the \\((\\lambda_j/\\lambda_1)^k\\) terms as \\(k \\to \\infty\\)?\nUsing your answer to part (c), what is \\(\\lim_{k \\to \\infty} A^k \\boldsymbol{x} / \\lambda_1^k\\)?\n\n\n\n\nTheorem C.4 (The Power Method) The following algorithm, called the power method will quickly find the eigenvalue of largest absolute value for a square matrix \\(A \\in \\mathbb{R}^{n \\times n}\\) as well as the associated (normalized) eigenvector. We are assuming that there are \\(n\\) linearly independent eigenvectors of \\(A\\).\n\nStep 1:\n\nGiven a non-zero vector \\(\\boldsymbol{x}\\), set \\(\\boldsymbol{v}^{(1)} = \\boldsymbol{x} / \\|\\boldsymbol{x}\\|\\). (Here the superscript indicates the iteration number) Note that the initial vector \\(\\boldsymbol{x}\\) is pretty irrelevant to the process so it can just be a random vector of the correct size..\n\nStep 2:\n\nFor \\(k=2, 3, \\ldots\\)\n\nStep 2a:\n\nCompute \\(\\tilde{\\boldsymbol{v}}^{(k)} = A \\boldsymbol{v}^{(k-1)}\\) (this gives a non-normalized version of the next estimate of the dominant eigenvector.)\n\nStep 2b:\n\nSet \\(\\lambda^{(k)} = \\tilde{\\boldsymbol{v}}^{(k)} \\cdot \\boldsymbol{v}^{(k-1)}\\). (this gives an approximation of the eigenvalue since if \\(\\boldsymbol{v}^{(k-1)}\\) was the actual eigenvector we would have \\(\\lambda = A \\boldsymbol{v}^{(k-1)} \\cdot \\boldsymbol{v}^{(k-1)}\\). Stop now and explain this.)\n\nStep 2c:\n\nNormalize \\(\\tilde{\\boldsymbol{v}}^{(k)}\\) by computing \\(\\boldsymbol{v}^{(k)} = \\tilde{\\boldsymbol{v}}^{(k)} / \\| \\tilde{\\boldsymbol{v}}^{(k)} \\|\\). (This guarantees that you will be sending a unit vector into the next iteration of the loop)\n\n\n\n\n\n\n\nExercise C.59 Go through Theorem¬†C.4 carefully and describe what we need to do in each step and why we are doing it. Then complete all of the missing pieces of the following Python function.\nimport numpy as np\ndef myPower(A, tol = 1e-8):\n    n = A.shape[0]\n    x = np.random.randn(n)\n    x = # turn x into a unit vector\n    # we do not actually need to keep track of the old iterates\n    L = 1 # initialize the dominant eigenvalue\n    counter = 0 # keep track of how many steps we have taken\n    # You can build a stopping rule from the definition\n    # Ax = lambda x ... \n    while (???) &gt; tol and counter &lt; 10000:\n        x = A @ x # update the dominant eigenvector\n        x = ??? # normalize\n        L = ??? # approximate the eignevalue\n        counter += 1 # increment the counter\n    return x, L\n\n\n\nExercise C.60 Test your myPower() function on several matrices where you know the eigenstructure. Then try the myPower() function on larger random matrices. You can check that it is working using np.linalg.eig() (be sure to normalize the vectors in the same way so you can compare them.)\n\n\n\nExercise C.61 In the Power Method iteration you may end up getting a different sign on your eigenvector as compared to np.linalg.eig(). Why might this happen? Generate a few examples so you can see this. You can avoid this issue if you use a while loop in your Power Method code and the logical check takes advantage of the fact that we are trying to solve the equation \\(A\\boldsymbol{x} = \\lambda \\boldsymbol{x}\\). Hint: \\(A\\boldsymbol{x} = \\lambda \\boldsymbol{x}\\) is equivalent to \\(A\\boldsymbol{x} - \\lambda \\boldsymbol{x} = \\boldsymbol{0}\\).\n\n\n\nExercise C.62 What happens in the power method iterations when \\(\\lambda_1\\) is complex. The maximum eigenvalue can certainly be complex if \\(|\\lambda_1|\\) (the modulus of the complex number) is larger than all of the other eigenvalues. It may be helpful to build a matrix specifically with complex eigenvalues.5\n\n\n\nExercise C.63 (Convergence Rate of the Power Method) The proof that the power method will work hinges on the fact that \\(|\\lambda_1| &gt; |\\lambda_2| \\geq |\\lambda_3| \\geq \\cdots \\geq |\\lambda_n|\\). In Exercise¬†C.58 we proved that the limit \\[\\begin{equation}\n\\lim_{k \\to \\infty} \\frac{A^k \\boldsymbol{x}}{\\lambda_1^k}\n\\end{equation}\\] converges to the dominant eigenvector, but how fast is the convergence? What does the speed of the convergence depend on?\nTake note that since we are assuming that the eigenvalues are ordered, the ratio \\(\\lambda_2 / \\lambda_1\\) will be larger than \\(\\lambda_j / \\lambda_1\\) for all \\(j&gt;2\\). Hence, the speed at which the power method converges depends mostly on the ratio \\(\\lambda_2 / \\lambda_1\\). Let us build a numerical experiment to see how sensitive the power method is to this ratio.\nBuild a \\(4 \\times 4\\) matrix \\(A\\) with dominant eigenvalue \\(\\lambda_1 = 1\\) and all other eigenvalues less than 1 in absolute value. Then choose several values of \\(\\lambda_2\\) and build an experiment to determine the number of iterations that it takes for the power method to converge to within a pre-determined tolerance to the dominant eigenvector. In the end you should produce a plot with the ratio \\(\\lambda_2 / \\lambda_1\\) on the horizontal axis and the number of iterations to converge to a fixed tolerance on the vertical axis. Discuss what you see in your plot.\nHint: To build a matrix with specific eigen-structure use the matrix factorization \\(A = PDP^{-1}\\) where the columns of \\(P\\) contain the eigenvectors of \\(A\\) and the diagonal of \\(D\\) contains the eigenvalues. In this case the \\(P\\) matrix can be random but you need to control the \\(D\\) matrix. Moreover, remember that \\(\\lambda_3\\) and \\(\\lambda_4\\) should be smaller than \\(\\lambda_2\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>¬† <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "nmLinAlg.html#algorithm-summaries",
    "href": "nmLinAlg.html#algorithm-summaries",
    "title": "Appendix C ‚Äî Linear Algebra",
    "section": "C.9 Algorithm Summaries",
    "text": "C.9 Algorithm Summaries\n\nExercise C.64 Explain in clear language how to efficiently solve an upper-triangular system of linear equations.\n\n\n\nExercise C.65 Explain in clear language how to efficiently solve a lower-triangular system of linear equations.\n\n\n\nExercise C.66 Explain in clear language how to solve the equation \\(A \\boldsymbol{x} = \\boldsymbol{b}\\) using an \\(LU\\) decomposition.\n\n\n\nExercise C.67 Explain in clear language how to solve an overdetermined system of linear equations (more equations than unknowns) numerically.\n\n\n\nExercise C.68 Explain in clear language the algorithm for finding the columns of the \\(Q\\) matrix in the \\(QR\\) factorization. Give all of the mathematical details.\n\n\n\nExercise C.69 Explain in clear language how to find the upper-triangular matrix \\(R\\) in the \\(QR\\) factorization. Give all of the mathematical details.\n\n\n\nExercise C.70 Explain in clear language how to solve the equation \\(A \\boldsymbol{x} = \\boldsymbol{b}\\) using a \\(QR\\) decomposition.\n\n\n\nExercise C.71 Explain in clear language how the power method works to find the dominant eigenvalue and eigenvector of a square matrix. Give all of the mathematical details.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>¬† <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "nmLinAlg.html#problems",
    "href": "nmLinAlg.html#problems",
    "title": "Appendix C ‚Äî Linear Algebra",
    "section": "C.10 Problems",
    "text": "C.10 Problems\n\nExercise C.72 As mentioned much earlier in this chapter, there is an rref() command in Python, but it is in the sympy library instead of the numpy library ‚Äì it is implemented as a symbolic computation instead of a numerical computation. OK. So what? In this problem we want to compare the time to solve a system of equations \\(A \\boldsymbol{x} = \\boldsymbol{b}\\) with each of the following techniques:\n\nrow reduction of an augmented matrix \\(\\begin{pmatrix} A & | & b\\end{pmatrix}\\) with sympy,\nour implementation of the \\(LU\\) decomposition,\nour implementation of the \\(QR\\) decomposition, and\nthe numpy.linalg.solve() command.\n\nTo time code in Python first import the time library. Then use start = time.time() at the start of your code and stop = time.time() and the end of your code. The difference between stop and start is the elapsed computation time.\nMake observations about how the algorithms perform for different sized matrices. You can use random matrices and vectors for \\(A\\) and \\(\\boldsymbol{b}\\). The end result should be a plot showing how the average computation time for each algorithm behaves as a function of the size of the coefficient matrix.\nThe code below will compute the reduced row echelon form of a matrix (RREF). Implement the code so that you know how it works.\nimport sympy as sp\nimport numpy as np\n# in this problem it will be easiest to start with numpy matrices\nA = np.array([[1, 0, 1], [2, 3, 5], [-1, -3, -3]])\nb = np.array([[3],[7],[3]])\nAugmented = np.c_[A,b] # augment b onto the right hand side of A\n\nMsymbolic = sp.Matrix(Augmented)\nMsymbolicRREF = Msymbolic.rref()\nprint(MsymbolicRREF)\nTo time code you can use code like the following.\nimport time\nstart = time.time()\n# some code that you want to time\nstop =  time.time()\ntotal_time = stop - start\nprint(\"Total computation time=\",total_time)\n\n\n\nExercise C.73 Imagine that we have a 1 meter long thin metal rod that has been heated to 100\\(^\\circ\\) on the left-hand side and cooled to 0\\(^\\circ\\) on the right-hand side. We want to know the temperature every 10 cm from left to right on the rod.\n\nFirst we break the rod into equal 10cm increments as shown. See Figure¬†C.2. How many unknowns are there in this picture?\nThe temperature at each point along the rod is the average of the temperatures at the adjacent points. For example, if we let \\(T_1\\) be the temperature at point \\(x_1\\) then \\[\\begin{equation}\nT_1 = \\frac{T_0 + T_2}{2}.\n\\end{equation}\\] Write a system of equations for each of the unknown temperatures.\nSolve the system for the temperature at each unknown node using either \\(LU\\) or \\(QR\\) decomposition.\n\n\n\n\n\n\n\nFigure¬†C.2: A rod to be heated broken into 10 equal-length segments.\n\n\n\n\n\n\nExercise C.74 Write code to solve the following systems of equations via both LU and QR decompositions. If the algorithm fails then be sure to explain exactly why.\n\n\\[\\begin{equation}\n\\begin{array}{rl} x + 2y + 3z &= 4 \\\\ 2x + 4y + 3z &= 5 \\\\ x + y &= 4 \\end{array}\n\\end{equation}\\]\n\\[\\begin{equation}\n\\begin{array}{rl} 2y + 3z &= 4 \\\\ 2x + 3z &= 5 \\\\ y &= 4 \\end{array}\n\\end{equation}\\]\n\\[\\begin{equation}\n\\begin{array}{rl} 2y + 3z &= 4 \\\\ 2x + 4y + 3z &= 5 \\\\ x+y &= 4 \\end{array}\n\\end{equation}\\]\n\n\n\n\nExercise C.75 Give a specific example of a non-zero matrix which will NOT have an \\(LU\\) decomposition. Give specific reasons why \\(LU\\) will fail on your matrix.\n\n\n\nExercise C.76 Give a specific example of a non-zero matrix which will NOT have an \\(QR\\) decomposition. Give specific reasons why \\(QR\\) will fail on your matrix.\n\n\n\nExercise C.77 Have you ever wondered how scientific software computes a determinant? The formula that you learned for calculating determinants by hand is horribly cumbersome and computationally intractable for large matrices. This problem is meant to give you glimpse of what is actually going on under the hood.6\nIf \\(A\\) has an \\(LU\\) decomposition then \\(A = LU\\). Use properties that you know about determinants to come up with a simple way to find the determinant for matrices that have an \\(LU\\) decomposition. Show all of your work in developing your formula.\nOnce you have your formula for calculating \\(\\det(A)\\), write a Python function that accepts a matrix, produces the \\(LU\\) decomposition, and returns the determinant of \\(A\\). Check your work against Python‚Äôs np.linalg.det() function.\n\n\n\nExercise C.78 For this problem we are going to run a numerical experiment to see how the process of solving the equation \\(A \\boldsymbol{x} = \\boldsymbol{b}\\) using the \\(LU\\) factorization performs on random coefficient matrices \\(A\\) and random right-hand sides \\(\\boldsymbol{b}\\). We will compare against Python‚Äôs algorithm for solving linear systems.\nWe will do the following:\nCreate a loop that does the following:\n\nLoop over the size of the matrix \\(n\\).\nBuild a random matrix \\(A\\) of size \\(n \\times n\\). You can do this with the code A = np.random.randn(n,n)\nBuild a random vector \\(\\boldsymbol{b}\\) in \\(\\mathbb{R}^{n}\\). You can do this with the code b = np.random.randn(n)\nFind Python‚Äôs answer to the problem \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) =0 using the command exact = np.linalg.solve(A,b)\nWrite code that uses your three \\(LU\\) functions (myLU, lsolve, usolve) to find a solution to the equation \\(A\\boldsymbol{x}=\\boldsymbol{b}\\).\nFind the error between your answer and the exact answer using the code np.linalg.norm(x - exact)\nMake a plot (plt.semilogy()) that shows how the error behaves as the size of the problem changes. You should run this for matrices of larger and larger size but be warned that the loop will run for quite a long time if you go above \\(300 \\times 300\\) matrices. Just be patient.\n\nConclusions: What do you notice in your final plot. What does this tell you about the behaviour of our \\(LU\\) decomposition code?\n\n\n\nExercise C.79 Repeat Exercise¬†C.78 for the \\(QR\\) decomposition. Your final plot should show both the behaviour of \\(QR\\) and of \\(LU\\) throughout the experiment. What do you notice?\n\n\n\nExercise C.80 Find a least squares solution to the equation \\(A \\boldsymbol{x} = \\boldsymbol{b}\\) in two different ways with \\[\\begin{equation}\nA = \\begin{pmatrix} 1 & 3 & 5 \\\\ 4 & -2 & 6 \\\\ 4 & 7 & 8 \\\\ 3 & 7 & 19 \\end{pmatrix} \\quad \\text{and} \\quad \\boldsymbol{b} = \\begin{pmatrix} 5 \\\\ 2 \\\\ -2 \\\\ 8\\end{pmatrix}.\n\\end{equation}\\]\n\n\n\nExercise C.81 Let \\(A\\) be defined as \\[\\begin{equation}\nA = \\begin{pmatrix} 10^{-20} & 1 \\\\ 1 & 1 \\end{pmatrix}\n\\end{equation}\\] and let \\(\\boldsymbol{b}\\) be the vector \\[\\begin{equation}\n\\boldsymbol{b}=\\begin{pmatrix} 2 \\\\ 3\\end{pmatrix}.\n\\end{equation}\\]\nNotice that \\(A\\) has a tiny, but non-zero, value in the first entry.\n\nSolve the linear system \\(A \\boldsymbol{x}=\\boldsymbol{b}\\) by hand.\nUse your myLU, lsolve, and usolve functions to solve this problem using the LU decomposition method.\nCompare your answers to parts (a) and (b). What went wrong?\n\n\n\n\nExercise C.82 (Hilbert Matrices) A Hilbert Matrix is a matrix of the form \\(H_{ij} = 1 / (i+j+1)\\) where both \\(i\\) and \\(j\\) both start indexed at \\(0\\). For example, a \\(4 \\times 4\\) Hilbert Matrix is \\[\\begin{equation}\nH = \\begin{pmatrix} 1 & \\frac{1}{2} & \\frac{1}{3} & \\frac{1}{4} \\\\ \\frac{1}{2} & \\frac{1}{3} & \\frac{1}{4} & \\frac{1}{5} \\\\ \\frac{1}{3} & \\frac{1}{4} & \\frac{1}{5} & \\frac{1}{6} \\\\ \\frac{1}{4} & \\frac{1}{5} & \\frac{1}{6} & \\frac{1}{7} \\end{pmatrix}.\n\\end{equation}\\] This type of matrix is often used to test numerical linear algebra algorithms since it is known to have some odd behaviours ‚Ä¶ which you will see in a moment.\n\nWrite code to build a \\(n\\times n\\) Hilbert Matrix and call this matrix \\(H\\). Test your code for various values of \\(n\\) to be sure that it is building the correct matrices.\nBuild a vector of ones called \\(\\boldsymbol{b}\\) with code b = np.ones((n,1)). We will use \\(\\boldsymbol{b}\\) as the right hand side of the system of equations \\(H\\boldsymbol{x} = \\boldsymbol{b}\\).\nSolve the system of equations \\(H \\boldsymbol{x} = \\boldsymbol{b}\\) using any technique you like from this chapter.\nNow let us say that you change the first entry of \\(\\boldsymbol{b}\\) by just a little bit, say \\(10^{-15}\\). If we were to now solve the equation \\(H \\boldsymbol{x}_{new} = \\boldsymbol{b}_{new}\\) what would you expect as compared to solving \\(H \\boldsymbol{x} = \\boldsymbol{b}\\).\nNow let us actually make the change suggested in part (d). Use the code bnew = np.ones((n,1)) and then bnew[0] = bnew[0] + 1e-15 to build a new \\(\\boldsymbol{b}\\) vector with this small change. Solve \\(H\\boldsymbol{x}=\\boldsymbol{b}\\) and \\(H\\boldsymbol{x}_{new}=\\boldsymbol{b}_{new}\\) and then compare the maximum absolute difference np.max(np.abs(x - xnew)). What do you notice? Make a plot with \\(n\\) on the horizontal axis and the maximum absolute difference on the vertical axis. What does this plot tell you about the solution to the equation \\(H \\boldsymbol{x} = \\boldsymbol{b}\\)?\nWe know that \\(H H^{-1}\\) should be the identity matrix. As we will see, however, Hilbert matrices are particularly poorly behaved! Write a loop over \\(n\\) that (i) builds a Hilbert matrix of size \\(n\\), (ii) calculates \\(H H^{-1}\\) (using np.linalg.inv() to compute the inverse directly), (iii) calculates the norm of the difference between the identity matrix (np.identity(n)) and your calculated identity matrix from part (ii). Finally. Build a plot that shows \\(n\\) on the horizontal axis and the normed difference on the vertical axis. What do you see? What does this mean about the matrix inversion of the Hilbert matrix.\nThere are cautionary tales hiding in this problem. Write a paragraph explaining what you can learn by playing with pathological matrices like the Hilbert Matrix.\n\n\n\n\nExercise C.83 Now that you have \\(QR\\) and \\(LU\\) code we are going to use both of them! The problem is as follows:\nWe are going to find the polynomial of degree 4 that best fits the function \\[\\begin{equation}\ny = \\cos(4t) + 0.1 \\varepsilon(t)\n\\end{equation}\\] at 50 equally spaced points \\(t\\) between \\(0\\) and \\(1\\). Here we are using \\(\\varepsilon(t)\\) as a function that outputs normally distributed random white noise. In Python you will build \\(y\\) as y = np.cos(4*t) + 0.1*np.random.randn(t.shape[0])\nBuild the \\(t\\) vector and the \\(y\\) vector (these are your data). We need to set up the least squares problems \\(A \\boldsymbol{x} = \\boldsymbol{b}\\) by setting up the matrix \\(A\\) as we did in the other least squares curve fitting problems and by setting up the \\(\\boldsymbol{b}\\) vector using the \\(y\\) data you just built. Solve the problem of finding the coefficients of the best degree 4 polynomial that fits this data. Report the sum of squared error and show a plot of the data along with the best fit curve.\n\n\n\nExercise C.84 Find the largest eigenvalue and the associated eigenvector of the matrix \\(A\\) WITHOUT using np.linalg.eig(). (Do not do this by hand either) \\[\\begin{equation}\nA = \\begin{pmatrix} 1 & 2 & 3 & 4 \\\\ 5 & 6 & 7 & 8 \\\\ 9 & 0 & 1 & 2 \\\\ 3 & 4 & 5 & 6 \\end{pmatrix}\n\\end{equation}\\]\n\n\n\nExercise C.85 It is possible in a matrix that the eigenvalues \\(\\lambda_1\\) and \\(\\lambda_2\\) are equal but with the corresponding eigenvectors not equal. Before you experiment with matrices of this sort, write a conjecture about what will happen to the power method in this case (look back to our proof in Exercise¬†C.58 of how the power method works). Now build several specific matrices where this is the case and see what happens to the power method.\n\n\n\nExercise C.86 Will the power method fail, slow down, or be unaffected if one (or more) of the non-dominant eigenvalues is zero? Give sufficient mathematical evidence or show several numerical experiments to support your answer.\n\n\n\nTheorem C.5 If \\(A\\) is a symmetric matrix with eigenvalues \\(\\lambda_1, \\lambda_2, \\ldots, \\lambda_n\\) then \\(|\\lambda_1| &gt; |\\lambda_2| &gt; \\cdots &gt; |\\lambda_n|\\). Furthermore, the eigenvectors will be orthogonal to each other.\n\n\n\nExercise C.87 (The Deflation Method) For symmetric matrices we can build an extension to the power method in order to find the second most dominant eigen-pair for a matrix \\(A\\). Theorem¬†C.5 suggests the following method for finding the second dominant eigen-pair for a symmetric matrix. This method is called the deflation method.\n\nUse the power method to find the dominant eigenvalue and eigenvector.\nStart with a random unit vector of the correct shape.\nMultiplying your vector by \\(A\\) will pull it toward the dominant eigenvector. After you multiply, project your vector onto the dominant eigenvector and find the projection error.\nUse the projection error as the new approximation for the eigenvector (Why should we do this? What are we really finding here?)\n\nNote that the deflation method is really exactly the same as the power method with the exception that we orthogonalize at every step. Hence, when you write your code expect to only change a few lines from your power method.\nWrite a function to find the second largest eigenvalue and eigenvector pair by putting the deflation method into practice. Test your code on a matrix \\(A\\) and compare against Python‚Äôs np.linalg.eig() command. Your code needs to work on symmetric matrices of arbitrary size and you need to write test code that clearly shows the error between your calculated eigenvalue and Python‚Äôs eigenvalue as well as your calculated eigenvector and ‚Äôs eigenvector.\nTo guarantee that you start with a symmetric matrix you can use the following code.\nimport numpy as np\nN = 40\nA = np.random.randn(N,N)\n# A = np.matrix(A) # No need for matrix class\nA = np.transpose(A) @ A # why should this build a symmetric matrix\n\n\n\nExercise C.88 (This concept for this problem is modified from (Meerschaert 2013)). The data is taken from NOAA and the National Weather Service with the specific values associated with La Crosse, WI.)\nFloods in the Mississippi River Valleys of the upper Midwest have somewhat predictable day-to-day behaviour in that the flood stage today has high predictive power for the flood stage tomorrow. Assume that the flood stages are:\n\nStage 0 (Normal): Average daily flow is below 90,000 \\(ft^3/sec\\) (cubic feet per second = cfs). This is the normal river level.\nStage 1 (Action Level): Average daily flow is between 90,000 cfs and 124,000 cfs.\nStage 2 (Minor Flood): Average daily flow is between 124,000 cfs and 146,000 cfs.\nStage 3 (Moderate Flood): Average daily flow is between 146,000 cfs and 170,000 cfs.\nStage 4 (Extreme Flood): Average daily flow is above 170,000 cfs.\n\nThe following table shows the probability of one stage transitioning into another stage from one day to the next.\n\n\n\n\n\n\n\n\n\n\n\n\n0 Today\n1 Today\n2 Today\n3 Today\n4 Today\n\n\n\n\n0 Tomorrow\n0.9\n0.3\n0\n0\n0\n\n\n1 Tomorrow\n0.05\n0.7\n0.4\n0\n0\n\n\n2 Tomorrow\n0.025\n0\n0.6\n0.6\n0\n\n\n3 Tomorrow\n0.015\n0\n0\n0.4\n0.8\n\n\n4 Tomorrow\n0.01\n0\n0\n0\n0.2\n\n\n\nMathematically, if \\(\\boldsymbol{s}_k\\) is the state at day \\(k\\) and \\(A\\) is the matrix given in the table above then the difference equation \\(\\boldsymbol{s}_{k+1} = A \\boldsymbol{s}_k\\) shows how a state will transition from day to day. For example, if we are currently in Stage 0 then \\[\\begin{equation}\n\\boldsymbol{s}_0 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n\\end{equation}\\] We can interpret this as ‚Äúthere is a probability of 1 that we are in Stage 0 today and there is a probability of 0 that we are in any other stage today.‚Äù\nIf we want to advance this model forward in time then we just need to iterate. In our example, the state tomorrow would be \\(\\boldsymbol{s}_1 = A \\boldsymbol{s}_0\\). The state two days from now would be \\(\\boldsymbol{s}_2 = A \\boldsymbol{s}_1\\), and if we use the expression for \\(\\boldsymbol{s}_1\\) we can simplify to \\(\\boldsymbol{s}_2 = A^2 \\boldsymbol{s}_0\\).\n\nProve that the state at day \\(n\\) is \\(\\boldsymbol{s}_n = A^n \\boldsymbol{s}_0\\).\nIf \\(n\\) is large then the steady state solution to the difference equation in part (1) is given exactly by the power method iteration that we have studied in this chapter. Hence, as the iterations proceed they will be pulled toward the dominant eigenvector. Use the power method to find the dominant eigenvector of the matrix \\(A\\).\nThe vectors in this problem are called probability vectors in the sense that the vectors sum to 1 and every entry can be interpreted as a probability. Re-scale your answer from part (b) so that we can interpret the entries as probabilities. That is, ensure that the sum of the vector from part (b) is 1.\nInterpret your answer to part (c) in the context of the problem. Be sure that your interpretation could be well understood by someone that does not know the mathematics that you just did.\n\n\n\n\nExercise C.89 The \\(LU\\) factorization as we have built it in this chapter is not smart about the way that it uses the memory on your computer. In the \\(LU\\) factorization the 1‚Äôs on the main diagonal do not actually need to be stored since we know that they will always be there. The zeros in the lower triangle of \\(U\\) do not need to be stored either. If you store the upper triangle values in the \\(U\\) matrix on top of the upper triangle of the \\(L\\) matrix then we still store a full matrix for \\(L\\) which contains both \\(L\\) and \\(U\\) simultaneously, but we do not have to store \\(U\\) separately and hence save computer memory. The modifications to the existing code for an \\(LU\\) solve is minimal ‚Äì every time you call on an entry of the \\(U\\) matrix it is stored in the upper triangle of \\(L\\) instead. Write code to implement this new data storage idea and demonstrate your code on a few examples.\n\n\n\nExercise C.90 In the algorithm that we used to build the \\(QR\\) factorization we built the \\(R\\) matrix as \\(R = Q^T A\\) The trouble with this step is that it fills in a lot of redundant zeros into the \\(R\\) matrix ‚Äì some of which may not be exactly zero. First explain why this will be the case. Then rewrite your \\(QR\\) factorization code so that the top triangle of \\(R\\) is filled with all of the projections (do this with a double for loop). Demonstrate that your code works on a few examples.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>¬† <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "nmLinAlg.html#projects",
    "href": "nmLinAlg.html#projects",
    "title": "Appendix C ‚Äî Linear Algebra",
    "section": "C.11 Projects",
    "text": "C.11 Projects\nIn this section we propose several ideas for projects related to numerical linear algebra. These projects are meant to be open ended, to encourage creative mathematics, to push your coding skills, and to require you to write and communicate your mathematics.\n\nC.11.1 The Google Page Rank Algorithm\nIn this project you will discover how the Page Rank algorithm works to give the most relevant information as the top hit on a Google search.\nSearch engines compile large indexes of the dynamic information on the Internet so they are easily searched. This means that when you do a Google search, you are not actually searching the Internet; instead, you are searching the indexes at Google.\nWhen you type a query into Google the following two steps take place:\n\nQuery Module: The query module at Google converts your natural language into a language that the search system can understand and consults the various indexes at Google in order to answer the query. This is done to find the list of relevant pages.\nRanking Module: The ranking module takes the set of relevant pages and ranks them. The outcome of the ranking is an ordered list of web pages such that the pages near the top of the list are most likely to be what you desire from your search. This ranking is the same as assigning a popularity score to each web site and then listing the relevant sites by this score.\n\nThis section focuses on the Linear Algebra behind the Ranking Module developed by the founders of Google: Sergey Brin and Larry Page. Their algorithm is called the Page Rank algorithm, and you use it every single time you use Google‚Äôs search engine.\nIn simple terms: A webpage is important if it is pointed to by other important pages.\nThe Internet can be viewed as a directed graph (look up this term here on Wikipedia) where the nodes are the web pages and the edges are the hyperlinks between the pages. The hyperlinks into a page are called in links, and the ones pointing out of a page are called out links. In essence, a hyperlink from my page to yours is my endorsement of your page. Thus, a page with more recommendations must be more important than a page with a few links. However, the status of the recommendation is also important.\nLet us now translate this into mathematics. To help understand this we first consider the small web of six pages shown in Figure¬†C.3 (a graph of the router level of the internet can be found here). The links between the pages are shown by arrows. An arrow pointing into a node is an in link and an arrow pointing out of a node is an out link. In Figure¬†C.3, node 3 has three out links (to nodes 1, 2, and 5) and 1 in link (from node 1).\n\n\n\n\n\n\nFigure¬†C.3: Example web graph.\n\n\n\nWe will first define some notation in the Page Rank algorithm:\n\n\\(|P_i|\\) is the number of out links from page \\(P_i\\)\n\\(H\\) is the hyperlink matrix defined as \\[\\begin{equation}\nH_{ij} = \\left\\{ \\begin{array}{cl} \\frac{1}{|P_j|}, & \\text{if there is a link from node $j$ to node $i$} \\\\ 0, & \\text{otherwise} \\end{array} \\right.\n\\end{equation}\\] where the ‚Äú\\(i\\)‚Äù and ‚Äú\\(j\\)‚Äù are the row and column indices respectively.\n\\(\\boldsymbol{x}\\) is a vector that contains all of the Page Ranks for the individual pages.\n\nThe Page Rank algorithm works as follows:\n\nInitialize the page ranks to all be equal. This means that our initial assumption is that all pages are of equal rank. In the case of Figure¬†C.3 we would take \\(\\boldsymbol{x}_0\\) to be \\[\\begin{equation}\n\\boldsymbol{x}_0 = \\begin{pmatrix} 1/6 \\\\ 1/6 \\\\ 1/6 \\\\ 1/6 \\\\ 1/6 \\\\ 1/6 \\end{pmatrix}.\n\\end{equation}\\]\nBuild the hyperlink matrix.\nAs an example we will consider node 3 in Figure¬†C.3. There are three out links from node 3 (to nodes 1, 2, and 5). Hence \\(H_{13}=1/3\\), \\(H_{23} = 1/3\\), and \\(H_{53} = 1/3\\) and the partially complete hyperlink matrix is \\[\\begin{equation}\nH = \\begin{pmatrix} - & - & 1/3 & - & - & - \\\\ - & - & 1/3 & - & - & - \\\\ - & - & 0 & - & - & - \\\\ - & - & 0 & - & - & - \\\\ - & - & 1/3 & - & - & - \\\\ - & - & 0 & - & - & - \\end{pmatrix}\n\\end{equation}\\]\nThe difference equation \\(\\boldsymbol{x}_{n+1} = H \\boldsymbol{x}_n\\) is used to iteratively refine the estimates of the page ranks. You can view the iterations as a person visiting a page and then following a link at random, then following a random link on the next page, and the next, and the next, etc. Hence we see that the iterations evolve exactly as expected for a difference equation.\n\n\n\n\n\n\n\n\nIteration\nNew Page Rank Estimation\n\n\n\n\n0\n\\(\\boldsymbol{x}_0\\)\n\n\n1\n\\(\\boldsymbol{x}_1 = H \\boldsymbol{x}_0\\)\n\n\n2\n\\(\\boldsymbol{x}_2 = H \\boldsymbol{x}_1 = H^2 \\boldsymbol{x}_0\\)\n\n\n3\n\\(\\boldsymbol{x}_3 = H \\boldsymbol{x}_2 = H^3 \\boldsymbol{x}_0\\)\n\n\n4\n\\(\\boldsymbol{x}_4 = H \\boldsymbol{x}_3 = H^4 \\boldsymbol{x}_0\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(k\\)\n\\(\\boldsymbol{x}_k = H^k \\boldsymbol{x}_0\\)\n\n\n\n\nWhen a steady state is reached we sort the resulting vector \\(\\boldsymbol{x}_k\\) to give the page rank. The node (web page) with the highest rank will be the top search result, the second highest rank will be the second search result, and so on.\n\nIt does not take much to see that this process can be very time consuming. Think about your typical web search with hundreds of thousands of hits; that makes a square matrix \\(H\\) that has a size of hundreds of thousands of entries by hundreds of thousands of entries! The matrix multiplications alone would take many minutes (or possibly many hours) for every search! ‚Ä¶but Brin and Page were pretty smart dudes!!\nWe now state a few theorems and definitions that will help us simplify the iterative Page Rank process.\n\n\nTheorem C.6 If \\(A\\) is an \\(n \\times n\\) matrix with \\(n\\) linearly independent eigenvectors \\(\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\boldsymbol{v}_3,\\) \\(\\ldots, \\boldsymbol{v}_n\\) and associated eigenvalues \\(\\lambda_1, \\lambda_2, \\lambda_3, \\ldots, \\lambda_n\\) then for any initial vector \\(\\boldsymbol{x} \\in \\mathbb{R}^n\\) we can write \\(A^k \\boldsymbol{x}\\) as \\[\\begin{equation}\nA^k \\boldsymbol{x} = c_1 \\lambda_1^k \\boldsymbol{v}_1 + c_2 \\lambda_2^k \\boldsymbol{v}_2 + c_3 \\lambda_3^k \\boldsymbol{v}_3 + \\cdots c_n \\lambda_n^k \\boldsymbol{v}_n\n\\end{equation}\\] where \\(c_1, c_2, c_3, \\ldots, c_n\\) are the constants found by expressing \\(\\boldsymbol{x}\\) as a linear combination of the eigenvectors.\nNote: We can assume that the eigenvalues are ordered such that \\(|\\lambda_1| &gt; |\\lambda_2| \\ge |\\lambda_3| \\ge \\cdots \\ge |\\lambda_n|\\).\n\n\n\nExercise C.91 Prove the preceding theorem.\n\n\n\nDefinition C.8 A probability vector is a vector with entries on the interval \\([0,1]\\) that add up to 1. A stochastic matrix is a square matrix whose columns are probability vectors.\n\n\n\nTheorem C.7 If \\(A\\) is a stochastic \\(n \\times n\\) matrix then \\(A\\) will have \\(n\\) linearly independent eigenvectors. Furthermore, the largest eigenvalue of a stochastic matrix will be \\(\\lambda_1 = 1\\) and the smallest eigenvalue will always be non-negative: \\(0 \\le |\\lambda_n| &lt; 1\\).\nSome of the following tasks will ask you to prove a statement or a theorem. This means to clearly write all of the logical and mathematical reasons why the statement is true. Your proof should be absolutely crystal clear to anyone with a similar mathematical background ‚Ä¶if you are in doubt then have a peer from a different group read your proof to you .\n\n\n\nExercise C.92 Finish writing the hyperlink matrix \\(H\\) from Figure¬†C.3.\n\n\n\nExercise C.93 Write code to implement the iterative process defined previously. Make a plot that shows how the rank evolves over the iterations.\n\n\n\nExercise C.94 What must be true about a collection of \\(n\\) pages such that an \\(n\\times n\\) hyperlink matrix \\(H\\) is a stochastic matrix.\n\n\n\nExercise C.95 The statement of the next theorem is incomplete, but the proof is given to you. Fill in the blank in the statement of the theorem and provide a few sentences supporting your answer.\n\n\n\nTheorem C.8 If \\(A\\) is an \\(n \\times n\\) stochastic matrix and \\(\\boldsymbol{x}_0\\) is some initial vector for the difference equation \\(\\boldsymbol{x}_{n+1} = A \\boldsymbol{x}_n\\), then the steady state vector is \\[\\begin{equation}\n\\boldsymbol{x}_{equilib} = \\lim_{k \\to\\infty} A^k \\boldsymbol{x}_0 = \\underline{\\hspace{1in}}.\n\\end{equation}\\]\nProof:\nFirst note that \\(A\\) is an \\(n \\times n\\) stochastic matrix so from Theorem¬†C.7 we know that there are \\(n\\) linearly independent eigenvectors. We can then substitute the eigenvalues from Theorem¬†C.7 in Theorem¬†C.6. Noting that if \\(0&lt;\\lambda_j&lt;1\\) we have \\(\\lim_{k \\to \\infty} \\lambda_j^k = 0\\) the result follows immediately.\n\n\n\nExercise C.96 Discuss how Theorem¬†C.8 greatly simplifies the PageRank iterative process described previously. In other words: there is no reason to iterate at all. Instead, just find ‚Ä¶ what?\n\n\n\nExercise C.97 Now use the previous two problems to find the resulting PageRank vector from the web in Figure¬†C.3? Be sure to rank the pages in order of importance. Compare your answer to the one that you got in problem 2.\n\n\n\n\n\n\n\n\nFigure¬†C.4: A second example web graph.\n\n\n\n\nExercise C.98 Consider the web in Figure¬†C.4.\n\nWrite the \\(H\\) matrix and find the initial state \\(\\boldsymbol{x}_0\\),\nFind steady state PageRank vector using the two different methods described: one using the iterative difference equation and the other using Theorem¬†C.8 and the dominant eigenvector.\nRank the pages in order of importance.\n\n\n\n\nExercise C.99 One thing that we did not consider in this version of the Google Page Rank algorithm is the random behaviour of humans. One, admittedly slightly naive, modification that we can make to the present algorithm is to assume that the person surfing the web will randomly jump to any other page in the web at any time. For example, if someone is on page 1 in Figure¬†C.4 then they could randomly jump to any page 2 - 8. They also have links to pages 2, 3, and 7. That is a total of 10 possible next steps for the web surfer. There is a \\(2/10\\) chance of heading to page 2. One of those is following the link from page 1 to page 2 and the other is a random jump to page 2 without following the link. Similarly, there is a \\(2/10\\) chance of heading to page 3, \\(2/10\\) chance of heading to page 7, and a \\(1/10\\) chance of randomly heading to any other page.\nImplement this new algorithm, called the random surfer algorithm, on the web in Figure¬†C.4. Compare your ranking to the non-random surfer results from the previous problem.\n\n\n\n\nC.11.2 Alternative Methods For Solving \\(A \\boldsymbol{x} = \\boldsymbol{b}\\)\nThroughout most of the linear algebra chapter we have studied ways to solve systems of equations of the form \\(A\\boldsymbol{x} = \\boldsymbol{b}\\) where \\(A\\) is a square \\(n \\times n\\) matrix, \\(\\boldsymbol{x} \\in \\mathbb{R}^n\\), and \\(\\boldsymbol{b} \\in\\mathbb{R}^n\\). We have reviewed by-hand row reduction and learned new techniques such as the \\(LU\\) decomposition and the \\(QR\\) decomposition ‚Äì all of which are great in their own right and all of which have their shortcomings.\nBoth \\(LU\\) and \\(QR\\) are great solution techniques and they generally work very very well. However (no surprise), we can build algorithms that will usually be faster!\nIn the following new algorithms we want to solve the linear system of equations \\[\\begin{equation}\nA \\boldsymbol{x} = \\boldsymbol{b}\n\\end{equation}\\] but in each we will do so iteratively by applying an algorithm over and over until the algorithm converges to an approximation of the solution vector \\(\\boldsymbol{x}\\). Convergence here means that \\(\\|A\\ \\boldsymbol{x} - \\boldsymbol{b} \\|\\) is less than some pre-determined tolerance.\nMethod 1: Start by ‚Äúfactoring‚Äù7 the matrix \\(A\\) into \\(A = L + U\\) where \\(L\\) is a lower-triangular matrix and \\(U\\) is an upper-triangular matrix. Take note that this time we will not force the diagonal entries of \\(L\\) to be \\(1\\) like we did in the classical \\(LU\\) factorization . The \\(U\\) in the factorization \\(A = L + U\\) is an upper-triangular matrix where the entries on the main diagonal are exactly 0.\nSpecifically, \\[\\begin{equation}\n\\begin{split}\nA = L + U =& \\begin{pmatrix} a_{00} & 0 & 0 & \\cdots & 0 \\\\ a_{10} & a_{11} & 0 & \\cdots & 0 \\\\ a_{20} & a_{21} & a_{22} & \\cdots & 0 \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{n0} & a_{n1} & a_{n2} & \\cdots & a_{n-1,n-1} \\end{pmatrix}\\\\\n&+ \\begin{pmatrix} 0 & a_{01} & a_{02} & \\cdots & a_{0,n-1} \\\\ 0 & 0 & a_{12} & \\cdots & a_{1,n-1} \\\\ 0 & 0 & 0 & a_{23} & \\cdots & \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & 0 & & 0 \\end{pmatrix}.\n\\end{split}\n\\end{equation}\\]\nAs an example,\n\\[\\begin{equation}\n\\begin{pmatrix} 2 & 3 & 4 \\\\ 5 & 6 & 7 \\\\ 8 & 9 & 1 \\end{pmatrix} = \\begin{pmatrix} 2 & 0 & 0 \\\\ 5 & 6 & 0 \\\\ 8 & 9 & 1\\end{pmatrix} + \\begin{pmatrix} 0 & 3 & 4 \\\\ 0 & 0 & 7 \\\\ 0 & 0 & 0 \\end{pmatrix}.\n\\end{equation}\\]\nAfter factoring the system of equations can be rewritten as \\[\\begin{equation}\nA \\boldsymbol{x} = \\boldsymbol{b} \\implies (L+U)x = \\boldsymbol{b} \\implies L\\boldsymbol{x} + U\\boldsymbol{x} = \\boldsymbol{b}.\n\\end{equation}\\] Moving the term \\(U\\boldsymbol{x}\\) to the right-hand side gives \\(L\\boldsymbol{x} = b-U\\boldsymbol{x}\\), and if we solve for the unknown \\(\\boldsymbol{x}\\) we get \\(\\boldsymbol{x} = L^{-1}(\\boldsymbol{b}-U\\boldsymbol{x}).\\)\nOf course we would never (ever!) actually compute the inverse of \\(L\\), and consequently we have to do something else in place of the matrix inverse. Stop and think here for a moment. We have run into this problem earlier in this chapter and you have some code that you will need to modify for this job (but take very careful note that the \\(L\\) matrix here does not quite have the same structure as the \\(L\\) matrix we used in the past). Moreover, notice that we have the unknown \\(\\boldsymbol{x}\\) on both sides of the equation. Initially this may seem like nonsense, but if we treat this as an iterative scheme by first making a guess about \\(x\\) and then iteratively find better approximations of solutions via the difference equation \\[\\begin{equation}\n\\boldsymbol{x}_{k+1} = L^{-1} (b-U\\boldsymbol{x}_k)\n\\end{equation}\\] we may, under moderate conditions on \\(A\\), quickly be able to approximate the solution to \\(A\\boldsymbol{x}=\\boldsymbol{b}\\). The subscripts in the iterative scheme represent the iteration number. Hence, \\[\\begin{equation}\n\\boldsymbol{x}_{1} = L^{-1} (b-U\\boldsymbol{x}_0)\n\\end{equation}\\]\n\\[\\begin{equation}\n\\boldsymbol{x}_{2} = L^{-1} (b-U\\boldsymbol{x}_1)\n\\end{equation}\\]\n\\[\\begin{equation}\n\\vdots\n\\end{equation}\\]\nWhat we need to pay attention to is that the method is not guaranteed to converge to the actual solution to the equation \\(A \\boldsymbol{x} = \\boldsymbol{b}\\) unless some conditions on \\(A\\) are met, and you will need to experiment with the algorithm to come up with a conjecture about the appropriate conditions.\nMethod 2: Start by factoring the matrix \\(A\\) into \\(A = L + D + U\\) where \\(L\\) is strictly lower-triangular (0‚Äôs on the main diagonal and in the entire upper triangle), \\(D\\) is a diagonal matrix, and \\(U\\) is a strictly upper-triangular matrix (0‚Äôs on the main diagonal and in the entire lower triangle). In this new factorization, the diagonal matrix \\(D\\) simply contains the entries from the main diagonal of \\(A\\). The \\(L\\) matrix is the lower triangle of \\(A\\), and the \\(U\\) matrix is the upper triangle of \\(A\\).\nConsidering the system of equations \\(A\\boldsymbol{x} = \\boldsymbol{b}\\) we get \\[\\begin{equation}\n(L+D+U)\\boldsymbol{x} = \\boldsymbol{b}\n\\end{equation}\\] and after simplifying, rearranging, and solving for \\(\\boldsymbol{x}\\) we get \\(\\boldsymbol{x} = D^{-1} (b-L\\boldsymbol{x}-U\\boldsymbol{x}).\\) A moment‚Äôs reflection should reveal that the inverse of \\(D\\) is really easy to find (no heavy-duty linear algebra necessary) if some mild conditions on the diagonal entries of \\(A\\) are met. Like before there is an \\(\\boldsymbol{x}\\) on both sides of the equation, but if we again make the algorithm iterative we can get successive approximations of the solution with \\[\\begin{equation}\n\\boldsymbol{x}_{k+1} = D^{-1}(b - L\\boldsymbol{x}_k - U\\boldsymbol{x}_k).\n\\end{equation}\\]\nYour Tasks:\n\nPick a small (larger than \\(3 \\times 3\\)) matrix and an appropriate right-hand side \\(\\boldsymbol{b}\\) and work each of the algorithms by hand. You do not need to write this step up in the final product, but this exercise will help you locate where things may go wrong in the algorithms and what conditions we might need on \\(A\\) in order to get convergent sequences of approximate solutions.\nBuild Python functions that accept a square matrix \\(A\\) and complete the factorizations \\(A = L+U\\) and \\(A = L+D+U\\).\nBuild functions to implement the two methods and then demonstrate that the methods work on a handful of carefully chosen test examples. As part of these functions you need to build a way to deal with the matrix inversions as well as build a stopping rule for the iterative schemes. Hint: You should use a while loop with a proper logical condition. Think carefully about what we are finding at each iteration and what we can use to check our accuracy at each iteration. It would also be wise to write your code in such a way that it checks to see if the sequence of approximations is diverging.\nDiscuss where each method might fail and then demonstrate the possible failures with several carefully chosen examples. Stick to small examples and work these out by hand to clearly show the failure.\nIterative methods such as these will produce a sequence of approximations, but there is no guarantee that either method will actually produce a convergent sequence. Experiment with several examples and propose a condition on the matrix \\(A\\) which will likely result in a convergent sequence. Demonstrate that the methods fail if your condition is violated and that the methods converge if your condition is met. Take care that it is tempting to think that your code is broken if it does not converge. The more likely scenario is that the problem that you have chosen to solve will result in a non-convergent sequence of iterations, and you need to think and experiment carefully when choosing the example problems to solve. One such convergence criterion has something to do with the diagonal entries of \\(A\\) relative to the other entries, but that does not mean that you should not explore other features of the matrices as well (I cannot give you any more hints than that). This task is not asking for a proof; just a conjecture and convincing numerical evidence that the conjecture holds. The actual proofs are beyond the scope of this project and this course.\nDevise a way to demonstrate how the time to solve a large linear system \\(A\\boldsymbol{x} = \\boldsymbol{b}\\) compares between our two new methods, the \\(LU\\) algorithm, and the \\(QR\\) algorithm that we built earlier in the chapter. Conclude this demonstration with appropriate plots and ample discussion.\n\nYou need to do this project without the help of your old buddy Google. All code must be originally yours or be modified from code that we built in class. You can ask Google how Python works with matrices and the like, but searching directly for the algorithms (which are actually well-known, well-studied, and named algorithms) is not allowed.\nFinally, solving systems of equations with the |np.linalg.solve() command can only be done to verify or check your answer(s).\n\n\n\n\nMeerschaert, Mark. 2013. Mathematical Modeling. 4th edition. Amsterdam ; Boston: Academic Press.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>¬† <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "nmLinAlg.html#footnotes",
    "href": "nmLinAlg.html#footnotes",
    "title": "Appendix C ‚Äî Linear Algebra",
    "section": "",
    "text": "You should also note that \\(\\|\\boldsymbol{u}\\| = \\sqrt{\\boldsymbol{u} \\cdot \\boldsymbol{u}}\\) is not the only definition of distance. More generally, if you let \\(\\left&lt; \\boldsymbol{u}, \\boldsymbol{v}\\right&gt;\\) be an inner product for \\(\\boldsymbol{u}\\) and \\(\\boldsymbol{v}\\) in some vector space \\(\\mathcal{V}\\) then \\(\\|\\boldsymbol{u}\\| = \\sqrt{\\left&lt; \\boldsymbol{u}, \\boldsymbol{u}\\right&gt;}\\). In most cases in this text we will be using the dot product as our preferred inner product so we will not have to worry much about this particular natural extension of the definition of the length of a vector.‚Ü©Ô∏é\nYou might have thought that naive multiplication was a much more natural way to do matrix multiplication when you first saw it. Hopefully now you see the power in the definition of matrix multiplication that we actually use. If not, then I give you this moment to ponder that (a) matrix multiplication is just a bunch of dot products, and (b) dot products can be seen as projections. Hence, matrix multiplication is really just a projection of the rows of \\(A\\) onto the columns of \\(B\\). This has much more rich geometric flavour than naive multiplication.‚Ü©Ô∏é\nTake careful note here. We have actually just built a special case of the \\(LU\\) decomposition. Remember that in row reduction you are allowed to swap the order of the rows, but in our \\(LU\\) algorithm we do not have any row swaps. The version of \\(LU\\) with row swaps is called \\(LU\\) with partial pivoting. We will not built the full partial pivoting algorithm in this text but feel free to look it up. The wikipedia page is a decent place to start. What you will find is that there are indeed many different versions of the \\(LU\\) decomposition.‚Ü©Ô∏é\nNumerical Linear Algebra is a huge field and there is way more to say ‚Ä¶ but alas, this is an introductory course in Numerical Analysis so we cannot do everything. Sigh.‚Ü©Ô∏é\nTo build a matrix with specific eigenvalues it may be helpful to recall the matrix factorization \\(A = PDP^{-1}\\) where the columns of \\(P\\) are the eigenvectors of \\(A\\) and the diagonal entries of \\(D\\) are the eigenvalues. If you choose \\(P\\) and \\(D\\) then you can build \\(A\\) with your specific eigen-structure. If you are looking for complex eigenvalues then remember that the eigenvectors may well be complex too.‚Ü©Ô∏é\nActually, the determinant computation uses LU with partial pivoting which we did not cover here in the text. What we are looking at in this exercise is a smaller sub-case of what happens when you have a matrix \\(A\\) that does not require any row swaps in the row reduction process.‚Ü©Ô∏é\nTechnically speaking we should not call this a ‚Äúfactorization‚Äù since we have not split the matrix \\(A\\) into a product of two matrices. Instead we should call it a ‚Äúpartition‚Äù since in number theory we call the process of breaking an integer into the sum of two integers is called a ‚Äúpartition.‚Äù Even so, we will still use the word factorization here for simplicity.‚Ü©Ô∏é",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>¬† <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "nmRoots1.html#footnotes",
    "href": "nmRoots1.html#footnotes",
    "title": "3¬† Roots 1",
    "section": "",
    "text": "You may find that we are going too slow for you and that you have the capacity and interest to learn more. In this case I can recommend the optional Appendix C on Numerical Linear Algebra.‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Roots 1</span>"
    ]
  }
]