<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2&nbsp; Functions ‚Äì Numerical Analysis</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./nmRoots1.html" rel="next">
<link href="./nmNumbers.html" rel="prev">
<link href="./faviconNA.webp" rel="icon">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-6686436bd8a269d58a385b3c8f4f2f3a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="2&nbsp; Functions ‚Äì Numerical Analysis">
<meta property="og:description" content="">
<meta property="og:image" content="nmFunctions_files/figure-html/fig-1.1-output-1.png">
<meta property="og:site_name" content="Numerical Analysis">
<meta property="og:image:alt" content="The first two polynomial approximations of the exponential function.">
<meta name="twitter:title" content="2&nbsp; Functions ‚Äì Numerical Analysis">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="nmFunctions_files/figure-html/fig-1.1-output-1.png">
<meta name="twitter:image:alt" content="The first two polynomial approximations of the exponential function.">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./nmFunctions.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Functions</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Numerical Analysis</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://vle.york.ac.uk/ultra/courses/_115112_1/outline" title="VLE" class="quarto-navigation-tool px-1" aria-label="VLE"><i class="bi bi-house-door-fill"></i></a>
    <a href="https://maths.york.ac.uk/moodle/course/view.php?id=2727" title="Moodle quizzes" class="quarto-navigation-tool px-1" aria-label="Moodle quizzes"><i class="bi bi-mortarboard-fill"></i></a>
    <a href="https://github.com/gustavdelius/NumericalAnalysis2026/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Numerical-Analysis.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nmNumbers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Numbers</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nmFunctions.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Functions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nmRoots1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Roots 1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nmRoots2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Roots 2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nmCalculus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Derivatives, Integrals</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nmOptimisation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Optimisation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nmODE1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">ODEs 1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nmODE2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">ODEs 2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nmPDE1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">PDEs 1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nmPDE2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">PDEs 2</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nmPython.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Python</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ex_exam_solns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Exam solutions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nmLinAlg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Linear Algebra</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-polynomial_approximations" id="toc-sec-polynomial_approximations" class="nav-link active" data-scroll-target="#sec-polynomial_approximations"><span class="header-section-number">2.1</span> Polynomial Approximations</a>
  <ul>
  <li><a href="#approximating-the-exponential-function" id="toc-approximating-the-exponential-function" class="nav-link" data-scroll-target="#approximating-the-exponential-function"><span class="header-section-number">2.1.1</span> Approximating the exponential function</a></li>
  <li><a href="#taylor-series" id="toc-taylor-series" class="nav-link" data-scroll-target="#taylor-series"><span class="header-section-number">2.1.2</span> Taylor Series</a></li>
  </ul></li>
  <li><a href="#sec-truncation_error" id="toc-sec-truncation_error" class="nav-link" data-scroll-target="#sec-truncation_error"><span class="header-section-number">2.2</span> Truncation Error</a></li>
  <li><a href="#exam-style-question" id="toc-exam-style-question" class="nav-link" data-scroll-target="#exam-style-question"><span class="header-section-number">2.3</span> Exam-style question</a></li>
  <li><a href="#sec-nn" id="toc-sec-nn" class="nav-link" data-scroll-target="#sec-nn"><span class="header-section-number">2.4</span> Neural Networks</a>
  <ul>
  <li><a href="#sec-single-neuron" id="toc-sec-single-neuron" class="nav-link" data-scroll-target="#sec-single-neuron"><span class="header-section-number">2.4.1</span> A Single Neuron</a></li>
  <li><a href="#sec-neural-layer" id="toc-sec-neural-layer" class="nav-link" data-scroll-target="#sec-neural-layer"><span class="header-section-number">2.4.2</span> Combining Neurons into a Layer</a></li>
  <li><a href="#sec-two-layer" id="toc-sec-two-layer" class="nav-link" data-scroll-target="#sec-two-layer"><span class="header-section-number">2.4.3</span> A Two-Layer Neural Network</a></li>
  <li><a href="#sec-deep" id="toc-sec-deep" class="nav-link" data-scroll-target="#sec-deep"><span class="header-section-number">2.4.4</span> Deep Neural Networks</a></li>
  <li><a href="#why-depth-matters-the-sawtooth-example" id="toc-why-depth-matters-the-sawtooth-example" class="nav-link" data-scroll-target="#why-depth-matters-the-sawtooth-example"><span class="header-section-number">2.4.5</span> Why Depth Matters: The Sawtooth Example</a></li>
  </ul></li>
  <li><a href="#sec-functions_problems" id="toc-sec-functions_problems" class="nav-link" data-scroll-target="#sec-functions_problems"><span class="header-section-number">2.5</span> Problems</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/gustavdelius/NumericalAnalysis2026/edit/main/nmFunctions.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-functions" class="quarto-section-identifier"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Functions</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>How does a computer <em>understand</em> a function like <span class="math inline">\(f(x) = e^x\)</span> or <span class="math inline">\(f(x) = \sin(x)\)</span> or <span class="math inline">\(f(x) = \log(x)\)</span>? What happens under the hood, so to speak, when you ask a computer to do a computation with one of these functions? A computer is good at arithmetic operations, but working with transcendental functions like these, or really any other sufficiently complicated functions for that matter, is not something that comes naturally to a computer. What is actually happening under the hood is that the computer only approximates the functions.</p>
<hr>
<section id="sec-polynomial_approximations" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="sec-polynomial_approximations"><span class="header-section-number">2.1</span> Polynomial Approximations</h2>
<p>A class of functions that computers are very good at working with are polynomial functions. This is because to evaluate a polynomial function at any point we only need to addition and multiplication operations. In this section we will explore how we can use polynomial functions to approximate other functions.</p>
<div id="exr-1.22" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.1</strong></span> üí¨ In this exercise you are going to make a bit of a wish list for all of the things that a computer will do when approximating a function. We are going to complete the following sentence:<br>
<em>If we are going to approximate</em> a smooth function <span class="math inline">\(f(x)\)</span> near the point <span class="math inline">\(x=x_0\)</span> with a simpler function <span class="math inline">\(g(x)\)</span> then ‚Ä¶</p>
<p>(I will get us started with the first two things that seems natural to wish for. The rest of the wish list is for you to complete.)</p>
<ul>
<li><p>the functions <span class="math inline">\(f(x)\)</span> and <span class="math inline">\(g(x)\)</span> should agree at <span class="math inline">\(x=x_0\)</span>. In other words, <span class="math inline">\(f(x_0) = g(x_0)\)</span></p></li>
<li><p>the function <span class="math inline">\(g(x)\)</span> should only involve addition, subtraction, multiplication, division, and integer exponents since computer are very good at those sorts of operations.</p></li>
<li><p>if <span class="math inline">\(f(x)\)</span> is increasing / decreasing near <span class="math inline">\(x=x_0\)</span> then <span class="math inline">\(g(x)\)</span> ‚Ä¶</p></li>
<li><p>if <span class="math inline">\(f(x)\)</span> is concave up / down near <span class="math inline">\(x=x_0\)</span> then <span class="math inline">\(g(x)\)</span>‚Ä¶</p></li>
<li><p>if we zoom into plots of the functions <span class="math inline">\(f(x)\)</span> and <span class="math inline">\(g(x)\)</span> near <span class="math inline">\(x=x_0\)</span> then ‚Ä¶</p></li>
<li><p>‚Ä¶ is there anything else that you would add?</p></li>
</ul>
</div>
<hr>
<div id="exr-1.23" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.2</strong></span> üí¨ Discuss: Could a polynomial function with a high enough degree satisfy everything in the wish list from the previous problem? Explain your reasoning.</p>
</div>
<hr>
<div id="exr-1.24" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.3</strong></span> üñã üéì Let us put some parts of the wish list into action. If <span class="math inline">\(f(x)\)</span> is a differentiable function at <span class="math inline">\(x=x_0\)</span> and if <span class="math inline">\(g(x) = A + B (x-x_0) + C (x-x_0)^2 + D (x-x_0)^3\)</span> then</p>
<ol type="1">
<li><p>What is the value of <span class="math inline">\(A\)</span> such that <span class="math inline">\(f(x_0) = g(x_0)\)</span>? <em>(Hint: substitute</em> <span class="math inline">\(x=x_0\)</span> into the <span class="math inline">\(g(x)\)</span> function)</p></li>
<li><p>What is the value of <span class="math inline">\(B\)</span> such that at <span class="math inline">\(x_0\)</span> <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> have the same slope? In other words, what is the value of <span class="math inline">\(B\)</span> such that <span class="math inline">\(f'(x_0) = g'(x_0)\)</span>? <em>(Hint: Start by taking the derivative of</em> <span class="math inline">\(g(x)\)</span>)</p></li>
<li><p>What is the value of <span class="math inline">\(C\)</span> such that at <span class="math inline">\(x_0\)</span> <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> have the same concavity? In other words, what is the value of <span class="math inline">\(C\)</span> such that <span class="math inline">\(f''(x_0) = g''(x_0)\)</span>?</p></li>
<li><p>What is the value of <span class="math inline">\(D\)</span> such that at <span class="math inline">\(x_0\)</span> <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> have the same third derivative? In other words, what is the value of <span class="math inline">\(D\)</span> such that <span class="math inline">\(f'''(x_0) = g'''(x_0)\)</span>?</p></li>
</ol>
</div>
<hr>
<p>In the previous 3 exercises you have built up some basic intuition for what we would want out of a mathematical operation that might build an approximation of a complicated function. What we have built is actually a way to get better and better approximations for functions out to pretty much any arbitrary accuracy that we like so long as we are near some anchor point (which we called <span class="math inline">\(x_0\)</span> in the previous exercises).</p>
<p>In the next several problems you will unpack the polynomial approximations of <span class="math inline">\(f(x) = e^x\)</span> and we will wrap the whole discussion with a little bit of formal mathematical language. Then we will examine other functions like <span class="math inline">\(\sin(x)\)</span> and <span class="math inline">\(\log(x)\)</span>. One of the points of this whole discussion is to give you a little glimpse as to what is happening behind the scenes in scientific programming languages when you do computations with these functions. A bigger point is to start getting a feel for how we might go in reverse and approximate an unknown function out of much simpler parts. This last goal is one of the big takeaways from numerical analysis: <em>we can mathematically model highly complicated functions out of fairly simple pieces.</em></p>
<hr>
<section id="approximating-the-exponential-function" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="approximating-the-exponential-function"><span class="header-section-number">2.1.1</span> Approximating the exponential function</h3>
<div id="exr-1.26" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.4</strong></span> üñã What is Euler‚Äôs number <span class="math inline">\(e\)</span>? You have been using this number often in Calculus and Differential Equations. Do you know the decimal approximation for this number? Moreover, is there a way that we could approximate something like <span class="math inline">\(\sqrt{e} = e^{0.5}\)</span> or <span class="math inline">\(e^{-1}\)</span> without actually having access to the full decimal expansion?</p>
<p>For all of the questions below let us work with the function <span class="math inline">\(f(x) = e^x\)</span>.</p>
<ol type="1">
<li><p>The function <span class="math inline">\(g_0(x) = 1\)</span> matches <span class="math inline">\(f(x) = e^x\)</span> exactly at the point <span class="math inline">\(x=0\)</span> since <span class="math inline">\(f(0) = e^0 = 1\)</span>. Furthermore if <span class="math inline">\(x\)</span> is very very close to <span class="math inline">\(0\)</span> then the functions <span class="math inline">\(f(x)\)</span> and <span class="math inline">\(g_0(x)\)</span> are really close to each other. Hence we could say that <span class="math inline">\(g_0(x) = 1\)</span> is an approximation of the function <span class="math inline">\(f(x) = e^x\)</span> for values of <span class="math inline">\(x\)</span> very very close to <span class="math inline">\(x=0\)</span>. Admittedly, though, it is probably pretty clear that this is a horrible approximation for any <span class="math inline">\(x\)</span> just a little bit away from <span class="math inline">\(x=0\)</span>.</p></li>
<li><p>Let us get a better approximation. What if we insist that our approximation <span class="math inline">\(g_1(x)\)</span> matches <span class="math inline">\(f(x) = e^x\)</span> exactly at <span class="math inline">\(x=0\)</span> and ALSO has exactly the same first derivative as <span class="math inline">\(f(x)\)</span> at <span class="math inline">\(x=0\)</span>.</p>
<ol type="1">
<li><p>What is the first derivative of <span class="math inline">\(f(x)\)</span>?</p></li>
<li><p>What is <span class="math inline">\(f'(0)\)</span>?</p></li>
<li><p>Use the point-slope form of a line to write the equation of the function <span class="math inline">\(g_1(x)\)</span> that goes through the point <span class="math inline">\((0,f(0))\)</span> and has slope <span class="math inline">\(f'(0)\)</span>. Recall from algebra that the point-slope form of a line is <span class="math inline">\(y = f(x_0) + m(x-x_0).\)</span> In this case we are taking <span class="math inline">\(x_0 = 0\)</span> so we are using the formula <span class="math inline">\(g_1(x) = f(0) + f'(0) (x-0)\)</span> to get the equation of the line.</p></li>
</ol></li>
<li><p>üíª Write Python code to build a plot like <a href="#fig-1.1" class="quarto-xref">Figure&nbsp;<span>2.1</span></a>. This plot shows <span class="math inline">\(f(x) = e^x\)</span>, our first approximation <span class="math inline">\(g_0(x) = 1\)</span> and our second approximation <span class="math inline">\(g_1(x) = 1+x\)</span>. You may want to look at <a href="nmPython.html#exm-A.46" class="quarto-xref">Example&nbsp;<span>A.43</span></a> in the Python chapter for a refresher on how to build plots containing the graphs of several functions. If you need a hint on how to plot the function <span class="math inline">\(g_0(x)\)</span> since it is a constant function, take a look at the <code>np.ones_like()</code> function in <a href="nmPython.html#exm-A.42" class="quarto-xref">Example&nbsp;<span>A.40</span></a>.</p></li>
</ol>
<div id="cell-fig-1.1" class="cell" data-execution_count="2">
<div class="cell-output cell-output-display">
<div id="fig-1.1" class="quarto-float quarto-figure quarto-figure-left anchored" alt="The first two polynomial approximations of the exponential function.">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-1.1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="nmFunctions_files/figure-html/fig-1.1-output-1.png" alt="The first two polynomial approximations of the exponential function." width="571" height="451" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-1.1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.1: The first two polynomial approximations of the exponential function.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div id="exr-1.27" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.5</strong></span> üñã Let us extend the idea from the previous problem to much better approximations of the function <span class="math inline">\(f(x) = e^x\)</span>.</p>
<ol type="1">
<li><p>Let us build a function <span class="math inline">\(g_2(x)\)</span> that matches <span class="math inline">\(f(x)\)</span> exactly at <span class="math inline">\(x=0\)</span>, has exactly the same first derivative as <span class="math inline">\(f(x)\)</span> at <span class="math inline">\(x=0\)</span>, AND has exactly the same second derivative as <span class="math inline">\(f(x)\)</span> at <span class="math inline">\(x=0\)</span>. To do this we will use a quadratic function. For a quadratic approximation of a function we just take a slight extension to the point-slope form of a line and use the equation <span class="math display">\[\begin{equation}
g_2(x) = f(x_0) + f'(x_0) (x-x_0) + \frac{f''(x_0)}{2} (x-x_0)^2.
\end{equation}\]</span> In this case we are using <span class="math inline">\(x_0 = 0\)</span> so the quadratic approximation function looks like <span class="math display">\[\begin{equation}
g_2(x) = f(0) + f'(0) x + \frac{f''(0)}{2} x^2.
\end{equation}\]</span></p>
<ol type="1">
<li><p>Find the quadratic approximation for <span class="math inline">\(f(x) = e^x\)</span>.</p></li>
<li><p>üíª Add your new function to the plot you created in the previous problem.</p></li>
</ol></li>
<li><p>Let us keep going!! Next we will do a cubic approximation. A cubic approximation takes the form <span class="math display">\[\begin{equation}
g_3(x) = f(x_0) + f'(0) (x-x_0) + \frac{f''(0)}{2}(x-x_0)^2 + \frac{f'''(0)}{3!}(x-x_0)^3
\end{equation}\]</span></p>
<ol type="1">
<li><p>Find the cubic approximation for <span class="math inline">\(f(x) = e^x\)</span>.</p></li>
<li><p>How do we know that this function matches the first, second, and third derivatives of <span class="math inline">\(f(x)\)</span> at <span class="math inline">\(x=0\)</span>? What‚Äôs the deal with the <span class="math inline">\(3!\)</span> on the cubic term?</p></li>
<li><p>üíª Add your function to the plot.</p></li>
</ol></li>
</ol>
</div>
<hr>
<div id="exr-1.29b" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.6</strong></span> üíª Write a function that takes the arguments <code>x</code> and <code>n</code> and returns the <code>n</code>th order Taylor series approximation of <span class="math inline">\(f(x) = e^x\)</span>. To remind yourself of how functions are defined in Python, you may want to look at <a href="nmPython.html#sec-python_functions" class="quarto-xref"><span>Section A.2.6</span></a> in the Python chapter. You will also need a loop, see <a href="nmPython.html#sec-control_flow" class="quarto-xref"><span>Section A.2.5</span></a>. Please start from the following skeleton and put your code where it says ‚ÄúTODO‚Äù.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> exp_approx(x, n):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Computes the nth order Taylor series approximation of e^x at x=0.</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters:</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">    x (float): The value at which to evaluate the approximation.</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">    n (int): The order of the Taylor series expansion.</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">    float: The nth order Taylor approximation of e^x.</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> n <span class="op">&lt;</span> <span class="dv">0</span>:</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"n must be at least 0"</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Start with zero-order approximation</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    approximation <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add higher-order terms</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, n <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        approximation <span class="op">+=</span> <span class="co"># </span><span class="al">TODO</span><span class="co">: calculate the i'th term of the Taylor series</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> approximation</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Make sure you perfectly understand the code. Use the AI to explain everything to you in detail. You can select parts of the code and ask the AI questions like ‚ÄúWhy do we need to use <code>range(1, n + 1)</code>?‚Äù and ‚ÄúWhat does <code>approximation +=</code> do?‚Äù</p>
</div>
<hr>
<div id="exr-1.30" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.7</strong></span> üéì üíª Use the function <code>exp_approx</code> that you have built in <a href="#exr-1.29b" class="quarto-xref">Exercise&nbsp;<span>2.6</span></a> to approximate <span class="math inline">\(\frac{1}{e} = e^{-1}\)</span>. Check the accuracy of your answer using <code>np.exp(-1)</code> in Python.</p>
</div>
<hr>
<div id="exr-1.29d" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.8</strong></span> üíª üí¨ Brainstorm within your group to see if you can make your function more efficient by writing it without using exponentiation and the factorial function, coding all multiplications and divisions explicitly. Try to minimise the number of arithmetic operations that you need to perform? Can you make it so that it only needs <span class="math inline">\(n-1\)</span> multiplications, <span class="math inline">\(n-1\)</span> divisions and <span class="math inline">\(n\)</span> additions?</p>
<p>Use the <code>%timeit</code> magic command to measure how fast your <code>exp_approx</code> function is. In a new code cell, run</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit exp_approx(<span class="fl">1.5</span>, <span class="dv">100</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This will run the function multiple times and give you an estimate of the execution time.</p>
</div>
<hr>
</section>
<section id="taylor-series" class="level3" data-number="2.1.2">
<h3 data-number="2.1.2" class="anchored" data-anchor-id="taylor-series"><span class="header-section-number">2.1.2</span> Taylor Series</h3>
<p>What we have been exploring so far in this section is the <strong>Taylor Series</strong> of a function.</p>
<div id="def-1.3" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.1 (Taylor Series)</strong></span> If <span class="math inline">\(f(x)\)</span> is an infinitely differentiable function at the point <span class="math inline">\(x_0\)</span> then <span class="math display">\[\begin{equation}
f(x) = f(x_0) + f'(x_0)(x-x_0) + \frac{f''(x_0)}{2}(x-x_0)^2 + \cdots \frac{f^{(n)}(x_0)}{n!}(x-x_0)^n + \cdots
\end{equation}\]</span> for any reasonably small interval around <span class="math inline">\(x_0\)</span>. The infinite polynomial expansion is called the <strong>Taylor Series</strong> of the function <span class="math inline">\(f(x)\)</span>. Taylor Series are named for the mathematician <a href="https://en.wikipedia.org/wiki/Brook_Taylor">Brook Taylor</a>.</p>
</div>
<hr>
<p>The Taylor Series of a function is often written with summation notation as <span class="math display">\[\begin{equation}
f(x) = \sum_{k=0}^\infty \frac{f^{(k)}(x_0)}{k!} (x-x_0)^k.
\end{equation}\]</span> Do not let the notation scare you. In a Taylor Series you are just saying: <em>give me a function that</em></p>
<ul>
<li><p><em>matches</em> <span class="math inline">\(f(x)\)</span> at <span class="math inline">\(x=x_0\)</span> exactly,</p></li>
<li><p><em>matches</em> <span class="math inline">\(f'(x)\)</span> at <span class="math inline">\(x=x_0\)</span> exactly,</p></li>
<li><p><em>matches</em> <span class="math inline">\(f''(x)\)</span> at <span class="math inline">\(x=x_0\)</span> exactly,</p></li>
<li><p><em>matches</em> <span class="math inline">\(f'''(x)\)</span> at <span class="math inline">\(x=x_0\)</span> exactly,</p></li>
<li><p>etc.</p></li>
</ul>
<p>(Take a moment and make sure that the summation notation makes sense to you.)</p>
<p>Moreover, Taylor Series are built out of the easiest types of functions: polynomials. Computers are rather good at doing computations with addition, subtraction, multiplication, division, and integer exponents, so Taylor Series are a natural way to express functions in a computer. The down side is that we can only get true equality in the Taylor Series if we have infinitely many terms in the series. A computer cannot do infinitely many computations. So, in practice, we <strong>truncate</strong> Taylor Series after many terms and think of the new polynomial function as being <em>close enough</em> to the actual function so far as we do not stray too far from the anchor <span class="math inline">\(x_0\)</span>.</p>
<hr>
<div id="exr-1.32" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.9</strong></span> üñã Do all of the calculations to show that the Taylor Series centred at <span class="math inline">\(x_0 = 0\)</span> for the function <span class="math inline">\(f(x) = \sin(x)\)</span> is indeed <span class="math display">\[\begin{equation}
\sin(x) = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + \cdots .
\end{equation}\]</span></p>
</div>
<hr>
<div id="exr-1.33" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.10</strong></span> üíª Write a Pyton function <code>sin_approx(x, n)</code> that computes the <span class="math inline">\(n\)</span>th order Taylor Series approximation of <span class="math inline">\(\sin(x)\)</span> centred at <span class="math inline">\(x_0 = 0\)</span>. Test your function by comparing its output to <code>np.sin(x)</code> for a few values of <span class="math inline">\(x\)</span> and <span class="math inline">\(n\)</span>. Use it to make a plot of the first three approximations for <span class="math inline">\(x\)</span> in the range <span class="math inline">\([-\pi, \pi]\)</span> similar to the plots you made for approximations of <span class="math inline">\(e^x\)</span> above.</p>
</div>
<hr>
<div id="exr-1.34" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.11</strong></span> üñã Let us compute a Taylor Series that is not centred at <span class="math inline">\(x_0 = 0\)</span>. For example, let us approximate the function <span class="math inline">\(f(x) = \log (x)\)</span> near <span class="math inline">\(x_0 = 1\)</span>. Near the point <span class="math inline">\(x_0 = 1\)</span>, the Taylor Series approximation will take the form <span class="math display">\[\begin{equation}
f(x) = f\left( 1 \right) + f'\left( 1 \right)\left( x - 1 \right) + \frac{f''\left( 1 \right)}{2!}\left( x - 1 \right)^2 + \frac{f'''\left( 1 \right)}{3!}\left( x - 1 \right)^3 + \cdots
\end{equation}\]</span></p>
<p>Write the first several terms of the Taylor Series for <span class="math inline">\(f(x) = \log x\)</span> centred at <span class="math inline">\(x_0 = 1\)</span> until you get a feel for the pattern.</p>
</div>
<hr>
<div id="exr-1.34b" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.12</strong></span> üíª Write a Pyton function <code>log_approx(x, n)</code> that computes the <span class="math inline">\(n\)</span>th order Taylor Series approximation of <span class="math inline">\(\log(x)\)</span> centred at <span class="math inline">\(x_0 = 1\)</span>. Use it to build the plot below showing the approximations.</p>
<div id="cell-fig-1.2" class="cell" data-execution_count="3">
<div class="cell-output cell-output-display">
<div id="fig-1.2" class="quarto-float quarto-figure quarto-figure-left anchored" alt="Taylor series approximation of the logarithm.">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-1.2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="nmFunctions_files/figure-html/fig-1.2-output-1.png" alt="Taylor series approximation of the logarithm." width="582" height="450" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-1.2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.2: Taylor series approximation of the logarithm.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<hr>
<div id="exm-1.4" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.1</strong></span> Let us conclude this brief section by examining an interesting example. Consider the function <span class="math display">\[\begin{equation}
f(x) = \frac{1}{1-x}.
\end{equation}\]</span> If we build a Taylor Series centred at <span class="math inline">\(x_0 = 0\)</span> it is not too hard to show that we get <span class="math display">\[\begin{equation}
f(x) = 1 + x + x^2 + x^3 + x^4 + x^5 + \cdots
\end{equation}\]</span> (you should stop now and verify this!). However, if we plot the function <span class="math inline">\(f(x)\)</span> along with several successive approximations for <span class="math inline">\(f(x)\)</span> we find that beyond <span class="math inline">\(x=1\)</span> we do not get the correct behaviour of the function (see <a href="#fig-1.3" class="quarto-xref">Figure&nbsp;<span>2.3</span></a>). More specifically, we cannot get the Taylor Series to change behaviour across the vertical asymptote of the function at <span class="math inline">\(x=1\)</span>. This example is meant to point out the fact that a Taylor Series will only ever make sense <em>near</em> the point at which you centre the expansion. For the function <span class="math inline">\(f(x) = \frac{1}{1-x}\)</span> centred at <span class="math inline">\(x_0 = 0\)</span> we can only get good approximations within the interval <span class="math inline">\(x \in (-1,1)\)</span> and no further.</p>
<div id="cell-fig-1.3" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># build the x and y values</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">101</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>y0 <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>x)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>y1 <span class="op">=</span> <span class="dv">1</span> <span class="op">+</span> <span class="dv">0</span><span class="op">*</span>x</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>y2 <span class="op">=</span> <span class="dv">1</span> <span class="op">+</span> x</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>y3 <span class="op">=</span> <span class="dv">1</span> <span class="op">+</span> x <span class="op">+</span> x<span class="op">**</span><span class="dv">2</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>y4 <span class="op">=</span> <span class="dv">1</span> <span class="op">+</span> x <span class="op">+</span> x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> x<span class="op">**</span><span class="dv">3</span> <span class="op">+</span> x<span class="op">**</span><span class="dv">4</span> <span class="op">+</span> x<span class="op">**</span><span class="dv">5</span> <span class="op">+</span> x<span class="op">**</span><span class="dv">6</span> <span class="op">+</span> x<span class="op">**</span><span class="dv">7</span> <span class="op">+</span> x<span class="op">**</span><span class="dv">8</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co"># plot each of the functions </span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y0, <span class="st">'r-'</span>, label<span class="op">=</span><span class="vs">r"</span><span class="dv">$</span><span class="vs">f</span><span class="kw">(</span><span class="vs">x</span><span class="kw">)</span><span class="vs">=1/</span><span class="kw">(</span><span class="vs">1-x</span><span class="kw">)</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y1, <span class="st">'c-'</span>, label<span class="op">=</span><span class="vs">r"constant"</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y2, <span class="st">'g:'</span>, label<span class="op">=</span><span class="vs">r"linear"</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y3, <span class="st">'b-.'</span>, label<span class="op">=</span><span class="vs">r"quadratic"</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y4, <span class="st">'k--'</span>, label<span class="op">=</span><span class="vs">r"8th order"</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="co"># set limits on the y axis</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="op">-</span><span class="dv">3</span>,<span class="dv">5</span>)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="co"># put in a grid, legend, title, and axis labels</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="vs">r"Taylor approximations of </span><span class="dv">$</span><span class="vs">f</span><span class="kw">(</span><span class="vs">x</span><span class="kw">)</span><span class="vs">=</span><span class="ch">\f</span><span class="vs">rac</span><span class="op">{1}</span><span class="vs">{1-x}</span><span class="dv">$</span><span class="vs"> around </span><span class="dv">$</span><span class="vs">x=0</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-1.3" class="quarto-float quarto-figure quarto-figure-left anchored" alt="Several Taylor Series approximations of the function $f(x) = 1/(1-x)$.">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-1.3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="nmFunctions_files/figure-html/fig-1.3-output-1.png" alt="Several Taylor Series approximations of the function $f(x) = 1/(1-x)$." width="569" height="435" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-1.3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.3: Several Taylor Series approximations of the function <span class="math inline">\(f(x) = 1/(1-x)\)</span>.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<hr>
<p>In the previous example we saw that we cannot always get approximations from Taylor Series that are good everywhere. For every Taylor Series there is a <strong>domain of convergence</strong> where the Taylor Series actually makes sense and gives good approximations. It is beyond the scope of this section to give all of the details for finding the domain of convergence for a Taylor Series. You have done that in your first-year Calculus module. However a good heuristic is to observe that a Taylor Series will only give reasonable approximations of a function from the centre of the series to the nearest asymptote. The domain of convergence is typically symmetric about the centre as well. For example:</p>
<ul>
<li><p>If we were to build a Taylor Series approximation for the function <span class="math inline">\(f(x) = \log(x)\)</span> centred at the point <span class="math inline">\(x_0 = 1\)</span> then the domain of convergence should be <span class="math inline">\(x \in (0,2)\)</span> since there is a vertical asymptote for the natural logarithm function at <span class="math inline">\(x=0\)</span>.</p></li>
<li><p>If we were to build a Taylor Series approximation for the function <span class="math inline">\(f(x) = \frac{5}{2x-3}\)</span> centred at the point <span class="math inline">\(x_0 = 4\)</span> then the domain of convergence should be <span class="math inline">\(x \in (1.5, 6.5)\)</span> since there is a vertical asymptote at <span class="math inline">\(x=1.5\)</span> and the distance from <span class="math inline">\(x_0 = 4\)</span> to <span class="math inline">\(x=1.5\)</span> is 2.5 units.</p></li>
<li><p>If we were to build a Taylor Series approximation for the function <span class="math inline">\(f(x) = \frac{1}{1+x^2}\)</span> centred at the point <span class="math inline">\(x_0 = 0\)</span> then the domain of convergence should be <span class="math inline">\(x \in (-1,1)\)</span>. This may seem quite odd (and perhaps quite surprising!) but let us think about where the nearest asymptote might be. To find the asymptote we need to solve <span class="math inline">\(1+x^2 = 0\)</span> but this gives us the values <span class="math inline">\(x = \pm i\)</span>. In the complex plane, the numbers <span class="math inline">\(i\)</span> and <span class="math inline">\(-i\)</span> are 1 unit away from <span class="math inline">\(x_0 = 0\)</span>, so the ‚Äúasymptote‚Äù is not visible in a real-valued plot but it is still only one unit away. Hence the domain of convergence is <span class="math inline">\(x \in (-1,1)\)</span>. You may want to pause now and build some plots to show yourself that this indeed appears to be true.</p></li>
</ul>
<p>Of course you learned all this and more in your first-year Calculus but I hope it was fun to now rediscover these things yourself. In your Calculus module it was probably not stressed how fundamental Taylor series are to doing numerical computations.</p>
</section>
</section>
<section id="sec-truncation_error" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="sec-truncation_error"><span class="header-section-number">2.2</span> Truncation Error</h2>
<p>The great thing about Taylor Series is that they allow for the representation of potentially very complicated functions as polynomials ‚Äì and polynomials are easily dealt with on a computer since they involve only addition, subtraction, multiplication, division, and integer powers. The down side is that the order of the polynomial is infinite. Hence, every time we use a Taylor series on a computer, what we are actually going to be using is a <strong>Truncated Taylor Series</strong> where we only take a finite number of terms. The idea here is simple in principle:</p>
<ul>
<li><p>If a function <span class="math inline">\(f(x)\)</span> has a Taylor Series representation it can be written as an infinite sum.</p></li>
<li><p>Computers cannot do infinite sums.</p></li>
<li><p>So stop the sum at some point <span class="math inline">\(n\)</span> and throw away the rest of the infinite sum.</p></li>
<li><p>Now <span class="math inline">\(f(x)\)</span> is approximated by some finite sum so long as you stay pretty close to <span class="math inline">\(x = x_0\)</span>,</p></li>
<li><p>and everything that we just chopped off of the end is called the <strong>remainder</strong> for the finite sum.</p></li>
</ul>
<p>Let us be a bit more concrete about it. The Taylor Series for <span class="math inline">\(f(x) = e^x\)</span> centred at <span class="math inline">\(x_0 = 0\)</span> is <span class="math display">\[\begin{equation}
e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \frac{x^4}{4!} + \cdots.
\end{equation}\]</span></p>
<p>When we truncate this series at order <span class="math inline">\(n\)</span>, we separate it into an approximation and a remainder: <span class="math display">\[\begin{equation}
e^x = \underbrace{1 + x + \frac{x^2}{2!} + \cdots + \frac{x^n}{n!}}_{\text{$n^{th}$ order approximation}} + \underbrace{\frac{x^{n+1}}{(n+1)!} + \frac{x^{n+2}}{(n+2)!} + \cdots}_{\text{remainder}}.
\end{equation}\]</span></p>
<p>For small values of <span class="math inline">\(x\)</span> near <span class="math inline">\(x_0 = 0\)</span>, the largest term in the remainder is <span class="math inline">\(\frac{x^{n+1}}{(n+1)!}\)</span>, since higher powers of <span class="math inline">\(x\)</span> become progressively smaller. We use <strong>Big-O notation</strong> to express this: <span class="math display">\[\begin{equation}
e^x \approx 1 + x + \frac{x^2}{2!} + \cdots + \frac{x^n}{n!} + \mathcal{O}(x^{n+1}),
\end{equation}\]</span> where the notation <span class="math inline">\(\mathcal{O}(x^{n+1})\)</span> (read ‚ÄúBig-O of <span class="math inline">\(x^{n+1}\)</span>‚Äù) signifies that the error is bounded by <span class="math inline">\(C|x|^{n+1}\)</span> for some constant <span class="math inline">\(C\)</span> as <span class="math inline">\(x \to 0\)</span>. This indicates that the error scales like <span class="math inline">\(x^{n+1}\)</span> for values of <span class="math inline">\(x\)</span> near the center <span class="math inline">\(x_0=0\)</span>.</p>
<p>For example:</p>
<ul>
<li><span class="math inline">\(0^{th}\)</span> order: <span class="math inline">\(e^x \approx 1 + \mathcal{O}(x)\)</span></li>
<li><span class="math inline">\(1^{st}\)</span> order: <span class="math inline">\(e^x \approx 1 + x + \mathcal{O}(x^2)\)</span></li>
<li><span class="math inline">\(2^{nd}\)</span> order: <span class="math inline">\(e^x \approx 1 + x + \frac{x^2}{2} + \mathcal{O}(x^3)\)</span></li>
</ul>
<p>Keep in mind that this sort of analysis is only good for values of <span class="math inline">\(x\)</span> that are very close to the centre of the Taylor Series. If you are making approximations that are too far away then all bets are off.</p>
<hr>
<div id="exr-1.38" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.13</strong></span> üéì üíª Now make the previous discussion a bit more concrete. You know the Taylor Series for <span class="math inline">\(f(x) = e^x\)</span> around <span class="math inline">\(x=0\)</span> quite well at this point so use it to approximate the values of <span class="math inline">\(f(0.1) = e^{0.1}\)</span> and <span class="math inline">\(f(0.2)=e^{0.2}\)</span> by truncating the Taylor series at different orders. Because <span class="math inline">\(x=0.1\)</span> and <span class="math inline">\(x=0.2\)</span> are pretty close to the centre of the Taylor Series <span class="math inline">\(x_0 = 0\)</span>, this sort of approximation is reasonable.</p>
<p>Then compare your approximate values to Python‚Äôs values <span class="math inline">\(f(0.1)=e^{0.1} \approx\)</span> <code>np.exp(0.1)</code> <span class="math inline">\(=1.1051709180756477\)</span> and <span class="math inline">\(f(0.2)=e^{0.2} \approx\)</span> <code>np.exp(0.2)</code> <span class="math inline">\(=1.2214027581601699\)</span> to calculate the truncation errors <span class="math inline">\(\epsilon_n(0.1)=|f(0.1)-f_n(0.1)|\)</span> and <span class="math inline">\(\epsilon_n(0.2)=|f(0.2)-f_n(0.2)|\)</span>.</p>
<p>Fill in the blanks in the table. If you like, you can copy and paste the code and extend it to fill in the missing rows. For a bit of explanation of the syntax of the print commands see <a href="nmPython.html#exm-A.20" class="quarto-xref">Example&nbsp;<span>A.20</span></a> but for more detailed information ask Gemini.</p>
<div id="98afa816" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create table header</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f"</span><span class="sc">{</span><span class="st">'Order n'</span><span class="sc">:&lt;8}</span><span class="ss"> | </span><span class="sc">{</span><span class="st">'f_n(0.1)'</span><span class="sc">:&lt;15}</span><span class="ss"> | </span><span class="sc">{</span><span class="st">'Œµ_n(0.1)'</span><span class="sc">:&lt;15}</span><span class="ss"> | "</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f"</span><span class="sc">{</span><span class="st">'f_n(0.2)'</span><span class="sc">:&lt;15}</span><span class="ss"> | </span><span class="sc">{</span><span class="st">'Œµ_n(0.2)'</span><span class="sc">:&lt;15}</span><span class="ss"> "</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">80</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Fill in n=0 row</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>f_n <span class="op">=</span> <span class="kw">lambda</span> x: <span class="dv">1</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>e_n <span class="op">=</span> <span class="kw">lambda</span> x: <span class="bu">abs</span>(np.exp(x) <span class="op">-</span> f_n(x))</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f"</span><span class="sc">{</span><span class="dv">0</span><span class="sc">:&lt;8}</span><span class="ss"> | </span><span class="sc">{</span>f_n(<span class="fl">0.1</span>)<span class="sc">:&lt;15.10g}</span><span class="ss"> | </span><span class="sc">{</span>e_n(<span class="fl">0.1</span>)<span class="sc">:&lt;15.10g}</span><span class="ss"> | "</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f"</span><span class="sc">{</span>f_n(<span class="fl">0.2</span>)<span class="sc">:&lt;15.10g}</span><span class="ss"> | </span><span class="sc">{</span>e_n(<span class="fl">0.2</span>)<span class="sc">:&lt;15.10g}</span><span class="ss"> "</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Fill in n=1 row</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>f_n <span class="op">=</span> <span class="kw">lambda</span> x: <span class="dv">1</span> <span class="op">+</span> x</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>e_n <span class="op">=</span> <span class="kw">lambda</span> x: <span class="bu">abs</span>(np.exp(x) <span class="op">-</span> f_n(x))</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f"</span><span class="sc">{</span><span class="dv">1</span><span class="sc">:&lt;8}</span><span class="ss"> | </span><span class="sc">{</span>f_n(<span class="fl">0.1</span>)<span class="sc">:&lt;15.10g}</span><span class="ss"> | </span><span class="sc">{</span>e_n(<span class="fl">0.1</span>)<span class="sc">:&lt;15.10g}</span><span class="ss"> | "</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f"</span><span class="sc">{</span>f_n(<span class="fl">0.2</span>)<span class="sc">:&lt;15.10g}</span><span class="ss"> | </span><span class="sc">{</span>e_n(<span class="fl">0.2</span>)<span class="sc">:&lt;15.10g}</span><span class="ss"> "</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Fill in more rows.</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>,<span class="dv">6</span>):</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co">. You can use your `exp_approx()` function here.</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>n<span class="sc">:&lt;8}</span><span class="ss"> | </span><span class="sc">{</span><span class="st">''</span><span class="sc">:&lt;15}</span><span class="ss"> | </span><span class="sc">{</span><span class="st">''</span><span class="sc">:&lt;15}</span><span class="ss"> | </span><span class="sc">{</span><span class="st">''</span><span class="sc">:&lt;15}</span><span class="ss"> | </span><span class="sc">{</span><span class="st">''</span><span class="sc">:&lt;15}</span><span class="ss"> "</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Order n  | f_n(0.1)        | Œµ_n(0.1)        | f_n(0.2)        | Œµ_n(0.2)        
--------------------------------------------------------------------------------
0        | 1               | 0.1051709181    | 1               | 0.2214027582    
1        | 1.1             | 0.005170918076  | 1.2             | 0.02140275816   
2        |                 |                 |                 |                 
3        |                 |                 |                 |                 
4        |                 |                 |                 |                 
5        |                 |                 |                 |                 </code></pre>
</div>
</div>
<p>You will find that, as expected, the truncation errors <span class="math inline">\(\epsilon_n(x)\)</span> decrease with <span class="math inline">\(n\)</span> but increase with <span class="math inline">\(x\)</span>.</p>
</div>
<hr>
<div id="exr-1.39b" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.14</strong></span> üíª To investigate the dependence of the truncation error <span class="math inline">\(\epsilon_n(x)\)</span> on <span class="math inline">\(n\)</span> and <span class="math inline">\(x\)</span> a bit more, add an extra column to the table from the previous exercise with the ratio <span class="math inline">\(\epsilon_n(0.2) / \epsilon_n(0.1)\)</span>.</p>
<div id="f9c7e24c" class="cell" data-execution_count="6">
<div class="cell-output cell-output-stdout">
<pre><code>Order n  | Œµ_n(0.1)        | Œµ_n(0.2)        |  Œµ_n(0.2) / Œµ_n(0.1)
--------------------------------------------------------------------------------
0        | 0.1051709181    | 0.2214027582    | 2.105170918     
1        | 0.005170918076  | 0.02140275816   | 4.139063479     
2        |                 |                 |                
3        |                 |                 |                
4        |                 |                 |                
5        |                 |                 |                </code></pre>
</div>
</div>
<p>üí¨ Formulate a conjecture about how <span class="math inline">\(\epsilon_n\)</span> changes as <span class="math inline">\(x\)</span> changes.</p>
</div>
<hr>
<div id="exr-1.39" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.15</strong></span> üíª To test your conjecture, examine the truncation error for the sine function near <span class="math inline">\(x_0 = 0\)</span>. You know that the sine function has the Taylor Series centred at <span class="math inline">\(x_0 = 0\)</span> as <span class="math display">\[\begin{equation}
f(x) = \sin(x) = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + \cdots.
\end{equation}\]</span> So there are only approximations of odd order. Use the truncated Taylor series to approximate <span class="math inline">\(f(0.1)=\sin(0.1)\)</span> and <span class="math inline">\(f(0.2)=\sin(0.2)\)</span> and use Python‚Äôs values <code>np.sin(0.1)</code> and <code>np.sin(0.2)</code> to calculate the truncation errors <span class="math inline">\(\epsilon_n(0.1)=|f(0.1)-f_n(0.1)|\)</span> and <span class="math inline">\(\epsilon_n(0.2)=|f(0.2)-f_n(0.2)|\)</span>.</p>
<p>Complete the following table:</p>
<div id="2100e2ac" class="cell" data-execution_count="7">
<div class="cell-output cell-output-stdout">
<pre><code>Order n  | Œµ_n(0.1)        | Œµ_n(0.2)        |  Œµ_n(0.2) / Œµ_n(0.1)      
--------------------------------------------------------------------------------
1        | 0.0001665833532 | 0.001330669205  | 7.988008283               
3        |                 |                 |                          
5        |                 |                 |                          
7        |                 |                 |                          
9        |                 |                 |                          </code></pre>
</div>
</div>
<p>To learn how you can loop over only odd integers in Python, see <a href="nmPython.html#exm-A.7" class="quarto-xref">Example&nbsp;<span>A.7</span></a>.</p>
<p>üí¨ Did these results force you to revise your conjecture of how <span class="math inline">\(\epsilon_n\)</span> changes as <span class="math inline">\(x\)</span> changes?</p>
<p>The entry in the last row of the table will almost certainly not agree with your conjecture. That is okay! That discrepancy has a different explanation. Can you figure out what it is? Hint: Think about the discussion of machine precision in <a href="nmNumbers.html" class="quarto-xref"><span>Chapter 1</span></a>.</p>
</div>
<hr>
<div id="exr-1.40" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.16</strong></span> üéì üíª Perform another check of your conjecture by approximating <span class="math inline">\(\log(1.02)\)</span> and <span class="math inline">\(\log(1.1)\)</span> from truncations of the Taylor series around <span class="math inline">\(x=1\)</span>: <span class="math display">\[
\log(1+x) = x - \frac{x^2}{2} + \frac{x^3}{3} - \frac{x^4}{4} + \frac{x^5}{5} - \cdots.
\]</span></p>
</div>
<hr>
<div id="exr-1.40" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.17</strong></span> üí¨ üñã Write down your groups‚Äôs observations about how the truncation error changes as <span class="math inline">\(x\)</span> changes. Explain this in terms of the form of the remainder of the truncated Taylor series.</p>
</div>
</section>
<section id="exam-style-question" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="exam-style-question"><span class="header-section-number">2.3</span> Exam-style question</h2>
<p>You know the Taylor Series expansion for <span class="math inline">\(f(x) = \sin(x)\)</span> centred at <span class="math inline">\(x_0 = 0\)</span>: <span class="math display">\[\sin(x) = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + \cdots.\]</span></p>
<ol type="a">
<li><p>Explain why there is no <span class="math inline">\(x^2\)</span> term in this Taylor Series expansion. [2 marks]</p></li>
<li><p>Complete the Python code to calculate the Taylor expansion for <span class="math inline">\(f(x) = \sin(x)\)</span> centred at <span class="math inline">\(x_0 = 0\)</span> up to order <span class="math inline">\(n\)</span>. [4 marks]</p></li>
</ol>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sin_taylor(x, n):</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Calculate the Taylor expansion for f(x) = sin(x) centred at x_0 = 0 up to order n.</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    ....</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(...):</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>        ....</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> result</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ol start="3" type="a">
<li><ol type="i">
<li><p>If we only keep terms up to <span class="math inline">\(x^3\)</span> in this Taylor Series expansion, what is the largest integer <span class="math inline">\(n\)</span> so that the truncation error is <span class="math inline">\(O(x^n)\)</span>? [2 marks]</p></li>
<li><p>If we use the same approximation to calculate <span class="math inline">\(\sin(0.05)\)</span> instead of <span class="math inline">\(\sin(0.1)\)</span>, by what factor do we expect the truncation error to decrease? [2 marks]</p></li>
</ol></li>
</ol>
<hr>
</section>
<section id="sec-nn" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="sec-nn"><span class="header-section-number">2.4</span> Neural Networks</h2>
<p>So far we have discussed approximating functions with polynomials, and in particular with Taylor Series. However there are other families of functions that can be used to approximate an arbitrary function. One of them you have already met in your first year: Fourier series. Another one that is usually discussed in a course on Numerical Analysis is splines. In this module we will not discuss these families of functions. Our aim in this module is not to be exhaustive, but to get the fundamental ideas across, so that you will be well equipped to acquire further knowledge on the topic later on.</p>
<p>There is however a family of functions that has recently become very popular in machine learning: <strong>neural networks</strong>. This section guides you through exercises to obtain a good intuitive understanding of neural networks.</p>
<p>It may well be that by the time you reach this point in the learning guide, you will not have much time left this week. For that reason the material on neural networks is not examinable. You are under no pressure to work through this material. However, if you do have the time and interest, I would encourage you to do so.</p>
<p>Similar to how a polynomial <span class="math inline">\(p(x)\)</span> is determined by giving the coefficients in front of the powers of <span class="math inline">\(x\)</span>, a neural network is determined by giving a set of parameters, called weights and biases. In this section you will explore how the weights and biases determine the neural network function.</p>
<hr>
<section id="sec-single-neuron" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="sec-single-neuron"><span class="header-section-number">2.4.1</span> A Single Neuron</h3>
<p>Let us start by building up the components of a neural network one piece at a time. The fundamental building block is called a <strong>neuron</strong>.</p>
<div id="exr-1.41" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.18</strong></span> üñã Consider the simple function <span class="math display">\[\begin{equation}
f(x) = \max(0, x).
\end{equation}\]</span> The function <span class="math inline">\(g(x) = \max(0, x)\)</span> is called the <strong>Rectified Linear Unit</strong> or <strong>ReLU</strong> for short.</p>
<ol type="1">
<li><p>By hand, sketch the graph of this function for <span class="math inline">\(x \in [-3, 3]\)</span>.</p></li>
<li><p>What is the derivative of <span class="math inline">\(f(x)\)</span> for <span class="math inline">\(x &gt; 0\)</span>? What about for <span class="math inline">\(x &lt; 0\)</span>? What happens at <span class="math inline">\(x = 0\)</span>?</p></li>
</ol>
</div>
<hr>
<p>The ReLU function is an example of what is called an <strong>activation function</strong> in neural networks. The idea is that the neuron ‚Äúactivates‚Äù (produces a non-zero output) only when the input exceeds a certain threshold.</p>
<div id="exr-1.42" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.19</strong></span> üñã Now let us make our neuron a bit more interesting by allowing it to have adjustable parameters.</p>
<p>Consider the function <span class="math display">\[\begin{equation}
h(x) = \max(0, w x + b)
\end{equation}\]</span> where <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span> are parameters that we can choose. These are called the <strong>weight</strong> and <strong>bias</strong> respectively.</p>
<ol type="1">
<li><p>By hand, sketch the graph of <span class="math inline">\(h(x)\)</span> for <span class="math inline">\(x \in [-3, 3]\)</span> for each of the following parameter values:</p>
<ol type="i">
<li><span class="math inline">\(w = 1, b = 0\)</span></li>
<li><span class="math inline">\(w = 2, b = 0\)</span></li>
<li><span class="math inline">\(w = 1, b = 1\)</span></li>
<li><span class="math inline">\(w = 1, b = -1\)</span></li>
<li><span class="math inline">\(w = -1, b = 0\)</span></li>
</ol></li>
<li><p>Describe in words what the weight <span class="math inline">\(w\)</span> controls about the function <span class="math inline">\(h(x)\)</span>.</p></li>
<li><p>Describe in words what the bias <span class="math inline">\(b\)</span> controls about the function <span class="math inline">\(h(x)\)</span>.</p></li>
<li><p>For which values of <span class="math inline">\(x\)</span> is the function ‚Äúactive‚Äù (i.e., non-zero) when <span class="math inline">\(w = 1\)</span> and <span class="math inline">\(b = -1\)</span>?</p></li>
</ol>
</div>
<hr>
<div id="exr-1.43" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.20</strong></span> üñã So far we have been working with a neuron that takes a single input <span class="math inline">\(x\)</span>. In many applications, we want to work with functions of multiple variables.</p>
<p>Suppose we have two inputs <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, and we define a neuron as <span class="math display">\[\begin{equation}
h(x_1, x_2) = \max(0, w_1 x_1 + w_2 x_2 + b)
\end{equation}\]</span> where <span class="math inline">\(w_1, w_2\)</span> are weights and <span class="math inline">\(b\)</span> is a bias.</p>
<ol type="1">
<li><p>What is the output of this neuron when <span class="math inline">\(x_1 = 1, x_2 = 2\)</span>, using the parameters <span class="math inline">\(w_1 = 1, w_2 = -1, b = 0\)</span>?</p></li>
<li><p>The expression <span class="math inline">\(w_1 x_1 + w_2 x_2 + b = 0\)</span> defines a line in the <span class="math inline">\((x_1, x_2)\)</span> plane. For the parameters in part 1, sketch this line in the region <span class="math inline">\(x_1 \in [-2, 2], x_2 \in [-2, 2]\)</span>.</p></li>
<li><p>On which side of this line is the neuron ‚Äúactive‚Äù (produces non-zero outputs)? Shade that area in your sketch from part 2.</p></li>
</ol>
</div>
<hr>
</section>
<section id="sec-neural-layer" class="level3" data-number="2.4.2">
<h3 data-number="2.4.2" class="anchored" data-anchor-id="sec-neural-layer"><span class="header-section-number">2.4.2</span> Combining Neurons into a Layer</h3>
<p>A single neuron can detect when the input crosses a particular threshold. But to approximate more complex functions, we need to combine multiple neurons together. Let us explore this idea.</p>
<div id="exr-1.44" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.21</strong></span> üñã Suppose we have two neurons, both taking the same input <span class="math inline">\(x\)</span>, but with different weights and biases: <span class="math display">\[\begin{align}
h_1(x) &amp;= \max(0, w_1 x + b_1) \\
h_2(x) &amp;= \max(0, w_2 x + b_2)
\end{align}\]</span></p>
<p>Consider the specific case where <span class="math inline">\(w_1 = 1, b_1 = -1\)</span> and <span class="math inline">\(w_2 = -1, b_2 = 1\)</span>.</p>
<ol type="1">
<li><p>For what values of <span class="math inline">\(x\)</span> is <span class="math inline">\(h_1(x)\)</span> active (non-zero)?</p></li>
<li><p>For what values of <span class="math inline">\(x\)</span> is <span class="math inline">\(h_2(x)\)</span> active (non-zero)?</p></li>
<li><p>By hand, sketch the graphs of both <span class="math inline">\(h_1(x)\)</span> and <span class="math inline">\(h_2(x)\)</span> on the same axes for <span class="math inline">\(x \in [-2, 2]\)</span>.</p></li>
<li><p>Now consider the sum <span class="math inline">\(h_1(x) + h_2(x)\)</span>. By hand, sketch the graph of this function. Describe its shape.</p></li>
<li><p>What is the minimum value of <span class="math inline">\(h_1(x) + h_2(x)\)</span>? At what value of <span class="math inline">\(x\)</span> does this minimum occur?</p></li>
</ol>
</div>
<hr>
<div id="exr-1.45" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.22</strong></span> üñã Rather than just adding two ReLU neurons, consider the weighted combination: <span class="math display">\[\begin{equation}
f(x) = c_1 h_1(x) + c_2 h_2(x)
\end{equation}\]</span> where <span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span> are <strong>output weights</strong> that can be positive or negative.</p>
<p>Let <span class="math inline">\(h_1(x) = \max(0, x)\)</span> and <span class="math inline">\(h_2(x) = \max(0, x - 1)\)</span>.</p>
<ul>
<li>What is <span class="math inline">\(f(x) = h_1(x) - 2h_2(x)\)</span> for <span class="math inline">\(x &lt; 0\)</span>?</li>
<li>What is <span class="math inline">\(f(x) = h_1(x) - 2h_2(x)\)</span> for <span class="math inline">\(0 \leq x &lt; 1\)</span>?</li>
<li>What is <span class="math inline">\(f(x) = h_1(x) - 2h_2(x)\)</span> for <span class="math inline">\(x \geq 1\)</span>?</li>
</ul>
<p>By hand, sketch the graph of this function.</p>
</div>
<hr>
<div id="exr-1.45" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.23</strong></span> üñã In the previous exercise you created a function with a triangular bump but the function continued decreasing to negative values beyond <span class="math inline">\(x=2\)</span>. Now see if you can find a way to combine three ReLU neurons to create a triangular bump function that:</p>
<ul>
<li>is zero for <span class="math inline">\(x &lt; 0\)</span> and <span class="math inline">\(x &gt; 2\)</span></li>
<li>equals <span class="math inline">\(x\)</span> for <span class="math inline">\(0 \leq x \leq 1\)</span></li>
<li>equals <span class="math inline">\(2-x\)</span> for <span class="math inline">\(1 &lt; x \leq 2\)</span></li>
</ul>
<ol type="1">
<li><p>Write down the weights <span class="math inline">\(w_1, b_1, w_2, b_2, w_3, b_3\)</span> and output weights <span class="math inline">\(c_1, c_2\)</span> and <span class="math inline">\(c_3\)</span> that produce the desired function. Hint: You need <span class="math inline">\(h_3\)</span> to ‚Äúcancel out‚Äù the negative values after <span class="math inline">\(x=2\)</span>.</p></li>
<li><p>Sketch the graph of your function to verify that it works.</p></li>
<li><p>How could you modify the output weights to make the bump twice as tall?</p></li>
<li><p>How could you modify the weights and biases to shift the bump so that it is centred at <span class="math inline">\(x = 5\)</span> instead of <span class="math inline">\(x = 1\)</span>?</p></li>
</ol>
</div>
<hr>
</section>
<section id="sec-two-layer" class="level3" data-number="2.4.3">
<h3 data-number="2.4.3" class="anchored" data-anchor-id="sec-two-layer"><span class="header-section-number">2.4.3</span> A Two-Layer Neural Network</h3>
<p>In the previous exercises, you discovered that by combining multiple neurons (each with a ReLU activation), we can build flexible function approximations. Let us now formalise this into what is called a <strong>two-layer neural network</strong> or <strong>single hidden layer network</strong>.</p>
<p>The structure is as follows:</p>
<ol type="1">
<li><p><strong>Input</strong>: We start with an input value <span class="math inline">\(x\)</span>.</p></li>
<li><p><strong>Hidden Layer</strong>: We apply <span class="math inline">\(n\)</span> neurons to the input, creating <span class="math inline">\(n\)</span> hidden values: <span class="math display">\[\begin{equation}
h_i(x) = \max(0, w_i x + b_i) \quad \text{for } i = 1, 2, \ldots, n
\end{equation}\]</span> The parameters <span class="math inline">\(w_i\)</span> and <span class="math inline">\(b_i\)</span> are called the <strong>weights</strong> and <strong>biases</strong> of the hidden layer.</p></li>
<li><p><strong>Output Layer</strong>: We combine the hidden values using output weights: <span class="math display">\[\begin{equation}
f(x) = c_1 h_1(x) + c_2 h_2(x) + \cdots + c_n h_n(x) + c_0
\end{equation}\]</span> where <span class="math inline">\(c_0, c_1, \ldots, c_n\)</span> are the output layer parameters.</p></li>
</ol>
<p>We can write this more compactly using summation notation: <span class="math display">\[\begin{equation}
f(x) = \sum_{i=1}^{n} c_i \max(0, w_i x + b_i) + c_0.
\end{equation}\]</span></p>
<div id="a4d23953" class="cell" data-execution_count="8">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="nmFunctions_files/figure-html/cell-8-output-1.png" width="758" height="370" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This function <span class="math inline">\(f(x)\)</span> is determined by the parameters:</p>
<ul>
<li>Hidden layer weights: <span class="math inline">\(w_1, w_2, \ldots, w_n\)</span></li>
<li>Hidden layer biases: <span class="math inline">\(b_1, b_2, \ldots, b_n\)</span><br>
</li>
<li>Output weights: <span class="math inline">\(c_1, c_2, \ldots, c_n\)</span></li>
<li>Output bias: <span class="math inline">\(c_0\)</span></li>
</ul>
<p>That gives us a total of <span class="math inline">\(3n + 1\)</span> parameters to work with.</p>
<p>Note that, due to the simple linear nature of the ReLU activation function that we have chosen to use here, we can absorb the weights of the hidden layer neurons into a rescaling of the of the output weights and the biases. This would not be true for more general activation functions.</p>
<p>So we choose to set all hidden weights to <span class="math inline">\(1\)</span> and thus work with <span class="math display">\[\begin{equation}
f_{nn}(x) = \sum_{i=1}^{n} c_i \max(0, x + b_i) + c_0
\end{equation}\]</span></p>
<p>The following Python code implements such a neural network. Make sure you understand the code.</p>
<div id="cbbf7e32" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> two_layer_network(x, biases, output_weights, output_bias<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Evaluates a two-layer neural network approximation.</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters:</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co">    x (float): The value at which to evaluate the network.</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co">    biases (list or array): Biases for hidden layer</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co">        These determine the knot points: x_i = -b_i</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co">    output_weights (list or array) : Weights for output layer</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co">    output_bias (float): Bias for output layer (default: 0)</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="co">    --------</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="co">    float: The output of the neural network.</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    f_nn <span class="op">=</span> output_bias <span class="op">*</span> np.ones_like(x)  <span class="co"># bias term</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(biases)):</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>        f_nn <span class="op">+=</span> output_weights[i] <span class="op">*</span> np.maximum(<span class="dv">0</span>, x <span class="op">+</span> biases[i])</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> f_nn</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<hr>
<div id="exr-1.47" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.24</strong></span> üí¨ Explain in your own words why a two-layer neural network is flexible enough to approximate many different functions. How do the output weights control the shape of the approximation?</p>
<p>How is this similar to polynomial approximation? How is it different?</p>
</div>
<hr>
<div id="exr-1.49" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.25</strong></span> üíª In this exercise we will approximate a smooth function with a neural network using a simple but powerful idea: we can create a piecewise linear approximation by using a sum of ReLU neurons, each of which activates at a new point. Each ReLU neuron that activates at a new point changes the slope of our piecewise linear approximation.</p>
<p>Consider approximating <span class="math inline">\(f(x) = \sin(x)\)</span> on the interval <span class="math inline">\([0, \pi]\)</span> using a two-layer neural network. We‚Äôll place ReLU neurons at several ‚Äúknot points‚Äù along the curve, and each neuron will adjust the slope.</p>
<ol type="1">
<li><p>Understanding slope changes: Suppose we use knot points at <span class="math inline">\(x_1 = -b_1 = 0, x_2 = -b_2 = \pi/2\)</span>.</p>
<ul>
<li>For <span class="math inline">\(x \in [0, \pi/2)\)</span>: What is the slope of <span class="math inline">\(f_{nn}(x)\)</span>? (Hint: which neurons are active?)</li>
<li>For <span class="math inline">\(x \in [\pi/2, \pi)\)</span>: What is the slope?</li>
</ul>
<p>Explain how the output weight <span class="math inline">\(c_2\)</span> controls the change in slope at <span class="math inline">\(x = \pi/2\)</span>.</p>
<ul>
<li>Suggest a choice for the output weights <span class="math inline">\(c_0, c_1, c_2\)</span> that you think will give some kind of crude approximation of <span class="math inline">\(\sin(x)\)</span> on the interval <span class="math inline">\([0,\pi]\)</span>. Use the Python function <code>plot_neural_network</code> provided below to make a plot of your approximation.</li>
</ul></li>
<li><p>Use the Python function <code>plot_neural_network</code> with 4 knot points at <span class="math inline">\(x = 0, \pi/4, \pi/2, 3\pi/4\)</span>. Try to find output weights that create a good approximation.</p></li>
<li><p>Calculate the maximum approximation error and discuss how it could be improved.</p></li>
<li><p>Experiment with more knot points (try 9 or 17 points evenly spaced). How does the approximation improve?</p></li>
</ol>
<p><strong>Python helper function:</strong></p>
<div id="afdeec72" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt </span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_neural_network(biases, output_weights, output_bias<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>                        x_range<span class="op">=</span>(<span class="dv">0</span>, np.pi), target_func<span class="op">=</span>np.sin):</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Plot a two-layer neural network approximation.</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters:</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="co">    biases (list or array): Biases for hidden layer</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co">        These determine the knot points: x_i = -b_i</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="co">    output_weights (list or array) : Weights for output layer</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="co">    output_bias (float): Bias for output layer (default: 0)</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="co">    x_range : tuple</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="co">        (x_min, x_max) for plotting (default: (0, np.pi))</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="co">    target_func : function</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="co">        The target function to approximate (default: np.sin)</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a><span class="co">    float: Maximum absolute error</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.linspace(x_range[<span class="dv">0</span>], x_range[<span class="dv">1</span>], <span class="dv">500</span>)</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute neural network output</span></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>    f_nn <span class="op">=</span> two_layer_network(x, biases, output_weights, output_bias)</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute target</span></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>    f_target <span class="op">=</span> target_func(x)</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot</span></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>    plt.plot(x, f_target, <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Target function'</span>)</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>    plt.plot(x, f_nn, <span class="st">'b--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Neural network'</span>)</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mark knot points</span></span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>    knots <span class="op">=</span> [<span class="op">-</span>b <span class="cf">for</span> b <span class="kw">in</span> biases]</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> knot <span class="kw">in</span> knots:</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> x_range[<span class="dv">0</span>] <span class="op">&lt;=</span> knot <span class="op">&lt;=</span> x_range[<span class="dv">1</span>]:</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>            plt.axvline(knot, color<span class="op">=</span><span class="st">'gray'</span>, linestyle<span class="op">=</span><span class="st">':'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'y'</span>)</span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Neural Network Approximation'</span>)</span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate and return error</span></span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>    error <span class="op">=</span> np.<span class="bu">max</span>(np.<span class="bu">abs</span>(f_target <span class="op">-</span> f_nn))</span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Maximum error: </span><span class="sc">{</span>error<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> error</span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage:</span></span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a><span class="co"># output_weights = [1, -0.3, -0.5, -0.4]</span></span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a><span class="co"># biases = [0, -np.pi/4, -np.pi/2, -3*np.pi/4]</span></span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a><span class="co"># plot_neural_network(biases, output_weights)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</div>
<hr>
<p>After your experimentation, the following result will no longer seem so surprising:</p>
<div id="thm-1.1" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.1 (Universal Approximation Theorem (informal version))</strong></span> A two-layer neural network with a sufficient number of neurons can approximate any continuous function on a bounded interval to arbitrary accuracy.</p>
<p>More precisely: for any continuous function <span class="math inline">\(f: [a,b] \to \mathbb{R}\)</span> and any <span class="math inline">\(\epsilon &gt; 0\)</span>, there exists a two-layer neural network <span class="math inline">\(f_{\text{nn}}\)</span> such that <span class="math display">\[\begin{equation}
\max_{x \in [a,b]} |f(x) - f_{\text{nn}}(x)| &lt; \epsilon.
\end{equation}\]</span></p>
</div>
<hr>
<div id="exr-1.52" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.26</strong></span> üí¨ Reflect on what you have learned about neural networks and compare them to Taylor series:</p>
<ol type="1">
<li><p>What are the advantages of using neural networks for function approximation compared to Taylor series?</p></li>
<li><p>What are the advantages of Taylor series compared to neural networks?</p></li>
<li><p>For each of the following functions, which approximation method (Taylor series or neural network) do you think would be more appropriate, and why?</p>
<ul>
<li><span class="math inline">\(f(x) = e^x\)</span> near <span class="math inline">\(x = 0\)</span></li>
<li><span class="math inline">\(f(x) = |x|\)</span> near <span class="math inline">\(x = 0\)</span><br>
</li>
<li><span class="math inline">\(f(x) = \sin(100x)\)</span> on <span class="math inline">\([0, 2\pi]\)</span></li>
<li>A function defined by experimental data points</li>
</ul></li>
<li><p>Neural networks are determined by their weights and biases. Taylor series are determined by the derivatives of the function at a point. Which do you think is easier to compute in practice, and why?</p></li>
</ol>
</div>
<hr>
</section>
<section id="sec-deep" class="level3" data-number="2.4.4">
<h3 data-number="2.4.4" class="anchored" data-anchor-id="sec-deep"><span class="header-section-number">2.4.4</span> Deep Neural Networks</h3>
<p>So far we have looked at <strong>two-layer</strong> neural networks (one hidden layer + one output layer). But we can also stack multiple layers on top of each other to create <strong>deep neural networks</strong>.</p>
<p>The idea is simple: instead of having the output layer directly combine the hidden neurons, we can feed the hidden neurons into another layer of neurons, and then another, and so on.</p>
<p>For example, a three-layer network would look like:</p>
<ol type="1">
<li><p><strong>Input</strong>: <span class="math inline">\(x\)</span></p></li>
<li><p><strong>First Hidden Layer</strong>: <span class="math display">\[\begin{equation}
h_i^{(1)}(x) = \max(0, w_i^{(1)} x + b_i^{(1)}) \quad \text{for } i = 1, \ldots, n_1
\end{equation}\]</span></p></li>
<li><p><strong>Second Hidden Layer</strong>: Each neuron in this layer takes all outputs from the first hidden layer: <span class="math display">\[\begin{equation}
h_j^{(2)} = \max\left(0, \sum_{i=1}^{n_1} w_{ji}^{(2)} h_i^{(1)} + b_j^{(2)}\right) \quad \text{for } j = 1, \ldots, n_2
\end{equation}\]</span></p></li>
<li><p><strong>Output Layer</strong>: <span class="math display">\[\begin{equation}
f(x) = \sum_{j=1}^{n_2} c_j h_j^{(2)} + c_0
\end{equation}\]</span></p></li>
</ol>
<p>This is called a <strong>deep neural network</strong> because it has multiple hidden layers. The term ‚Äúdeep learning‚Äù comes from using such multi-layer architectures.</p>
<div id="60a345c3" class="cell" data-execution_count="11">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="nmFunctions_files/figure-html/cell-11-output-1.png" width="950" height="470" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<hr>
</section>
<section id="why-depth-matters-the-sawtooth-example" class="level3" data-number="2.4.5">
<h3 data-number="2.4.5" class="anchored" data-anchor-id="why-depth-matters-the-sawtooth-example"><span class="header-section-number">2.4.5</span> Why Depth Matters: The Sawtooth Example</h3>
<p>Why are deep neural networks, with many layers, often more powerful than shallow networks with just one large hidden layer? In the following exercises, you will explore a classic example, the <strong>sawtooth wave</strong>, to discover a concrete answer. You‚Äôll build both a shallow and a deep network to represent the same function, and compare their efficiency as the complexity grows. Through this exploration, you‚Äôll see how deep networks can represent certain functions far more efficiently by exploiting their compositional structure.</p>
<p>We‚Äôll work with a sawtooth function that has 2 triangular peaks on the interval <span class="math inline">\([0, 1]\)</span>: <span class="math display">\[\begin{equation}
f(x) = \begin{cases}
4x &amp; \text{if } 0 \leq x &lt; 0.25 \\
2 - 4x &amp; \text{if } 0.25 \leq x &lt; 0.5 \\
4x - 2 &amp; \text{if } 0.5 \leq x &lt; 0.75 \\
4 - 4x &amp; \text{if } 0.75 \leq x \leq 1
\end{cases}
\end{equation}\]</span></p>
<div id="b8fb7414" class="cell" data-execution_count="12">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="nmFunctions_files/figure-html/cell-12-output-1.png" width="663" height="376" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<hr>
<div id="exr-1.50" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.27 (Building a Shallow Network for the Sawtooth)</strong></span> You already know how to represent piecewise linear functions with a two-layer neural network. Each of the 5 ‚Äúhinge points‚Äù (where the slope changes) requires a ReLU neuron.</p>
<p><strong>a)</strong> For each hinge point at position <span class="math inline">\(x_i\)</span>, you need a ReLU neuron with bias <span class="math inline">\(b_i = -x_i\)</span>.</p>
<p><strong>b)</strong> What are the output weights with which you need to combinae these neurons?</p>
<p>Use this to complete the code below and use the plot to check that the resulting function has the desired form.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> shallow_2_peak_network(x):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    biases <span class="op">=</span> [.....]</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    output_weights <span class="op">=</span> [.....]</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> two_layer_network(x, biases, output_weights)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">0.1</span>,<span class="fl">1.1</span>,<span class="dv">500</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>plt.plot(x, shallow_2_peak_network(x), <span class="st">'r-'</span>, lw<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'f(x)'</span>)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'2-Peak Sawtooth Function'</span>)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<hr>
<div id="exr-1.51" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.28 (Building the Deep Network)</strong></span> A deeper network takes a more elegant approach. Instead of directly creating all the peaks, it uses one layer to create a single peak and then another layer to duplicate this peak.</p>
<p>The key insight is to create a single-peak function <span class="math display">\[g(x) = \begin{cases}2x&amp;0\leq x&lt;1/2\\2(1-x)&amp;1/2\leq x\leq1\\0 &amp; x\not\in [0,1]\end{cases}\]</span> that produces one triangular peak at <span class="math inline">\(x = 0.5\)</span> with height 1, returning to 0 at <span class="math inline">\(z &lt; 0\)</span> and <span class="math inline">\(z &gt; 1\)</span>.</p>
<div id="a6d215cc" class="cell" data-execution_count="13">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="nmFunctions_files/figure-html/cell-13-output-1.png" width="812" height="523" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Then, our 2-peak sawtooth is simply <span class="math inline">\(f(x) = g(g(x))\)</span>.</p>
<p><strong>a)</strong> First, create a single-peak as a shallow network with three hidden neurons.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> g(x):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" Single triangular peak """</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    biases <span class="op">=</span> [.....]</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    output_weights <span class="op">=</span> [.....]</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> two_layer_network(x, biases, output_weights)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">0.1</span>,<span class="fl">1.1</span>,<span class="dv">500</span>)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>plt.plot(x, g(x), <span class="st">'r-'</span>, lw<span class="op">=</span><span class="dv">3</span>))</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'g(x)'</span>)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Single peak function'</span>)</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>b)</strong> Now comes the magic! Pass the result of your network <span class="math inline">\(g(x)\)</span> through the same network again: compute <span class="math inline">\(g(g(x))\)</span>. Plot the result and compare it to the target.</p>
<div id="21096fa2" class="cell" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> <span class="kw">lambda</span> x: g(g(x))</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">0.1</span>,<span class="fl">1.1</span>,<span class="dv">500</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>plt.plot(x, f(x), <span class="st">'r-'</span>, lw<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'f(x)'</span>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'2-Peak Sawtooth Function'</span>)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="nmFunctions_files/figure-html/cell-14-output-1.png" width="812" height="523" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p><strong>c)</strong> Explain in your own words: Why does applying <span class="math inline">\(g\)</span> twice produce two peaks? Think about what happens as the first application of <span class="math inline">\(g\)</span> creates a peak that rises from 0 to 1 and back to 0. What does the second application of <span class="math inline">\(g\)</span> see as its input?</p>
<p><strong>d)</strong> Create a 4-peak sawtooth by composing <span class="math inline">\(g\)</span> three times: <span class="math inline">\(g(g(g(x)))\)</span>. Plot it. How many hidden neurons does this use? Compare to the shallow network approach from the previous exercise.</p>
</div>
<hr>
<div id="exr-1.52" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.29 (Scaling Analysis: The Exponential Advantage)</strong></span> Now let‚Äôs compare how the two approaches scale as we increase the number of peaks.</p>
<p><strong>a)</strong> üñã Complete this table by implementing both shallow and deep networks for different numbers of peaks:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 30%">
<col style="width: 26%">
<col style="width: 22%">
</colgroup>
<thead>
<tr class="header">
<th>Number of Peaks</th>
<th>Shallow Network Neurons</th>
<th>Deep Network Neurons</th>
<th>Advantage (ratio)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2</td>
<td>5</td>
<td>6</td>
<td>0.83</td>
</tr>
<tr class="even">
<td>4</td>
<td>?</td>
<td>?</td>
<td>?</td>
</tr>
<tr class="odd">
<td>8</td>
<td>?</td>
<td>?</td>
<td>?</td>
</tr>
<tr class="even">
<td>16</td>
<td>?</td>
<td>?</td>
<td>?</td>
</tr>
</tbody>
</table>
<p><strong>b)</strong> üñã Based on your table, write formulas for the number of neurons needed as a function of the number of peaks <span class="math inline">\(n\)</span> for both approaches. What kind of growth do you observe (linear, logarithmic, exponential)?</p>
<p><em>Hint:</em> For the deep network, if <span class="math inline">\(n = 2^k\)</span> peaks, how many layers do you need? How many neurons per layer?</p>
<p><strong>c)</strong> üíª Visualize the scaling behavior. Plot the number of neurons (y-axis) versus the number of peaks (x-axis) for both approaches:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">TODO</span><span class="co">: Create arrays for different numbers of peaks</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>peaks <span class="op">=</span> [<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">16</span>, <span class="dv">32</span>, <span class="dv">64</span>]</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>shallow_neurons <span class="op">=</span> [...]  <span class="co"># Fill in based on your formula</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>deep_neurons <span class="op">=</span> [...]     <span class="co"># Fill in based on your formula</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>plt.plot(peaks, shallow_neurons, <span class="st">'bo-'</span>, label<span class="op">=</span><span class="st">'Shallow Network'</span>, lw<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>plt.plot(peaks, deep_neurons, <span class="st">'mo-'</span>, label<span class="op">=</span><span class="st">'Deep Network'</span>, lw<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Number of Peaks'</span>)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Number of Hidden Neurons'</span>)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Scaling: Shallow vs Deep Networks'</span>)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>d)</strong> üí¨ Based on your analysis, explain why deep networks can be more efficient than shallow networks for certain types of functions. What specific property of the sawtooth function makes it particularly well-suited for a deep architecture?</p>
<p><strong>e)</strong> üí¨ The deep network achieves its efficiency by <strong>reusing</strong> the same learned ‚Äúpeak-making‚Äù module across layers through composition. Can you think of other real-world functions or patterns that might benefit from deep architectures in a similar way? Consider:</p>
<ul>
<li>Functions with repetitive structures at different scales (e.g., fractals, wavelets)</li>
<li>Hierarchical patterns (e.g., language: letters ‚Üí words ‚Üí sentences)</li>
<li>Functions that can be naturally expressed as compositions of simpler functions</li>
</ul>
<p><strong>f)</strong> üí¨ What are the limitations of this advantage? Can you think of functions where a shallow network might be more efficient than a deep one? (Hint: What if the function has no repetitive or compositional structure?)</p>
</div>
<hr>
</section>
</section>
<section id="sec-functions_problems" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="sec-functions_problems"><span class="header-section-number">2.5</span> Problems</h2>
<div id="exr-1.54" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.30</strong></span> Find the Taylor Series for <span class="math inline">\(f(x) = \frac{1}{\log(x)}\)</span> centred at the point <span class="math inline">\(x_0 = e\)</span>. Then use the Taylor Series to approximate the number <span class="math inline">\(\frac{1}{\log(3)}\)</span> to 4 decimal places.</p>
</div>
<hr>
<div id="exr-1.55" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.31</strong></span> In this problem we will use Taylor Series to build approximations for the irrational number <span class="math inline">\(\pi\)</span>.</p>
<ol type="a">
<li><p>Write the Taylor series centred at <span class="math inline">\(x_0=0\)</span> for the function <span class="math display">\[\begin{equation}
f(x) = \frac{1}{1+x}.
\end{equation}\]</span></p></li>
<li><p>Now we want to get the Taylor Series for the function <span class="math inline">\(g(x) = \frac{1}{1+x^2}\)</span>. It would be quite time consuming to take all of the necessary derivatives to get this Taylor Series. Instead we will use our answer from part (a) of this problem to shortcut the whole process.</p>
<ol type="1">
<li><p>Substitute <span class="math inline">\(x^2\)</span> for every <span class="math inline">\(x\)</span> in the Taylor Series for <span class="math inline">\(f(x) = \frac{1}{1+x}\)</span>.</p></li>
<li><p>Make a few plots to verify that we indeed now have a Taylor Series for the function <span class="math inline">\(g(x) = \frac{1}{1+x^2}\)</span>.</p></li>
</ol></li>
<li><p>Recall from Calculus that <span class="math display">\[\begin{equation}
\int \frac{1}{1+x^2} dx = \arctan(x).
\end{equation}\]</span> Hence, if we integrate each term of the Taylor Series that results from part (2) we should have a Taylor Series for <span class="math inline">\(\arctan(x)\)</span>.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p></li>
<li><p>Now recall the following from Calculus:</p>
<ul>
<li><p><span class="math inline">\(\tan(\pi/4) = 1\)</span></p></li>
<li><p>so <span class="math inline">\(\arctan(1) = \pi/4\)</span></p></li>
<li><p>and therefore <span class="math inline">\(\pi = 4\arctan(1)\)</span>.</p></li>
</ul>
<p>Let us use these facts along with the Taylor Series for <span class="math inline">\(\arctan(x)\)</span> to approximate <span class="math inline">\(\pi\)</span>: we can just plug in <span class="math inline">\(x=1\)</span> to the series, add up a bunch of terms, and then multiply by 4. Write a loop in Python that builds successively better and better approximations of <span class="math inline">\(\pi\)</span>. Stop the loop when you have an approximation that is correct to 6 decimal places.</p></li>
</ol>
</div>
<hr>
<div id="exr-1.56" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.32</strong></span> In this problem we will prove the famous formula <span class="math display">\[\begin{equation}
e^{i\theta} = \cos(\theta) + i \sin(\theta).
\end{equation}\]</span> This is known as Euler‚Äôs formula after the famous mathematician Leonard Euler. Show all of your work for the following tasks.</p>
<ol type="1">
<li><p>Write the Taylor series for the functions <span class="math inline">\(e^x\)</span>, <span class="math inline">\(\sin(x)\)</span>, and <span class="math inline">\(\cos(x)\)</span>.</p></li>
<li><p>Replace <span class="math inline">\(x\)</span> with <span class="math inline">\(i\theta\)</span> in the Taylor expansion of <span class="math inline">\(e^x\)</span>. Recall that <span class="math inline">\(i = \sqrt{-1}\)</span> so <span class="math inline">\(i^2 = -1\)</span>, <span class="math inline">\(i^3 = -i\)</span>, and <span class="math inline">\(i^4 = 1\)</span>. Simplify all of the powers of <span class="math inline">\(i\theta\)</span> that arise in the Taylor expansion. I will get you started: <span class="math display">\[\begin{equation}
\begin{aligned} e^x &amp;= 1 + x + \frac{x^2}{2} + \frac{x^3}{3!} + \frac{x^4}{4!} + \frac{x^5}{5!} + \cdots \\ e^{i\theta} &amp;= 1 + (i\theta) + \frac{(i\theta)^2}{2!} + \frac{(i\theta)^3}{3!} + \frac{(i\theta)^4}{4!} + \frac{(i\theta)^5}{5!} + \cdots \\ &amp;= 1 + i\theta + i^2 \frac{\theta^2}{2!} + i^3 \frac{\theta^3}{3!} + i^4 \frac{\theta^4}{4!} + i^5 \frac{\theta^5}{5!} + \cdots \\ &amp;= \ldots \text{ keep simplifying ... } \ldots \end{aligned}
\end{equation}\]</span></p></li>
<li><p>Gather all of the real terms and all of the imaginary terms together. Factor the <span class="math inline">\(i\)</span> out of the imaginary terms. What do you notice?</p></li>
<li><p>Use your result from part (3) to prove that <span class="math inline">\(e^{i\pi} + 1 = 0\)</span>.</p></li>
</ol>
</div>
<hr>
<div id="exr-1.57" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.33</strong></span> In physics, the <em>relativistic energy</em> of an object is defined as <span class="math display">\[\begin{equation}
E_{rel} = \gamma mc^2
\end{equation}\]</span> where <span class="math display">\[\begin{equation}
\gamma = \frac{1}{\sqrt{1 - \frac{v^2}{c^2}}}.
\end{equation}\]</span> In these equations, <span class="math inline">\(m\)</span> is the mass of the object, <span class="math inline">\(c\)</span> is the speed of light (<span class="math inline">\(c \approx 3 \times 10^8\)</span>m/s), and <span class="math inline">\(v\)</span> is the velocity of the object. For an object of fixed mass (m) we can expand the Taylor Series centred at <span class="math inline">\(v=0\)</span> for <span class="math inline">\(E_{rel}\)</span> to get <span class="math display">\[\begin{equation}
E_{rel} = mc^2 + \frac{1}{2} mv^2 + \frac{3}{8} \frac{mv^4}{c^2} + \frac{5}{16} \frac{mv^6}{c^4} + \cdots.
\end{equation}\]</span></p>
<ol type="1">
<li><p>What do we recover if we consider an object with zero velocity?</p></li>
<li><p>Why might it be completely reasonable to only use the quadratic approximation <span class="math display">\[\begin{equation}
E_{rel} = mc^2 + \frac{1}{2} mv^2
\end{equation}\]</span> for the relativistic energy equation?<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p></li>
<li><p>(some physics knowledge required) What do you notice about the second term in the Taylor Series approximation of the relativistic energy function?</p></li>
<li><p>Show all of the work to derive the Taylor Series centred at <span class="math inline">\(v = 0\)</span> given above.</p></li>
</ol>
</div>


</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>There are many reasons why integrating an infinite series term by term should give you a moment of pause. For the sake of this problem we are doing this operation a little blindly, but in reality we should have verified that the infinite series actually converges uniformly.<a href="#fnref1" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn2"><p>This is something that people in physics and engineering do all the time ‚Äì there is some complicated nonlinear relationship that they wish to use, but the first few terms of the Taylor Series captures almost all of the behaviour since the higher-order terms are very very small.<a href="#fnref2" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./nmNumbers.html" class="pagination-link" aria-label="Numbers">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Numbers</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./nmRoots1.html" class="pagination-link" aria-label="Roots 1">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Roots 1</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/gustavdelius/NumericalAnalysis2026/edit/main/nmFunctions.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></div></div></footer></body></html>